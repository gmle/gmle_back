{"pages":[{"title":"分类","date":"2016-04-22T04:39:04.000Z","updated":"2018-07-21T13:46:28.793Z","comments":false,"path":"categories/index.html","permalink":"http://gmle.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2016-04-22T04:39:04.000Z","updated":"2018-07-21T13:46:26.102Z","comments":false,"path":"tags/index.html","permalink":"http://gmle.github.io/tags/index.html","excerpt":"","text":""},{"title":"简历","date":"2018-09-26T12:53:46.551Z","updated":"2018-09-26T12:53:46.551Z","comments":true,"path":"resume/index.html","permalink":"http://gmle.github.io/resume/index.html","excerpt":"","text":"联系方式 手机：(全天) 185xxxxxxxx 136xxxxxxxx Email： xxx@xxx.com xxx@xxx.com 网络联系方式： 企鹅：122xxxx 微信：wlxxxx 个人信息 王乐/男/1996 专科/河北软件学院/软件工程系/软件开发与设计方向 期望职位：C++服务端开发，C++开发/研发 期望城市：北京 技能清单以下均为我熟练使用的技能 熟悉Linux下C/C++编程 熟悉linuxc多线程/多进程编程 熟悉TCP/IP、Socket网络通信编程技术，对HTTP协议有了解 熟练使用常用的调试工具及调试方法 GDB/LLDB 能够熟练编写shell脚本并完成一系列的自动化操作 熟悉Java、Scala、python的使用, 了解HTML、CSS、JS 了解Java框架例如Spring/SpringMVC/Hibrnate/MyBities等 熟悉数据库 MySQL/Oracle/HBase 熟悉常用版本控制工具Git/SVN 熟悉使用构建工具例如CMake 熟悉常用算法和数据结构 Hadoop、Spark、kafka、flume、storm、Zookeeper、Hive的使用、基本配置调优，具有丰富的集群调错能力。 工作经历大快搜索（北京）数据技术有限公司(官网：DkSou) （ 2016年06月 ~ 2018年04月 ）模块：数据流加密（FreeRCH底层模块）产品描述数据流加密是为FreeRCH开发的一款客户端与服务器调用的时候做数据流(调用的类库与Jar包)加密工作，防止篡改其连接串信息、泄露数据仓库用户名、密码等重要信息。 职位与任务 担任职位：开发、测试 项目涵盖：libevent、OpenSSL、json、oracle、Qt 主要工作内容：设计此模块接口，利用该接口对FreeRCH数据流从客户端开始进行深度加密、并传输至服务端进行对应算法解密后再对DKH进行开发调用。 技术点： 使用libevent进行服务端开发 libevent实现多客户进行同步开发时的并发处理 使用OpenSSL进行数据流加密解密 使用3DES对数据进行加密解密 数据流解密后直接使用shell解释器执行 数据流校验 对客户端提供的加密数据流进行md5校验 产品：DKHadoop（DKH大数据平台）产品描述DKH是打通大数据生态系统与传统非大数据公司之间的通道而设计的大数据通用计算平台。通过使用DKH，可以直接使用其集成大数据相关组件。将大数据、搜索、分布式计算、内存计算、图计算、流计算、自然语言处理、机器学习等复杂的技术变成简单的接口和类库。 职位与任务 担任职位：DKH平台运维，开发，测试，技术支持。 项目涵盖：Hadoop HA、HBase HA、Zookeeper Cluster、Spark HA Cluster、Flume、Strom Cluster、Hive、Kafka Cluster、ElasticSearch Cluster、Sqoop、MySQL热备。 主要工作内容：设计各个组件之间的耦合度，着手自动化数据平台搭建，利用Bash、python进行平台的自动化配置各种组件并进行自动化测试。 技术点： 使用shell、tcl、except进行平台中各个组件的自动化安装 集群(三台服务器及以上)组件分发，保证传输速度，避免分发进程僵死。 使用tcl、except实现集群间自动通信、ssh集群免密 利用Shell为前台提供所有基础服务的一键启动、关闭、重启，提供组件的服务启动界面化的功能实现。 整个集群的开关，集群中小服务的启动和停止 整个集群配置文件的修改后分发、备份 产品：FreeRCH（大数据与人工智能开发框架）产品描述采用类黑箱框架模式，由36个类共计近200种方法组成的类库，基本可以完成各类大数据和人工智能项目的开发，用户直接调用大快的相关类，即可完成过去复杂的编码工作，降低大数据生态系统的入门难度。 职位与任务 担任职位：开发，测试。 项目涵盖：数据源与SQL引擎、数据采集（自定义爬虫）模块、数据处理模块、机器学习算法、自然语言处理模块和搜索引擎模块六部分组成。 主要工作内容：开发以DKH为基础的大数据框架，整合DKH与FreeRCH的关系，测试相关框架的性能，并使其具有DKH唯一性。 基本了解常用分类/聚类机器学习算法（朴素贝叶斯、K-Means、频繁项集、PCA、ACM、随机森林等）。 技术点: 测试： NLP、数据导入导出 NLP：使用HanLP框架，测试方向：分词、新词发现、多音字检索。 数据导入导出：使用DK-Sqoop实现各个数据源中的数据导入导出，例：Hadoop-&gt;Oracle/MySQL/HBase 等。 开发： 机器学习算法 机器学习分类聚类的实现(包括服务端和客户端)，服务端采用Scala开发，主要使用Spark-API，客户端使用Java进行API调用。 数据处理模块 数据清洗的实现(包括服务端和客户端)，服务端采用Java开发，主要使用Hive-API，客户端使用Java进行API调用。 廊坊市大华夏神农信息技术有限公司(实习) （ 2015年11月 ~ 2015年12月 ）项目：三省农场 项目描述：三省农场是华夏幸福物业的一个物联网的智能农场项目，分为一个管理平台和三个APP端(农场管理人员、劳作人员、领导)。实现了农场的所有操作全部入网，包括蔬菜/家禽等动植物的养殖、浇水等一系列操作，提供了每一种动植物的追根溯源。 担任职位：驻场运维(偏硬件方向) 主要工作内容：参与需求信息调研和设计、测试数据、执行测试、提交和跟踪缺陷. 以下是我基本掌握的技能并不断的深入 Qt Python CDH 自我评价本人具有比较强的专业理论知识,基础扎实，涉及广泛，为人真诚乐观自信，勤奋务实，责任感强，热爱集体，助人为乐，能恪守以大局为重的原则，愿意服从集体利益的需要，具备奉献精神。爱好专研新技术，拥有较强的自学能力，能快速学习新技术，喜欢有挑战的项目，并从中快速进步，抗压能力强，适应出差。 致谢感谢您花时间阅读我的简历，期待能有机会能和您共事。"},{"title":"关于","date":"2018-05-10T07:06:38.245Z","updated":"2018-03-16T09:42:21.000Z","comments":true,"path":"about/index.html","permalink":"http://gmle.github.io/about/index.html","excerpt":"","text":"Info 性别 x 爱好 x #1x。"},{"title":"读书","date":"2016-04-22T04:39:04.000Z","updated":"2019-05-19T16:33:35.412Z","comments":false,"path":"books/index.html","permalink":"http://gmle.github.io/books/index.html","excerpt":"","text":""}],"posts":[{"title":"Linux-shebang","slug":"linux-shebang详解","date":"2018-10-18T01:21:21.849Z","updated":"2018-10-18T01:27:21.434Z","comments":true,"path":"2018/10/18/linux-shebang详解/","link":"","permalink":"http://gmle.github.io/2018/10/18/linux-shebang详解/","excerpt":"shebang 是linux shell脚本中的第一行的符号即 “#!”","text":"shebang 是linux shell脚本中的第一行的符号即 “#!” Shebang这个符号通常在Unix系统的脚本中第一行开头中写到，它指明了执行这个脚本文件的解释程序。 如果脚本文件中没有“#！”这一行，那么他执行时会默认使用当前shell去解释这个脚本（即$SHELL环境变量）。 如果“#！”之后的解释程序是一个可执行文件，那么执行这个脚本是，他就会把文件名及其参数作为参数传给那个解释程序去执行。 如果“#！”指定的解释程序没有可执行权限，则会报错“bad interpreter：Permission denied”（拒绝访问，也就是没有权限）。如果“#！”指定的解释程序不是一个可执行文件，那么指定的解释程序会被忽略，转而给当前的shell去执行这个脚本。 如果“#！”指定的解释程序不存在，那么会报错“bad interpret ： No such file ordirectory”，注意：“#！”之后的解释程序，需要些其绝对路径（例如：/bin/bash）,他是不会自动到$PATH中寻找解释器的。 当然，如果你使用的“bash test.sh”这样的命令来执行脚本，那么“#！”这一行将被忽略，解释器当然是用命令行中显示式指定的bash。","categories":[{"name":"linux","slug":"linux","permalink":"http://gmle.github.io/categories/linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://gmle.github.io/tags/Linux/"}]},{"title":"fastDFS使用与介绍","slug":"fastDFS","date":"2018-10-14T02:21:43.545Z","updated":"2018-10-15T07:31:52.866Z","comments":true,"path":"2018/10/14/fastDFS/","link":"","permalink":"http://gmle.github.io/2018/10/14/fastDFS/","excerpt":"fastDFS是用c写的一款开源的分布式文件系统，可以很容易地搭建一套分布式文件系统。","text":"fastDFS是用c写的一款开源的分布式文件系统，可以很容易地搭建一套分布式文件系统。 fastDFS中的三个角色 追踪器(Tracker)-&gt;管理者：管理存储节点，类似于Hadoop中的namenode 需要多个Tracker以避免单点故障 与Hadoop不同的是Tracker是以轮询的方式实现的 存储节点(storage)-&gt;存储节点：存储数据，类似于Hadoop中的DataNode (实现)客户端(client)-&gt;界面管理：上传下载功能的实现 安装下载 libfastcommon与fastdfs安装过程相同：​ 先安装libfastcommon​ ./make.sh &amp;&amp; ./make.sh install​ 再安装fastdfs​ ./make.sh &amp;&amp; ./make.sh install 工作顺序 启动Tracker Tracker接收storage的心跳信息并与其建立映射 启动storage 单独开启一个线程 汇报节点状态 汇报当前存储节点的磁盘信息 汇报数据同步情况 汇报数据被下载的次数 启动客户端 上传 连接Tracker，询问存储节点信息 Tracker查询符合上传文件大小的存储节点并返回节点信息 Tracker将节点信息返回给client (集群)选择需要连接的Tracker 发送文件 选择存储的group 存储上传文件后返回的一个文件地址 下载 连接Tracker，查询文件所在节点 Tracker返回存储文件的节点信息，返回节点的ip:端口 (集群)选择需要连接的Tracker 下载文件 测试安装安装libfastcommon-1.36.zip（fastdfs的基础库包 ） unzip libfastcommon-1.36.zip ./make.sh ./make.sh install fastdfs-5.10.tar.gz​ tar zxvf fastdfs-5.10.tar.gz​ ./make.sh​ ./make.sh install 测试12#fastDFS安装的所有的可执行程序: /usr/bin/fdfs_*fdfs_test fastDFS配置文件 配置文件默认位置: /etc/fdfs client.conf.sample storage.conf.sample storage_ids.conf.sample tracker.conf.sample tracker.conf12345# 将追踪器和部署的主机的IP地址进程绑定, 也可以不指定# 如果不指定, 会自动绑定当前主机IP, 如果是云服务器建议不要写 bind_addr=192.168.247.135# 追踪器监听的端口port=22122# 追踪器存储日志信息的目录, xxx.pid文件, 必须是一个存在的目录 base_path=/home/yuqing/fastdfs storage.conf1234567891011# 当前存储节点对应的主机属于哪一个组 group_name=group1 group_name=group1# 当前存储节点和所应该的主机进行IP地址的绑定, 如果不写, 有fastdfs自动绑定 bind_addr=# 存储节点绑定的端口port=23000# 存储节点写log日志的路径 base_path=/home/yuqing/fastdfs# 存储节点提供的存储文件的路径个数 store_path_count=2# 具体的存储路径 store_path0=/home/yuqing/fastdfs store_path1=/home/yuqing/fastdfs1# 追踪器的地址信息 tracker_server=192.168.247.135:22122 tracker_server=192.168.247.136:22122tracker_server=192.168.247.135:22122tracker_server=192.168.247.136:22122 client.conf123456# 客户端写log日志的目录# 该路径必须存在# 当前的用户对于该路径中的文件有读写权限# 当前用户robin# 指定的路径属于root base_path=/home/yuqing/fastdfs# 要连接的追踪器的地址信息 tracker_server=192.168.247.135:22122 tracker_server=192.168.247.136:22122 启动tracker追踪器的启动1234567# 启动程序在 /usr/bin/fdfs_*# 启动fdfs_trackerd 追踪器的配置文件(/etc/fdfs/tracker.conf)# 关闭fdfs_trackerd 追踪器的配置文件(/etc/fdfs/tracker.conf) stop# 重启fdfs_trackerd 追踪器的配置文件(/etc/fdfs/tracker.conf) restart storage存储节点的启动123456# 启动fdfs_storaged 存储节点的配置文件(/etc/fdfs/stroga.conf)# 关闭fdfs_storaged 存储节点的配置文件(/etc/fdfs/stroga.conf) stop# 重启fdfs_storaged 存储节点的配置文件(/etc/fdfs/stroga.conf) restart client客户端启动12345# 上传fdfs_upload_file 客户端的配置文件(/etc/fdfs/client.conf) 要上传的文件# 得到的结果字符串: group1/M00/00/00/wKj3h1vC-PuAJ09iAAAHT1YnUNE31352.c# 下载fdfs_download_file 客户端的配置文件(/etc/fdfs/client.conf) 上传成功之后得到的字符串(fileID) fastdfs状态检测12# 检测命令fdfs_monitor /etc/fdfs/client.conf storage的七种状态 1234567# FDFS_STORAGE_STATUS:INIT:初始化，尚未得到同步已有数据的源服务器# FDFS_STORAGE_STATUS:WAIT_SYNC :等待同步，已得到同步已有数据的源服务器# FDFS_STORAGE_STATUS:SYNCING:同步中# FDFS_STORAGE_STATUS:DELETED:已删除，该服务器从本组中摘除# FDFS_STORAGE_STATUS:OFFLINE:离线# FDFS_STORAGE_STATUS:ONLINE:在线，尚不能提供服务# FDFS_STORAGE_STATUS:ACTIVE:在线，可以提供服务 fileIDfile.md -&gt; group1/M00/00/00wKhS_VlrEf0AdIZyAAAJT0wCgr43848.md group1 文件上传到的存储节点的哪个组 如果有多个组，组名是变化的 M00 虚拟目录 和存储节点的配置项映射 store_path0=/home/yuqing/fastdfs/data -&gt; M00 store_path1=/home/yuqing/fastdfs1/data -&gt; M01 00/00 实际路径 可变 wKhS_VlrEfOAdIZyAAAJTOwCGr43848.md 文件名包含的信息， Base64编码，信息包括： 源storage server ip地址 文件创建时间 文件大小 文件CRC32校验码 循环冗余校验 随机数","categories":[{"name":"fastDFS","slug":"fastDFS","permalink":"http://gmle.github.io/categories/fastDFS/"}],"tags":[{"name":"fastDFS","slug":"fastDFS","permalink":"http://gmle.github.io/tags/fastDFS/"}]},{"title":"结构体数据对齐详解","slug":"结构体数据对齐","date":"2018-09-18T06:46:21.626Z","updated":"2018-09-20T06:26:32.706Z","comments":true,"path":"2018/09/18/结构体数据对齐/","link":"","permalink":"http://gmle.github.io/2018/09/18/结构体数据对齐/","excerpt":"数据对齐结构体数据对齐：指结构体内各个数据的内存地址的对齐在结构体中的第一个成员的首地址等于整个结构体的变量的首地址后面的成员地址随着它声明的首地址和实际占用的字节数递增。而为了总的结构体大小对齐，会在结构体中插入一些没有实际意义的字符填充结构体","text":"数据对齐结构体数据对齐：指结构体内各个数据的内存地址的对齐在结构体中的第一个成员的首地址等于整个结构体的变量的首地址后面的成员地址随着它声明的首地址和实际占用的字节数递增。而为了总的结构体大小对齐，会在结构体中插入一些没有实际意义的字符填充结构体 通俗点讲，计算机系统对基本类型的数据在内存中存放的位置有限制，系统会要求这些数据的首地址的值是某个数(这个数一般为4或者8的)的倍数，这就是所谓的内存对齐 而32位机器上默认的对齐模数一般为4，64位机上位8。 在结构体中，成员数据对齐满足以下规则： 结构体重的第一个成员的首地址即时结构体变量的首地址。 结构体中的每一个成员的首地址相对于结构体IDE首地址的偏移量是该成员数据类型大小的整数倍。 结构体的总大小是对齐模数（对齐模数等于#pragma pack(n)所指定的n与结构体重最大数据类型的成员大小的最小值）的整数倍 Example：12345678910struct One&#123; double d; char c; int i;&#125;struct Two&#123; char c; double d; int i;&#125; struct type pack(4) pack(8) one double 8 8 char 1+3 1+3 int 4 4 result 16 16 two char 1+3 1+7 double 8 8 int 4 4+4 result 16 24 进阶C++C++中的数据对齐环境：macOS 11.13.6 64位编译器：clang-902.0.39.2系统int大小为4字节，指针大小为8字节。 空类1class A &#123;&#125;; 空类sizeof的结果为1，为什么不是0呢？因为C++标准规定两个不同实例的内存地址必须不同，所以用这一个字节来占用不同的内存地址，让空类的两个实例可以相互区分。 单个数据类型大多数编译器支持空基类优化（Empty Base Class Optimization, EBCO），即从空基类中派生出来的类并不会增加1字节，如：123class B:public A&#123; int a;&#125;; sizeof(B)的结果为4而不是5或8。 静态数据成员类型1234class C&#123; int a; static int b;&#125;; sizeof 结果为4,静态数据成员被存放在类对象之外。 带非虚函数成员的类12345class D &#123; public: void func1() &#123;&#125; static void func2() &#123;&#125;&#125;; sizeof(D)结果为1，无论是普通成员函数还是静态成员函数都被存放在类对象之外。 带虚函数成员的类1234class E &#123; public: virtual void func() &#123;&#125;&#125;; sizeof(E)结果为8，带虚函数成员的类对象会包含一个指向该类的virtual table的指针。 普通派生类123class G : public C &#123; int a;&#125;; sizeof(G)的结果为8，派生类会存放基类中非静态数据成员(C中的a)的副本。 基类带虚函数的派生类1class H : public E &#123;&#125;; sizeof(H)结果为8，由于基类中带虚函数，派生类中也必须保存一个指向派生类的virtual table的指针。 多重继承的派生类12345678910class E1 &#123;public: virtual void func() &#123;&#125;&#125;;class E2 &#123;public: virtual void func() &#123;&#125;&#125;;class E3 :public E1, public E2&#123;&#125;; sizeof(E3)结果为16，子类中保存了两个virtual table的指针 虚继承的派生类1234567class H1&#123;&#125;;class H2&#123; int a&#123;&#125;;&#125;;class H3:public virtual H1, public virtual H2&#123;&#125;; sizeof(H3)的结果为16，是两个基类中的virtual table指针 普通类的对齐规则1234class F &#123; char a; int b;&#125;; sizeof(F)的结果为8而不是5，由于F的最大对齐值为4（int），因此a和b之间被补齐3字节。 多重继承下的对齐规则12345678910111213class G1&#123; long l; int a; char *b; virtual void func()&#123;&#125;;&#125;;class G2&#123; int a; char b; virtual void func()&#123;&#125;;&#125;;class G3:public G1, public G2&#123;&#125;; sizeof(G3)的结果为48，默认对齐模数为8的情况下G1 = （8）+（4）+（8）+（8）G2 = （4+4）+（1+7）+（8）G3 = 48","categories":[{"name":"C","slug":"C","permalink":"http://gmle.github.io/categories/C/"}],"tags":[{"name":"C","slug":"C","permalink":"http://gmle.github.io/tags/C/"}]},{"title":"kill && kill -9","slug":"kill与kill -9的区别","date":"2018-08-25T01:35:51.228Z","updated":"2018-09-20T01:42:36.403Z","comments":true,"path":"2018/08/25/kill与kill -9的区别/","link":"","permalink":"http://gmle.github.io/2018/08/25/kill与kill -9的区别/","excerpt":"测试kill和kill -9信号的区别。","text":"测试kill和kill -9信号的区别。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * Powered by Jetbrains Clion. * Created by 王乐. * Date: 2018/8/25. * FileName: kill.c. *///***** Code is coming! *****//#include &lt;unistd.h&gt;#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/time.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;void sigHandler(int sig) &#123; printf(\"signo = %d\\n\", sig); kill(getpid(), SIGKILL);&#125;int main(int argc, char *argv[]) &#123; sigset_t signals; signal(SIGINT, sigHandler); // 置空信号集 sigemptyset(&amp;signals); // 将所有位置1，与上个函数相反 // sigfillset(&amp;signals); // 添加和删除信号。 sigaddset(&amp;signals, SIGINT); sigaddset(&amp;signals, SIGQUIT); // SIGKILL 进程不能被阻塞和拦截，该处阻塞无效 sigaddset(&amp;signals, SIGKILL); // 将集合中的信号加入到阻塞信号集中 sigprocmask(SIG_BLOCK, &amp;signals, NULL); sigset_t pending; int i = 0; int j = 1; while (1) &#123; // 获取未决信号集 sigemptyset(&amp;pending); sigpending(&amp;pending); for (i = 1; i &lt; 32; i++) &#123; if (sigismember(&amp;pending, i) == 1) &#123; printf(\"1\"); &#125; else &#123; printf(\"0\"); &#125; &#125; puts(\"\"); if (j++ % 10 == 0) &#123; sigprocmask(SIG_UNBLOCK, &amp;signals, NULL); &#125; sleep(1); &#125; return 0;&#125; 编译运行按CTRL-C可以发送信号SIGINT。按CTRL-\\可以发送信号SIGQUIT。 发送完成可直接捕获到这两个信号。我们可以使用man手册查看信号部分 1man 7 signal 其中有一句话： The signals SIGKILL and SIGSTOP cannot be caught, blocked, or ignored. 表示SIGKILL和SIGSTOP信号不能被捕获。 再查看信号的详细信息： SIGKILL 9 Term Kill signal SIGTERM 15 Term Termination signal 不难发现 信号9是kill信号，直接杀死该进程，而kill默认发送的信号SIGTERM则是kill命令(例：kill 1234)则默认发送15号信号。 15号信号可以被捕获到，而我们的程序捕获到15号信号之后可以做一些回收资源、打印日志等操作。 但当接收到信号9的时候，则直接被杀死，不会来得及释放内存，打印日志等。 综合来看，我们应尽量使用kill而不是kill -9.","categories":[{"name":"C","slug":"C","permalink":"http://gmle.github.io/categories/C/"}],"tags":[{"name":"C","slug":"C","permalink":"http://gmle.github.io/tags/C/"},{"name":"Linux","slug":"Linux","permalink":"http://gmle.github.io/tags/Linux/"}]},{"title":"利用栈规则做就近括号匹配","slug":"利用栈规则做就近括号匹配","date":"2018-07-11T13:50:43.276Z","updated":"2018-07-11T13:53:48.751Z","comments":true,"path":"2018/07/11/利用栈规则做就近括号匹配/","link":"","permalink":"http://gmle.github.io/2018/07/11/利用栈规则做就近括号匹配/","excerpt":"利用栈规则做就近括号匹配","text":"利用栈规则做就近括号匹配 从第一个字符开始扫描当遇见普通字符时忽略，当遇见左括号时压入栈中当遇见右括号时从栈中弹出栈顶符号，并进行匹配匹配成功：继续读入下一个字符匹配失败：立即停止，并报错结束：成功: 所有字符扫描完毕，且栈为空失败：匹配失败或所有字符扫描完毕但栈非空 FILE1： 12345678910111213141516171819202122232425262728293031323334353637383940#ifndef _SEQSTACK_H#define _SEQSTACK_H#include&lt;stdio.h&gt;#include&lt;string.h&gt;#include&lt;stdlib.h&gt;#include &lt;stdbool.h&gt;#define MAX 1024struct SStack &#123; void *data[MAX]; //栈的数组 int m_Size; //栈大小&#125;;typedef void *SeqStack;//初始化栈SeqStack init_SeqStack();//入栈void push_SeqStack(SeqStack stack, void *data);//出栈void pop_SeqStack(SeqStack stack);//返回栈顶void *top_SeqStack(SeqStack stack);//返回栈大小int size_SeqStack(SeqStack stack);//判断栈是否为空bool isEmpty_SeqStack(SeqStack stack);//销毁栈void destroy_SeqStack(SeqStack stack);#endif //_SEQSTACK_H FILE2: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113#include&lt;stdio.h&gt;#include&lt;string.h&gt;#include&lt;stdlib.h&gt;#include \"SeqStack.h\"//初始化栈SeqStack init_SeqStack() &#123; struct SStack *myStack = malloc(sizeof(struct SStack)); if (myStack == NULL) &#123; return NULL; &#125; //初始化数组 memset(myStack-&gt;data, 0, sizeof(void *) * MAX); //初始化栈大小 myStack-&gt;m_Size = 0; return myStack;&#125;//入栈void push_SeqStack(SeqStack stack, void *data) &#123; //入栈本质 --- 数组尾插 if (stack == NULL) &#123; return; &#125; if (data == NULL) &#123; return; &#125; struct SStack *mystack = stack; if (mystack-&gt;m_Size == MAX) &#123; return; &#125; mystack-&gt;data[mystack-&gt;m_Size] = data; mystack-&gt;m_Size++;&#125;//出栈void pop_SeqStack(SeqStack stack) &#123; //出栈本质 --- 数组尾删 if (stack == NULL) &#123; return; &#125; struct SStack *mystack = stack; if (mystack-&gt;m_Size == 0) &#123; return; &#125; mystack-&gt;data[mystack-&gt;m_Size - 1] = NULL; mystack-&gt;m_Size--;&#125;//返回栈顶void *top_SeqStack(SeqStack stack) &#123; if (stack == NULL) &#123; return NULL; &#125; struct SStack *mystack = stack; if (mystack-&gt;m_Size == 0) &#123; return NULL; &#125; return mystack-&gt;data[mystack-&gt;m_Size - 1];&#125;//返回栈大小int size_SeqStack(SeqStack stack) &#123; if (stack == NULL) &#123; return -1; &#125; struct SStack *mystack = stack; return mystack-&gt;m_Size;&#125;//判断栈是否为空bool isEmpty_SeqStack(SeqStack stack) &#123; if (stack == NULL) &#123; return true;//返回-1代表真 空栈 &#125; struct SStack *mystack = stack; if (mystack-&gt;m_Size == 0) &#123; return true; &#125; return false; //返回0 代表 不是空栈&#125;//销毁栈void destroy_SeqStack(SeqStack stack) &#123; if (stack == NULL) &#123; return; &#125; free(stack); stack = NULL;&#125; FILE3: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980int isLeft(char ch) &#123; return ch == '(';&#125;int isRight(char ch) &#123; return ch == ')';&#125;static void test01() &#123; char *str = \"123*(was)s(sd)sdb(fs())())))(()\"; char *p = str; SeqStack *stack = init_SeqStack(); while (*p != '\\0') &#123; //如果是左括号，入栈 if (isLeft(*p)) &#123; push_SeqStack(stack, p); &#125; else if (isRight(*p)) &#123; if (size_SeqStack(stack) &gt; 0) &#123; pop_SeqStack(stack); &#125; else &#123; error(str, \"Error!\", p); break; &#125; &#125; p++; &#125; while (size_SeqStack(stack) &gt; 0) &#123; error(str, \"Error\", top_SeqStack(stack)); pop_SeqStack(stack); &#125; destroy_SeqStack(stack); stack = NULL; bool b = isEmpty_SeqStack(stack); printf(\"\\n=-=-=-=-=-=-=-====-=-=\\n%d\\n\",b);&#125;static void error(char *str, char *string, const char *p) &#123; printf(\"\\n%s\\n\", string); printf(\"%s\\n\", str); int num = (int) (p - str); for (int i = 0; i &lt; num; i++) &#123; printf(\" \"); &#125; printf(\"I\");&#125;int main(int argc, char *argv[]) &#123; test01(); bool b = true; printf(\"%d\\n\", b); return 0;&#125;","categories":[{"name":"C","slug":"C","permalink":"http://gmle.github.io/categories/C/"}],"tags":[{"name":"C","slug":"C","permalink":"http://gmle.github.io/tags/C/"}]},{"title":"Shell的加减乘除操作","slug":"Shell中的加减乘除","date":"2018-05-15T03:06:09.818Z","updated":"2018-10-18T01:26:55.838Z","comments":true,"path":"2018/05/15/Shell中的加减乘除/","link":"","permalink":"http://gmle.github.io/2018/05/15/Shell中的加减乘除/","excerpt":"写了这么长时间的Shell竟然不知道乘除怎么写。。。","text":"写了这么长时间的Shell竟然不知道乘除怎么写。。。 表达式1234((i=$j+$k)) i=`expr $j + $k`((i=$j-$k)) i=`expr $j -$k`((i=$j*$k)) i=`expr $j \\*$k`((i=$j/$k)) i=`expr $j /$k` let Let expressions 执行一个或多个表达式。表达式中的变量前不必有$. 如果表达式中包含了空格或其他特殊字符，则必须引起来。 123let “I = I + 1”let i=i+1 算术运算算术运算符指的是可以在程序中实现加、减、乘、除等数学运算的运算符。Shell中常用的数学运算符如下所示。 — +：对两个变量做加法。 — -：对两个变量做减法。 — *：对两个变量做乘法。 — /：对两个变量做除法。 — **：对两个变量做幂运算。 — %：取模运算，第一个变量除以第二个变量求余数。 — +=：加等于，在自身基础上加第二个变量。 — -=：减等于，在第一个变量的基础上减去第二个变量。 — *=：乘等于，在第一个变量的基础上乘以第二个变量。 — /=：除等于，在第一个变量的基础上除以第二个变量。 — %=：取模赋值，第一个变量对第二个变量取模运算，再赋值给第一个变量。 在使用这些运算符时，需要注意到运算顺序的问题。例如输入下面的命令，输出1+2的结果。 1echo 1+2 Shell并没有输出结果3，而是输出了1+2。在shell中有三种方法可以更改运算顺序。 用expr改变运算顺序。可以用echo expr 1 +2来输出1+2的结果，用expr表示后面的表达式为一个数学运算。需要注意的是，`并不是一个单引号，而是“Tab”键上面的那个符号。 用let指示数学运算。可以先将运算的结果赋值给变量b，运算命令是b=let 1 + 2。然后用echo$b来输出b的值。如果没有let，则会输出1+2。 用$[]表示数学运算。将一个数学运算写到$[]符号的中括号中，中括号中的内容将先进行数学运算。例如命令echo$[1+2]，将输出结果3。 下面是一个shell程序实例，实现数学函数S=3(xy)+4x2+5y+6的运算。在程序中以位置变量的方式输入x与y的值。程序的编写步骤如下所示。 123456789101112131415161718192021222324252627#!/bin/bash#4.4.shs=0 #定义一个求和变量，初值为0。t=`expr$1**$2` #用expr改变运算顺序，求x的y次方。t=$[t*3] #t乘以3。s=$[s+t] #结果相加。t=$[$1**2] #求x的平方。t=$[t*4] #结果乘以4。s=$[s+t] #结果相加。t=`expr$2*5` #求5y的值。s=$[s+t] #结果相加。s=$[s+6] #结果加上6。echo$s #输出结果。echo$((a%b)) #取余 在这个程序中，需要注意算术运算的写法。如果没有expr或$[]更改运算顺序，则会将运算式以字符串的形式赋值，而不会赋值运算结果。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://gmle.github.io/categories/Linux/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://gmle.github.io/tags/Shell/"},{"name":"Linux","slug":"Linux","permalink":"http://gmle.github.io/tags/Linux/"}]},{"title":"不同编译器下的差异","slug":"不同编译器对C++的一些影响","date":"2018-03-16T10:21:43.000Z","updated":"2018-07-21T13:54:52.341Z","comments":true,"path":"2018/03/16/不同编译器对C++的一些影响/","link":"","permalink":"http://gmle.github.io/2018/03/16/不同编译器对C++的一些影响/","excerpt":"在学习C++的时候遇到了一些问题。在不同编译器下 某些代码无法实现。","text":"在学习C++的时候遇到了一些问题。在不同编译器下 某些代码无法实现。 在Mac 默认的编译器 clang下：123456789101112131415161718192021#include &lt;iostream&gt;using namespace std;struct TestType&#123; int data;&#125;;int main(int argc, char *argv[]) &#123; const TestType x = &#123;20&#125;; TestType *x1 = const_cast&lt;TestType*&gt;(&amp;x); x1-&gt;data = 200; cout &lt;&lt; x.data &lt;&lt; endl; cout &lt;&lt; x1-&gt;data&lt;&lt; endl; cout &lt;&lt; \"Hello world\"&lt;&lt; endl; return 0;&#125; 编译通过 ，但是运行的时候会出问题。 检查原因，运行到赋值的地方：1x1-&gt;data = 200; 程序会退出。 但是在msvc等其他编译器下执行则不会出现问题。 其实，我们用const时为了提醒一下自己这个值是个常量，不要动了。但是事实证明 const 常量也是可以改变值的。 但是这就完全没有必要了。","categories":[{"name":"Cpp","slug":"Cpp","permalink":"http://gmle.github.io/categories/Cpp/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://gmle.github.io/tags/Cpp/"}]},{"title":"关于Spark环境变量问题","slug":"关于Spark环境变量问题","date":"2017-11-06T08:44:11.000Z","updated":"2017-11-06T08:44:11.000Z","comments":true,"path":"2017/11/06/关于Spark环境变量问题/","link":"","permalink":"http://gmle.github.io/2017/11/06/关于Spark环境变量问题/","excerpt":"记录一次因为spark内置环境问题引发的惨案","text":"记录一次因为spark内置环境问题引发的惨案 问题：Spark在spark-env.sh中的环境变量不生效 错误日志 Terminal12345678[root@ceshi3 sbin]# ./start-slaves.sh - /usr/local/spark-1.6.0-bin-hadoop2.6/conf/spark-env.sh: line 9: export: `/usr/local/spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar': not a valid identifier- ceshi3: /usr/local/spark-1.6.0-bin-hadoop2.6/conf/spark-env.sh: line 9: export: `/usr/local/spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar': not a valid identifier- ceshi3: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-ceshi3.out- ceshi3: failed to launch org.apache.spark.deploy.worker.Worker:- ceshi3: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/spark-class: line 87: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/java: 没有那个文件或目录- ceshi3: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/spark-class: line 87: exec: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/java: cannot execute: 没有那个文件或目录- ceshi3: full log in /usr/local/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-ceshi3.out 发现启动worker的时候会出现错误：12- ceshi3: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/spark-class: line 87: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/java: 没有那个文件或目录- ceshi3: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/spark-class: line 87: exec: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/java: cannot execute: 没有那个文件或目录 这个bin/java明明是$ JAVA_HOME 的，为什么会变为 $SPARK_HOME 呢 既然启动报错，而且报的是 $JAVA_HOME，那就要看几个东西:一个是正常的系统变量配置，再一个就是在要启动的服务里是否使用了这个配置变量，再确认下自己的配置是否已经有了。 查看 spark 关于环境变量的配置文件1234567891011export SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=dk31:2181,dk32:2181,dk34:2181 -Dspark.deploy.zookeeper.dir=/spark\"export JAVA_HOME=$&#123;JAVA_HOME&#125;export HADOOP_HOME=/usr/local/hadoop-2.6.0export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport SCALA_HOME=/usr/local/scala-2.10.4export SPARK_WORKER_MEMORY=4gexport SPARK_EXECUTOR_MEMORY=2gexport SPARK_DRIVER_MEMORY=1gexport SPARK_WORKER_CORES=4export SPARK_CLASSPATH=/usr/local/spark-1.6.0-bin-hadoop2.6/lib/mysql-connector-java.jarexport SPARK_CLASSPATH=$SPARK_CLASSPATH:$CLASSPATH 发现 $JAVA_HOME 变量是取的系统变量·，但是系统变量为什么取不到？ 查了下：在脚本中使用export, 只在脚本中有效，退出这个脚本，设置的变量就没有了。由于spark-class使用了 spark-env.sh 在使用的时候 已经取不到该值，所以无效了。但是想不通为什么会变成 $SPARK_HOME 的变量","categories":[{"name":"Spark","slug":"Spark","permalink":"http://gmle.github.io/categories/Spark/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://gmle.github.io/tags/Shell/"},{"name":"Spark","slug":"Spark","permalink":"http://gmle.github.io/tags/Spark/"},{"name":"Linux","slug":"Linux","permalink":"http://gmle.github.io/tags/Linux/"}]},{"title":"Spark-HA的worker问题","slug":"Spark-HA的worker问题","date":"2017-11-06T08:39:00.000Z","updated":"2017-11-06T08:39:00.000Z","comments":true,"path":"2017/11/06/Spark-HA的worker问题/","link":"","permalink":"http://gmle.github.io/2017/11/06/Spark-HA的worker问题/","excerpt":"关于 HA 中 Spark worker节点连接Master的问题","text":"关于 HA 中 Spark worker节点连接Master的问题 问题：Spark Woker 不去连接ALIVE Master机器： 192.168.1.128 Master 192.168.1.129 Master Worker 192.168.1.130 Worker 启动时两个Master的状态不可控，不知道哪个是ALIVE的Master，worker节点在连接Master的时候，会判断当前Master的状态是否为ALIVE，如果为StandBy，则不继续链接，然后去寻找ALIVE，直到找到ALIVE节点的MASTER。 现在的问题是 Worker在找到StandBy节点后，并没有去寻找新的Master，导致了worker注册不到集群上，自动关闭。 原因待定。 根据一些帖子发现，如果配置了Spark on yarn ，则 Spark HA 基本没有任何作用。 错误日志 Terminal1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties17/10/09 13:05:08 INFO Worker: Registered signal handlers for [TERM, HUP, INT]17/10/09 13:05:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable17/10/09 13:05:09 INFO SecurityManager: Changing view acls to: root17/10/09 13:05:09 INFO SecurityManager: Changing modify acls to: root17/10/09 13:05:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)17/10/09 13:05:10 INFO Utils: Successfully started service 'sparkWorker' on port 39766.17/10/09 13:05:10 INFO Worker: Starting Spark worker 192.168.10.129:39766 with 4 cores, 4.0 GB RAM17/10/09 13:05:10 INFO Worker: Running Spark version 1.6.017/10/09 13:05:10 INFO Worker: Spark home: /opt/dkh/spark-1.6.0-bin-hadoop2.617/10/09 13:05:11 INFO Utils: Successfully started service 'WorkerUI' on port 8081.17/10/09 13:05:11 INFO WorkerWebUI: Started WorkerWebUI at http://192.168.10.129:808117/10/09 13:05:11 INFO Worker: Connecting to master dkm:7077...17/10/09 13:05:11 WARN Worker: Failed to connect to master dkm:7077java.io.IOException: Failed to connect to dkm/192.168.10.128:7077 at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216) at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167) at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200) at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187) at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: java.net.ConnectException: 拒绝连接: dkm/192.168.10.128:7077 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) ... 1 more17/10/09 13:05:24 INFO Worker: Retrying connection to master (attempt # 1)17/10/09 13:05:24 INFO Worker: Connecting to master dkm:7077...17/10/09 13:05:37 INFO Worker: Retrying connection to master (attempt # 2)17/10/09 13:05:37 INFO Worker: Connecting to master dkm:7077...17/10/09 13:05:50 INFO Worker: Retrying connection to master (attempt # 3)17/10/09 13:05:50 INFO Worker: Connecting to master dkm:7077...17/10/09 13:06:03 INFO Worker: Retrying connection to master (attempt # 4)17/10/09 13:06:03 INFO Worker: Connecting to master dkm:7077...17/10/09 13:06:16 INFO Worker: Retrying connection to master (attempt # 5)17/10/09 13:06:16 INFO Worker: Connecting to master dkm:7077...17/10/09 13:06:29 INFO Worker: Retrying connection to master (attempt # 6)17/10/09 13:06:29 INFO Worker: Connecting to master dkm:7077...17/10/09 13:07:47 INFO Worker: Retrying connection to master (attempt # 7)17/10/09 13:07:47 INFO Worker: Connecting to master dkm:7077...17/10/09 13:09:05 INFO Worker: Retrying connection to master (attempt # 8)17/10/09 13:09:05 INFO Worker: Connecting to master dkm:7077...17/10/09 13:10:23 INFO Worker: Retrying connection to master (attempt # 9)17/10/09 13:10:23 INFO Worker: Connecting to master dkm:7077...17/10/09 13:11:41 INFO Worker: Retrying connection to master (attempt # 10)17/10/09 13:11:41 INFO Worker: Connecting to master dkm:7077...17/10/09 13:12:59 INFO Worker: Retrying connection to master (attempt # 11)17/10/09 13:12:59 INFO Worker: Connecting to master dkm:7077...17/10/09 13:14:17 INFO Worker: Retrying connection to master (attempt # 12)17/10/09 13:14:17 INFO Worker: Connecting to master dkm:7077...17/10/09 13:15:35 INFO Worker: Retrying connection to master (attempt # 13)17/10/09 13:15:35 INFO Worker: Connecting to master dkm:7077...17/10/09 13:16:53 INFO Worker: Retrying connection to master (attempt # 14)17/10/09 13:16:53 INFO Worker: Connecting to master dkm:7077...17/10/09 13:18:11 INFO Worker: Retrying connection to master (attempt # 15)17/10/09 13:18:11 INFO Worker: Connecting to master dkm:7077...17/10/09 13:19:29 INFO Worker: Retrying connection to master (attempt # 16)17/10/09 13:19:29 INFO Worker: Connecting to master dkm:7077...17/10/09 13:20:47 ERROR Worker: All masters are unresponsive! Giving up. 既然如此，那干脆不启动第二个Master，Start-all 后，会发现集群正常，但是没有第二个Master。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://gmle.github.io/categories/Spark/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://gmle.github.io/tags/Shell/"},{"name":"Spark","slug":"Spark","permalink":"http://gmle.github.io/tags/Spark/"},{"name":"Linux","slug":"Linux","permalink":"http://gmle.github.io/tags/Linux/"}]},{"title":"Maven一键安装 centos平台","slug":"yum一键安装maven","date":"2017-09-12T02:48:22.000Z","updated":"2017-09-12T02:48:22.000Z","comments":true,"path":"2017/09/12/yum一键安装maven/","link":"","permalink":"http://gmle.github.io/2017/09/12/yum一键安装maven/","excerpt":"maven一键安装","text":"maven一键安装 添加maven的仓库1wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo yum 安装1yum -y install apache-maven","categories":[{"name":"linux","slug":"linux","permalink":"http://gmle.github.io/categories/linux/"}],"tags":[{"name":"Centos","slug":"Centos","permalink":"http://gmle.github.io/tags/Centos/"},{"name":"maven","slug":"maven","permalink":"http://gmle.github.io/tags/maven/"}]},{"title":"Bash/Shell调用MySQL并忽略警告","slug":"Bash调用MySql","date":"2017-06-25T09:26:19.000Z","updated":"2017-06-25T09:26:19.000Z","comments":true,"path":"2017/06/25/Bash调用MySql/","link":"","permalink":"http://gmle.github.io/2017/06/25/Bash调用MySql/","excerpt":"Shell对MySQL的调用与脚本中如何写 Shell脚本如何搞定 MySQL的增删改查","text":"Shell对MySQL的调用与脚本中如何写 Shell脚本如何搞定 MySQL的增删改查 用Shell对mysql操作非常的简单我们利用 mysql 命令去操作数据库里面的所有东西。 shell脚本1234567891011121314# 一坨一坨的运行：mysql -uroot -p123456 -e \"select * from tmp_test where tmp_name = 'a';select * from tmp_test where tmp_name = 'b';select * from tmp_test where tmp_id = 1;select tmp_name from tmp_test where tmp_id = 2;quit\"# 赋值：id=$(mysql -uroot -p123456 -e \"SELECT tmp_id from tmp_test WHERE tmp_name = 'a';\")# 会发现还有字段名字，加参数去掉字段名，只保留我们要查询的：id=$(mysql -uroot -p123456 -Bse \"SELECT tmp_id from tmp_test WHERE tmp_name = 'a';\") 过后我们会发现每次查询之后会出现警告，每次都出：1mysql: [Warning] Using a password on the command line interface can be insecure. MySQL 版本 5.6+ 的安全策略MySQL5.6版本向上有一个密码安全问题，即在命令行输入密码会出现警告： 12mysql -uroot -proot mysql: [Warning] Using a password on the command line interface can be insecure. 读取配置文件的参数也不可以，这样我们 需要指定一个mysql的配置文件作为mysql的配置输入进去： cnf配置文件my.cnf123#!/bin/bash[mysql]password=root 然后再在脚本中调用：123#!/bin/bash# 继续赋值，这样就不会出现警告信息：id=$(mysql --defaults-file=./my.cnf -uroot -Bse \"SELECT tmp_id from tmp_test WHERE tmp_name = 'a';\")","categories":[{"name":"Shell","slug":"Shell","permalink":"http://gmle.github.io/categories/Shell/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://gmle.github.io/tags/Shell/"},{"name":"Bash","slug":"Bash","permalink":"http://gmle.github.io/tags/Bash/"},{"name":"MySQL","slug":"MySQL","permalink":"http://gmle.github.io/tags/MySQL/"}]},{"title":"在Centos6.5下升级python至3.6.0","slug":"centos6.5下python2.6.6升级至3.6","date":"2017-05-11T07:57:48.000Z","updated":"2017-05-11T07:57:48.000Z","comments":true,"path":"2017/05/11/centos6.5下python2.6.6升级至3.6/","link":"","permalink":"http://gmle.github.io/2017/05/11/centos6.5下python2.6.6升级至3.6/","excerpt":"此次升级保留旧版本的环境。","text":"此次升级保留旧版本的环境。 配置系统环境安装开发工具1yum groupinstall -y developement 安装python3解码支持包1yum install -y zlib-devel openssl-devel sqlite-devel bzip2-devel 准备更新版本验证原有的python版本1python -V python 2.6.6 下载python3.6.0包1wget http://www.python.org/ftp/python/3.6.0/Python-3.6.0.tar.xz 解压编译python安装包解压12xz -d Python-3.6.0.tar.xztar -xvf Python-3.6.0.tar 编译123cd Python-3.6.0# 配置安装路径./configure --prefix=/usr/local 如果出现编译错误可能是因为gcc gcc-c++版本太低或者未安装，使用代码 1yum -y install gcc gcc-c++ 进行安装，然后重新编译./configure 执行安装1make &amp;&amp; make altinstall 建立软连接(就是快捷方式)12mv /usr/bin/python /usr/bin/python2.6.6 ##你的python版本可能不同ln -s /usr/local/bin/python3.6 /usr/bin/python 重新验证python版本， 1python -V python3.6.0 yum指令会报错，将其重新指向旧版本的python1vi /usr/bin/yum 将文件的头部#！/usr/bin/python改为#！/usr/bin/python2.6.6 安装新pip1$ wget https://pypi.python.org/packages/source/p/pip/pip-1.3.1.tar.gz --no-check-certificate 解压安装pip1234chmod +x pip-1.3.1.tar.gztar xzvf pip-1.3.1.tar.gzcd pip-1.3.1python setup.py install 查看pip安装1pip -V pip 1.3.1 from /usr/local/lib/python3.6/site-packages/pip-1.3.1-py3.6.egg (python 3.6)","categories":[{"name":"Centos","slug":"Centos","permalink":"http://gmle.github.io/categories/Centos/"}],"tags":[{"name":"Centos","slug":"Centos","permalink":"http://gmle.github.io/tags/Centos/"},{"name":"Python","slug":"Python","permalink":"http://gmle.github.io/tags/Python/"}]},{"title":"Hadoop_玩转 HDFS之 ACL","slug":"玩转HDFS-ACL","date":"2017-05-03T06:38:39.000Z","updated":"2017-05-03T06:38:39.000Z","comments":true,"path":"2017/05/03/玩转HDFS-ACL/","link":"","permalink":"http://gmle.github.io/2017/05/03/玩转HDFS-ACL/","excerpt":"Hadoop从2.4.0版本开始支持hdfs的ACL， 通俗的讲就是文件访问控制权限 下面对其进行一些测试： unnamed user (file owner) 文件的拥有者 unnamed group (file group) 文件的所属组 named user 除了文件的拥有者和拥有组之外，的其它用户 named group 除了文件的拥有者和拥有组之外，的其它用户 mask 权限掩码，用于过滤named user和named group的权限","text":"Hadoop从2.4.0版本开始支持hdfs的ACL， 通俗的讲就是文件访问控制权限 下面对其进行一些测试： unnamed user (file owner) 文件的拥有者 unnamed group (file group) 文件的所属组 named user 除了文件的拥有者和拥有组之外，的其它用户 named group 除了文件的拥有者和拥有组之外，的其它用户 mask 权限掩码，用于过滤named user和named group的权限 一、启用ACL启用ACL功能 修改hdfs-site.xml 增加如下属性 开启ACL12345678&lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 修改core-site.xml 设置用户组默认权限.1234&lt;property&gt; &lt;name&gt;fs.permissions.umask-mode&lt;/name&gt; &lt;value&gt;002&lt;/value&gt;&lt;/property&gt; 一个访问控制列表（ACL）是一组ACL词目(entries)的集合，每个ACL词目会指定一个用户/组，并赋予读/写/执行上等权限。例如：123456user::rw- user:bruce:rwx #effective:r-- group::r-x #effective:r-- group:sales:rwx #effective:r-- mask::r-- other::r-- 这里面，没有命名的用户/组即该文件的基本所属用户/组。每一个ACL都有一个掩码(mask)，如果用户不提供掩码，那么该掩码会自动根据所有ACL条目的并集来获得(属主除外）。在该文件上运行chmod会改变掩码的权限。由于掩码用于过滤，这有效地限制了权限的扩展ACL条目，而不是仅仅改变组条目，并可能丢失的其他扩展ACL条目。 定义默认 （default）ACL条目，新的子文件和目录会自动继承默认的ACL条目设置，而只有目录会有默认的ACL条目。例如： 123456789user::rwx group::r-x other::r-x default:user::rwx default:user:bruce:rwx #effective:r-x default:group::r-x default:group:sales:rwx #effective:r-x default:mask::r-x default:other::r-x ACL相关的文件API：1234567891011public void modifyAclEntries(Path path, List aclSpec) throws IOException;public void removeAclEntries(Path path, List aclSpec) throws IOException;public void public void removeDefaultAcl(Path path) throws IOException;public void removeAcl(Path path) throws IOException;public void setAcl(Path path, List aclSpec) throws IOException;public AclStatus getAclStatus(Path path) throws IOException; 命令行命令：显示文件和目录的访问控制列表。如果一个目录有默认的ACL，getfacl也可以显示默认的ACL设置。1hdfs dfs -getfacl [-R] path 设置文件和目录的ACL1hdfs dfs -setfacl [-R] [-b|-k -m|-x acl_spec path]|[--set acl_spec path] 12345-R: Use this option to recursively list ACLs for all files and directories.-b: Revoke all permissions except the base ACLs for user, groups and others.-k: Remove the default ACL.-m: Add new permissions to the ACL with this option. Does not affect existing permissions.-x: Remove only the ACL specified. 当ls的权限位输出以+结束时，那么该文件或目录正在启用一个ACL。1hdfs dfs -ls args 实际使用：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# 默认只有基本的权限控制hdfs dfs -getfacl /data# file: /data# owner: hive# group: hadoopuser::rwxgroup::r-xother::r-x#递归显示/data下所有文件的ACLhdfs dfs -getfacl -R /data# file: /data# owner: hive# group: hadoopuser::rwxgroup::r-xother::r-x# file: /data/test.zero# owner: hive# group: hadoopuser::rw-group::r--other::r--# file: /data/test.zero.2# owner: hive# group: hadoopuser::rw-group::r--other::r--#添加一个用户ACL条目hdfs dfs -setfacl -m user:hbase:rw- /data/test.zero#添加一个组ACL条目和一个用户ACL条目（如果设置一个未命名条目，可以用user::r-x，group::r-w或者other::r-x等来设置）hdfs dfs -setfacl -m group:crm:--x,user:app1:rwx /data/test.zero.2#移除一个ACL条目hdfs dfs -setfacl -x user:app1 /data/test.zero.2#“+”已开启了ACL功能hdfs dfs -ls -R /data-rw-rwxr--+ 3 hive hadoop 1073741824 2014-12-21 15:32 /data/test.zero-rw-r-xr--+ 3 hive hadoop 1073741824 2014-12-21 15:50 /data/test.zero.2# 查看当前ACL，此时mask已经被生成hdfs dfs -getfacl -R /data/test.zero.2# file: /data/test.zero.2# owner: hive# group: hadoopuser::rw-group::r--group:crm:--xmask::r-xother::r--hdfs dfs -getfacl /data/test.zero.2# 为data目录添加default权限hdfs dfs -setfacl -m default:user:debugo:rwx /datahdfs dfs -mkdir /data/d1hdfs dfs -getfacl /data/d1user::rwxuser:debugo:rwx #effective:r-xgroup::r-xmask::r-xother::r-xdefault:user::rwxdefault:user:debugo:rwxdefault:group::r-xdefault:mask::rwxdefault:other::r-x#可以看出，default虽然继承给了d1，但是被mask=r-x所过滤，所以这里还需要设置mask。此时debugo用户的权限可以被正常访问。hdfs dfs -setfacl -m mask::rwx /data/d1hdfs dfs -getfacl /data/d1# file: /data/d1# owner: hdfs# group: hadoopuser::rwxuser:debugo:rwxgroup::r-xmask::rwxother::r-x","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://gmle.github.io/tags/HDFS/"},{"name":"ACL","slug":"ACL","permalink":"http://gmle.github.io/tags/ACL/"}]},{"title":"MacOS下配置Hadoop和Spark","slug":"MacOS安装Hadoop&Spark","date":"2017-05-03T06:37:38.000Z","updated":"2017-05-03T06:37:38.000Z","comments":true,"path":"2017/05/03/MacOS安装Hadoop&Spark/","link":"","permalink":"http://gmle.github.io/2017/05/03/MacOS安装Hadoop&Spark/","excerpt":"首先，准备MacOS环境略过Java、Scala、Python的环境安装，从Hadoop和Spark说起","text":"首先，准备MacOS环境略过Java、Scala、Python的环境安装，从Hadoop和Spark说起 安装Hadoop安装Hadoop，最简单的安装方式：1brew install hadoop 找到安装目录安装完成后，找到Hadoop配置文件目录： 1cd /usr/local/Cellar/hadoop/2.7.3/libexec/etc/hadoop 修改core-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/Cellar/hadoop/2.7.3/libexec/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:8020&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 修改hdfs-site.xml1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/Cellar/hadoop/2.7.3/libexec/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/Cellar/hadoop/2.7.3/libexec/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 添加环境变量123#Hadoop environment configs export HADOOP_HOME=/usr/local/Cellar/hadoop/2.7.3/libexec export PATH=$PATH:$&#123;HADOOP_HOME&#125;/bin 格式化HDFS12cd /usr/local/Cellar/hadoop/2.7.3/bin ./hdfs namenode -format 启动Hadoop12cd /usr/local/Cellar/hadoop/2.7.3/sbin ./start-all.sh 在终端输入 jps 查看java进程1231206 DataNode 1114 NameNode 1323 SecondaryNameNode 安装SparkSpark的安装也是使用 brew1brew install apache-spark 找到安装目录找到Spark配置文件目录1cd /usr/local/Cellar/apache-spark/2.1.0/libexec/conf 修改spark-env.sh1234cp spark-env.sh.template spark-env.shvi spark-env.shexport SPARK_HOME=/usr/local/Cellar/apache-spark/2.1.0/libexec export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_102.jdk/Contents/Home 加入环境变量12export SPARK_HOME=/usr/local/Cellar/apache-spark/2.1.0/libexec export PATH=$PATH:$&#123;SPARK_HOME&#125;/bin 启动Spark12cd /usr/local/Cellar/apache-spark/1.6.0/bin ./start-all.sh 查看进程12345678910jps6052 Worker6022 Master6728 Jps5546 NameNode5739 SecondaryNameNode5947 NodeManager5630 DataNode5855 ResourceManager 配置Pycharm开发spark应用打开Pycharm（我的python版本是2.7）新建xxxx，新建类：一个简单的wordcount12345678910from pyspark import SparkContextlogFile = \"/Users/admin/Desktop/BackUp\"sc = SparkContext(\"local\",\"Simple App\")logData = sc.textFile(logFile).cache()numAs = logData.filter(lambda s: 'a' in s).count()numBs = logData.filter(lambda s: 'b' in s).count()print(\"Lines with a: %i, lines with b: %i\"%(numAs, numBs)) F4打开当前可运行代码的配置项Environment Variables 选项填写：1PYTHONPATH /usr/local/Cellar/apache-spark/2.1.0/libexec/python 至此，环境完成。","categories":[{"name":"MacOS","slug":"MacOS","permalink":"http://gmle.github.io/categories/MacOS/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"MacOS","slug":"MacOS","permalink":"http://gmle.github.io/tags/MacOS/"},{"name":"Spark","slug":"Spark","permalink":"http://gmle.github.io/tags/Spark/"}]}]}