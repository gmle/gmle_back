{"pages":[{"title":"关于","date":"2018-03-16T09:42:21.328Z","updated":"2018-03-16T09:42:21.328Z","comments":true,"path":"about/index.html","permalink":"http://gmle.github.io/about/index.html","excerpt":"","text":"Info 性别 x 爱好 x #1x。"},{"title":"简历","date":"2018-03-16T09:43:06.258Z","updated":"2018-03-16T09:43:06.258Z","comments":true,"path":"resume/index.html","permalink":"http://gmle.github.io/resume/index.html","excerpt":"","text":"联系方式 手机：(全天) 18515666314 13623372344 Email： e.le.lee.leee@outlook.com e.le.lee.leee@gmail.com 网络联系方式： 企鹅：1229527246 微信：wl1229527246 个人信息 王乐/男/1996 专科/河北软件学院软件工程系软件开发与设计方向 - Java 我的博客：gmle.github.io 期望职位：Hadoop开发工程师、Spark开发工程师、大数据平台运维工程师。 期望城市：任何 工作经历廊坊市大华夏神农科技有限公司 （ 2015年11月 ~ 2015年12月 ）项目：三省农场 担任职位：信息采集，设备测试，源代码测试，服务器调试。 主要工作内容：参与需求信息调研和设计、测试数据、执行测试、提交和跟踪缺陷. 大快搜索（北京）数据技术有限公司 （ 2016年06月 ~ 至今 ）项目：DKH大快大数据平台 担任职位：开发，测试，运维。 项目涵盖：Hadoop HA、HBase HA、Zookeeper Cluster、Spark HA、Flume、Strom、Hive、Kafka Cluster、ElasticSearch Cluster、Sqoop、分布式MySQL 主要工作内容：设计各个组件之间的耦合度，着手自动化数据平台搭建，利用Bash进行平台的自动化配置各种组件.并进行自动化测试。项目：FreeRCH（DKH平台基础框架） 担任职位：开发，测试。 项目涵盖：NLP模块、数据导入导出模块、机器学习模块 主要工作内容：开发以DKH为基础的大数据框架，整合DKH与FreeRCH的关系，并使其具有DKH唯一性（别的平台不可用）。熟悉常用分类聚类机器学习算法（朴素贝叶斯、K-Means等）。 技能清单以下均为我熟练使用的技能 Web开发：JavaWeb Web框架：Spring/SpringMVC/ 服务器的安装、调试、命令。 Hadoop、Spark配置调优、简单的Hive配置调优，丰富的集群调错能力 Hadoop、Spark、Hive、HBase、Zookeeper的使用 常规Hive HQL编写 版本管理：Git/Svn 系统：Win/Linux/MacOS Docker 以下是我基本掌握的技能并不断的深入 Python2 Scala CDH 自我评价本人具有比较强的专业理论知识,基础扎实，涉及广泛，为人真诚乐观自信，勤奋务实，责任感强，热爱集体，助人为乐，能恪守以大局为重的原则，愿意服从集体利益的需要，具备奉献精神。爱好专研新技术，拥有较强的自学能力，能快速学习新技术,特别爱喜欢有挑战的项目,并从中快速进步，适应出差。 致谢感谢您花时间阅读我的简历，期待能有机会和您共事。"},{"title":"标签","date":"2016-04-22T04:39:04.000Z","updated":"2018-03-16T09:45:57.393Z","comments":false,"path":"tags/index.html","permalink":"http://gmle.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2016-04-22T04:39:04.000Z","updated":"2018-03-16T09:42:52.000Z","comments":false,"path":"categories/index.html","permalink":"http://gmle.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"软技能：代码之外的生存指南","slug":"软技能：代码之外的生存指南","date":"2018-03-16T09:51:31.129Z","updated":"2018-03-28T14:58:08.970Z","comments":true,"path":"2018/03/16/软技能：代码之外的生存指南/","link":"","permalink":"http://gmle.github.io/2018/03/16/软技能：代码之外的生存指南/","excerpt":"","text":"读者续： 很久之前买的这本书了，一直到我辞去我的第一份工作后，整理书架的时候看到它。这是我众多收藏的书中，唯一一本“非技术”的书。当初并未在意，甚至不知道为什么要去买它。 这本书并未讲述一些技术性的东西，所以我真正拿起这本书的时候，我竟然不是去翻阅目录看这本书到底讲了什么，并没有一点要去获取新的知识（框架/算法/语言）的迫切感。 谨以此续当做我开始读此书的开篇。","categories":[{"name":"读万卷书","slug":"读万卷书","permalink":"http://gmle.github.io/categories/读万卷书/"}],"tags":[{"name":"Books","slug":"Books","permalink":"http://gmle.github.io/tags/Books/"}]},{"title":"C++基本类中的构造函数，析构函数，拷贝构造器和运算符重载","slug":"C++构造析构拷贝构造运算符重载","date":"2017-12-18T02:56:52.978Z","updated":"2017-12-18T09:39:05.000Z","comments":true,"path":"2017/12/18/C++构造析构拷贝构造运算符重载/","link":"","permalink":"http://gmle.github.io/2017/12/18/C++构造析构拷贝构造运算符重载/","excerpt":"这可能是一道笔试题。","text":"这可能是一道笔试题。 123456789101112class Test&#123;public: Test() = default; virtual ~Test() = 0; Test(const Test &amp;another); Test &amp;operator=(const Test &amp;another);&#125;;int main(int argc, char *argv[])&#123; return 0;&#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://gmle.github.io/tags/HDFS/"}]},{"title":"不同编译器下的差异","slug":"不同编译器对C++的一些影响","date":"2017-11-27T06:59:16.289Z","updated":"2018-03-16T10:21:43.457Z","comments":true,"path":"2017/11/27/不同编译器对C++的一些影响/","link":"","permalink":"http://gmle.github.io/2017/11/27/不同编译器对C++的一些影响/","excerpt":"在学习C++的时候遇到了一些问题。在不同编译器下 某些代码无法实现。","text":"在学习C++的时候遇到了一些问题。在不同编译器下 某些代码无法实现。 在Mac 默认的编译器 clang下：123456789101112131415161718192021#include &lt;iostream&gt;using namespace std;struct TestType&#123; int data;&#125;;int main(int argc, char *argv[]) &#123; const TestType x = &#123;20&#125;; TestType *x1 = const_cast&lt;TestType*&gt;(&amp;x); x1-&gt;data = 200; cout &lt;&lt; x.data &lt;&lt; endl; cout &lt;&lt; x1-&gt;data&lt;&lt; endl; cout &lt;&lt; \"Hello world\"&lt;&lt; endl; return 0;&#125; 编译通过 ，但是运行的时候会出问题。 检查原因，运行到赋值的地方：1x1-&gt;data = 200; 程序会退出。 但是在msvc等其他编译器下执行则不会出现问题。 其实，我们用const时为了提醒一下自己这个值是个常量，不要动了。但是事实证明 const 常量也是可以改变值的。 但是这就完全没有必要了。","categories":[{"name":"C++","slug":"C","permalink":"http://gmle.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://gmle.github.io/tags/C/"}]},{"title":"C++ Primer 读书笔记","slug":"cpp_primer","date":"2017-10-25T06:21:39.585Z","updated":"2018-03-09T01:05:51.000Z","comments":true,"path":"2017/10/25/cpp_primer/","link":"","permalink":"http://gmle.github.io/2017/10/25/cpp_primer/","excerpt":"做点什么吧","text":"做点什么吧 第一章 开始 最近感觉自己很浮躁，看看书让自己静下心。 我之前学过Java，对于C++这种面向对象的语言也有一定的了解 之前也模模糊糊的看过一点C++的东西，也不全面，\b我现在尝试以一个没有学过c语言的人去学习C++，并且拜读圣经，希望能得到不错的成果。 本书采用的C++版本为 C++11 编写一个简单的C++程序 \b就像Java有程序入口一样，C++程序也有入口，它们的入口函数都是 main 函数。 在执行成程序的时候，系统会调用 main 来运行程序。 \bExample：123int main&#123; return 0;&#125; 我保留了Java的书写习惯，将起始的大括号放在了 main 函数的一行，当然我感觉这样写好看一些吧（也可能是习惯）。 这是一个C++里最简单的函数，这段代码的作用是返回给操作系统一个值：0 。 C++函数构成包括四个部分： 返回类型 main函数的返回值类型必须为 int，即整数类型。 int类型是一种内置类型。 函数名 主函数名字为main函数，自定义的函数名字可以自行取。 参数列表 本例中没有带有任何参数。 函数体 大括号括起来的语句块即为函数体，此函数体中只包含一条语句。 此语句是结束词函数的执行，并向调用者返回一个值；此返回值类型必须与函数的返回值类型相同 重要概念：类型 因为Java是从C++演变而来，所以对于类型也有很深的认识，因为没有学过C++，没有具体了解到C++的所有类型，不方便发表自己的看法。(其实类型好像没什么看法) 类型是程序设计的\b\b最基本的概念之一，一种类型不仅仅定义了数据元素的内容，还定义了这类数据上可以进行的运算 程序所处理的数据都保存在变量中，而每个变量都有自己的类型。 编译、运行程序编写好程序之后 我们就需要去编译它。 编译环境我用的是CMake；CMake的使用，我参考了这份资料CMake实践 输入输出C++并没有定义输入输出语句，但是它有一个全面的标准库(std)来提供IO机制以及其他操作。 标准输入输出对象本示例使用 iostream 库， iostream 库中包含两个基础类型 istream 和 ostream，分别表示输入流和输出流。 ‘流’想要表达的是：随着时间的推移，自复式顺序生成或者消耗的。 标准库定义了四个IO对象。 为了处理输入，我们使用一个\b名为cin的istream类型的对象。这个对象成为标准输入。 为了处理输出，我们使用一个\b名为cout的ostream类型的对象。这个对象成为标准输出。 为了处理警告和错误消息，我们使用一个\b名为cerr的ostream类型的对象，我们称之为标准错误 clog则用来输出程序运行时的一般性信息。 一个使用 IO 库的程序​1234567891011#include (iostream) //告诉编译器我们要使用的库为 iostreamint main()&#123; std::cout &lt;&lt; \" Enter two numbers: \"&lt;&lt;std::endl; int v1 = 0, v2 = 0; std::cin &gt;&gt; v1 &gt;&gt; v2; std::cout &lt;&lt; \"The sum of \" &lt;&lt; v1 &lt;&lt; \" and \" &lt;&lt; v2 &lt;&lt; \" is \" &lt;&lt;v1+v2 &lt;&lt; \" .\" &lt;&lt;std::endl; return 0;&#125; #include (iostream) # 告诉编译器我们要使用的库为 iostream std指代库iostream中的命名空间。 endl 则被称之为 操纵符 的特殊值，写入endl的效果是结束当前行。并将与设备关联的缓冲区(buffer)刷到设备中。 缓冲刷新操作可以保证截至到目前的所有输出都写入到输出流中。 如果在调试的时候，我们不应该去执行endl将它写入到缓冲区中，应该一直使这个输出流报纸刷新。 注释注释可以帮助人类读者理解带有注释的程序。在编译的时候，编译器会自动忽略注释。 Example：1234567891011121314151617#include (iostream)/** * 这是多行注释 * 请注意 注释 界定符不能嵌套 * */ int main()&#123; std::cout &lt;&lt; \" Enter two numbers: \"&lt;&lt;std::endl; // 这是单行注释。 int v1 = 0, v2 = 0; std::cin &gt;&gt; v1 &gt;&gt; v2; std::cout &lt;&lt; \"The sum of \" &lt;&lt; v1 &lt;&lt; \" and \" &lt;&lt; v2 &lt;&lt; \" is \" &lt;&lt;v1+v2 &lt;&lt; \" .\" &lt;&lt;std::endl; return 0;&#125; 运算符： (&lt;&lt;) 输出运算符 (&gt;&gt;) 输入运算符 (&lt;=) 小于等于 (&gt;=) 大于等于 控制流字面意思：控制程序的运行路径。 while语句while语句会反复执行一段带吗，直到给定的条件为假为止。Example：​12345678910111213141516#include &lt;iostream&gt;int main()&#123; int sum = 0, val = 1; while (val &lt;= 1000) &#123; // 只要val的值小于10，循环就会持续执行， sum += val; // 将sum+val赋值给sum ++val; //val+1 &#125; std::cout &lt;&lt; sum &lt;&lt;std::endl; return 0;&#125; 复合赋值运算符： += 将右侧的运算对象加到左侧运算对象上。 前缀递增运算符 ++ 前缀++可以作为左值来使用，将运算的对象+1 1.4.2 for语句上个while例子中的循环条件检测变量，再循环体中增加变量的模式使用非常频繁，所以C++专门定义了第二种循环语句：for语句。 使用for语句重写从1加到10的程序：Example：123456789101112131415#include &lt;iostream&gt;int main () &#123;​ int num1 = 0, num2 = 0; std::cout &lt;&lt; \"输入 num1： \"&lt;&lt;std::endl; std::cin &gt;&gt; num1; std::cout &lt;&lt; \"输入 num2： \"&lt;&lt;std::endl; for (std::cin &gt;&gt; num2; num2&gt;=num1; ++num1) &#123; std::cout &lt;&lt; num1 &lt;&lt;std::endl; &#125; return 0;&#125; 读取数量不定的输入数据如果我们预先不知道要对多少个数求和，这就需要不断读取数据直至没有新的数据输入为止。 ​12345678910#include &lt;iostream&gt;int main() &#123; int sum = 0, value = 0; while (std::cin &gt;&gt; value)&#123; sum += value; &#125; std::cout &lt;&lt; \"sum is \" &lt;&lt; sum &lt;&lt; std::endl;&#125; 因为 value定义为 int类型，所以如果你输入了别的类型的字符，就会导致判断失败，从而不会再次进行循环，然后返回你输入的值的和。 1.4.4 if语句与大多数语言一样，c++也提供了 if语句来支持条件执行。 Example：123456789101112131415161718192021#include &lt;iostream&gt;int main ()&#123; int isnums = 0, nums = 0; if (std::cin &gt;&gt; isnums) &#123; int count = 1; while (std::cin &gt;&gt; nums) &#123; if (isnums == nums) &#123; ++count; &#125;else &#123; std::cout &lt;&lt; \"The \"&lt;&lt; isnums &lt;&lt; \" occurs \"&lt;&lt; count &lt;&lt; \" times.\" &lt;&lt; std::endl; isnums = nums; count = 1; &#125; &#125; std::cout &lt;&lt; \"The \"&lt;&lt; isnums &lt;&lt; \" occurs \"&lt;&lt; count &lt;&lt; \" times.\" &lt;&lt; std::endl; &#125; return 0;&#125; 类简介在C++中，我们铜鼓哦定义一个类来定义自己的数据结构。一个类定义了一个类型以及与其关联的一组操作。类机制就是C++最重要的特性之一。实际上，C++最初的一个设计焦点上就是能定义使用上像内置类型一样自然的类类型。为了使用类，我们需要了解三件事情。 类名是什么 它是在哪儿定义的 它支持什么操作 对于我们即将写的书店程序来说，假定我们的类名为Sales_item，头文件 Sales_item.h中已经定义了这个类。 Sales_item类Sales_item 类的作用是表示一本书的总销售额、售出册数和平均售价。我们现在不关心这些数据如何存储、如何计算。为了使用一个雷，我们不必关心它是如何实现的，只需要知道类对象可以执行什么操作每个类实际上都定义了一个新的类型，其类型名就是类名。因此，我们的Sales_item类定义了一个名为Sales_item的类型，与内置类型一样，我们可以定义类类型的变量。Example:1Sales_item item; 此语句是想表达item是一个Sales_item类型的对象，我们通常将 “item是一个Sales_item类型的对象” 简单说成 “一个Sales_item对象”或者更简单的说成“一个Sales_item”。 除了定义Sales_item类型的变量之外呢，我们还可以： 调用一个名为isbn的函数从一个Sales_item对象中提取 ISBN 书号 用输入运算符（&gt;&gt;）和输出运算符（&lt;&lt;）读写Sales_item类型的对象。 用加法运算符（+）将两个Sales_item对象相加，两个对象必须表示同一本书。加法结果是一个新的Sales_item对象，其ISBN与两个运算对象相同，而其总销售额和售出册数则是两个运算对象的对应值之和。 使用复合赋值运算符讲一个Sales_item对象加到另一个对象上。 重要概念：类定义了行为 当你度这些程序时，类Sales_item的作者定义了类对象可以执行的所有动作。即，Sales_item类定义了创建一个Sales_item对象时会发生什么事情。以及对Sales_item对象进行赋值、加法或输入输出运算时会发生什么事情。 一般而言，类的作者决定了类类型对象上可以使用的所有操作。 读写Sales_item 既然即应知道可以对Sales_item对象执行哪些操作，，我们现在就可以便携使用类的程序了。 例如，下面的程序从标准输入读入数据，存入一个Sales_item对象中，然后将Sales_item的内容写回到标准输出。1234567891011121314#include \"Sales_item.hpp\"#include &lt;iostream&gt;int main()&#123; Sales_item book; // 读入ISBN号、售出的册数以及销售价格。 std::cin &gt;&gt; book; // 写入ISBN号、售出的册数、总销售额和平均价格。 std::cout &lt;&lt; book &lt;&lt; std::endl; return 0;&#125; 新的include形式： 来自标准库的头文件 用 ( &lt;&gt; )包围头文件名。 来自不属于标准库的头文件，用 ( “” )包围。 Sales_item 对象的加法下面是一个对象相加的例子。 1234567891011121314151617#include \"addItems.hpp\"#include &lt;iostream&gt;#include \"Sales_item.hpp\"int main()&#123; Sales_item item1, item2; // 读取一对交易记录 std::cin &gt;&gt; item1 &gt;&gt; item2; //打印和 std::cout &lt;&lt; item1 + item2 &lt;&lt; std::endl; return 0; &#125; 初识成员函数将两个Sales_item对象相加的程序首先应该价差两个对象是否具有相同的ISBN。方法如下：12345678910111213141516171819202122#include \"CheckSame.hpp\"#include &lt;iostream&gt;#include \"Sales_item.hpp\"int main()&#123; Sales_item item1, item2; // 读取一对交易记录 std::cin &gt;&gt; item1 &gt;&gt; item2; //首先检查item1和item2是否表示相同的书 if (item1.isbn() == item2.isbn()) &#123; std::cout &lt;&lt; item1 + item2 &lt;&lt;std::endl; &#125; else &#123; std::cerr &lt;&lt; \"Data must refer to same ISBN\" return -1; &#125; return 0; &#125; 这个if语句的检测条件1item1.isbn() == item2.isbn() 调用名为isbn的成员函数。成员函数式定义为类的一部分函数，有时也被称为方法(method)。我们通常使用 点运算符(.)来调用方法。通常，此方法必须是当前类类型的。当我们访问一个成员函数时，通常我们是想调用该函数，我们使用调用运算符( () )来调用一个函数，调用运算符是一顿圆括号，里面放置参数列表(可能为空)。因为我们现在的成员函数 isbn并不接受参数，因此：1item1.isbn() 调用名为 item1 的对象的成员函数 isbn，此函数返回 item1 中保存的 ISBN书号。 自此下面的就不去写了， 感觉本书对此处写的像是磕磕绊绊，一些细节性的东西没有去发现， 可能不适合初学者读吧。初学者只想知道为什么运行不起来，不会去关心这些跑不起来的东西竟然还要写例子。 第二章 变量和基本类型数据类型是程序设计的基础，它告诉我们数据的意义以及我们能在数据上执行的操作 数据类型决定了程序中数据和操作的意义。如下所示的语句是一个简单示例： 1i = i + j; 其含义依赖于 i 和 j 的数据类型。 如果i j 是整形数，那么这条语句执行的就是最普通的加法运算。 基本内置类型C++定义了一套包括 算数尅性 和 空类型（void） 在内的基本数据类型。其中算术类型包含了 字符、整形数、布尔值、和浮点数。空类型不对应具体的值，仅用于一些特殊的场合。例如最常见的是，如果函数不返回任何值的时候使用空类型作为返回类型。 算术类型算术类型分为两类： 整形（包括字符和布尔类型在内）和浮点型。算数类型的尺寸（也就是该类型数据所占的比特数）在不同机器上有所差别，所表示的范围也不一样。 布尔类型的取值是 真（true）/假（false）。 带符号类型和无符号类型 除去布尔型和扩展的字符型之外，其他整形可以划分为带符号的和无符号的两种。带符号类型可表示正数、负数或0，无符号的类型则仅能表示大禹等于0的值。 类型int、short、long、和long long 都是带符号的，通过在其前面加 unsigened就可以得到无符号类型。 Example： 1unsigened int 与其他整形不同，字符型被分为了三种： char、sigend char 和 unsigned char。尽管字符型有三种，但表现形式却只有两种，带符号的和无符号的。具体是哪种由编译器决定。 建议：如何选择类型 和C语言一样，C++的设计准则之一也是尽可能的接近硬件。C++的算数类型必须满足各种硬件特质。 - 当明确知道数值不可能为负时，选用无符号类型。 - 使用int执行整数运算。 - 执行浮点数运算选用double。 类型转换对象的类型定义了对象能包含的数据和能参与的运算，其中一种运算被大多数类型支持，就是讲对象从一种给定的类型转换为另一种相关类型。当我们像下面这样发吧一种算术类型的值付给另外一种类型时：123456bool b = 42; //bw为真int i = b; //i的值为1i = 3.14; //i的值为3double pi = i; //pi的值为3.0unsigend char c = -1; //假设char占8bytes c的值为255sigend char c2 = 256; //假设char占8bytes c2的值是未定义的。 类型所能表示的值的范围决定了转换的过程。 当我们把一个非布尔类型的算数值赋给布尔类型时，初始值为0则结果为false，否则为true。 当我们把一个布尔值赋给非布尔类型时，初始值为false则结果为0，否则为1. 当我们把一个浮点数赋给整数类型时，进行了近似处理。结果值将仅保留浮点数中小数点之前的部分。 当我们把一个整数值赋给浮点类型时，小数部分记为0.如果该整数所占的空间超过了浮点类型的容量，精度有可能损失。 当我们赋给带符号类型一个超出它表示范围的值时，结果是未定义的，此时，程序有可能继续工作，可能崩溃，也可能生成垃圾数据。 当我们赋给无符号类型一个超出它表示范围的值时，结果是初始值对无符号类型表示树枝总数取模后的余数。 建议：避免无法预知和依赖于实现环境的行为。 无法预知的行为源于编译器无需检测的错误。即使代码编译通过了，如果程序执行了一条未定义的表达式，仍有可能产生错误。 不幸的是，在某些情况或某些编译器下，含有无法预知行为的程序也能正确执行。但是我们却无法保证同样一个程序在别的编译器下能正常工作。甚至已经编译通过的代码再次执行也可能会出错。 程序也应尽量避免依赖于实现环境的行为。 字面值常量一个形如42的值被称作字面值常量，这样的值一望而知。每个字面值常量都对应一种数据类型，字面值常来你的形式和值决定了他的数据类型。 整型和浮点型字面值我们可以将整型字面值写作十进制数、八进制数或十六进制数的形式。以0开头的整数代表八进制数，以0x或者0X开头的代表十六进制。 整型字面值具体的数据类型由它的值和符号决定，默认情况下，十进制字面值是带符号数，八进制和十六进制字面值极可能带符号也可能是无符号。浮点型字面值是一个double。 字符和字符串字面值由单引号括起来的一个字符成为char型字面值，双引号括起来的另个或多个字符则构成字符串型的字面值‘a’ – 字符型字面值“abc” – 字符串型字面值 转义序列有两类字符程序员不能直接使用，一类是 不可打印的字符，如退格或其他控制字符，因为它们没有可视的图符；另一类是在C++语言中有特殊含义的字符（单引号，双引号，问号，反斜线）。在这些情况下需要用到\b转义序列，转义序列均以反斜线作为开始。 换行符 \\n 横向制表符 \\t 报警符 \\a纵向制表符 \\v 退格符 \\b 双引号 \\”反斜线 \\\\ 问号 \\? 单引号 \\’回车符 \\r 进纸符 \\f 在程序中，上述转义序列被当做一个字符使用。 布尔字面值和指针字面值true和false是布尔类型的字面值。bool test = false;nullptr 是指针字面值。 变量变量提供一个具体名字，可供程序操作的存储空间，c++中的每个变量都有其数据类型。数据类型决定着变量所占内存空间的大小和布局方式。该空间能存储的值的范围以及变量能参与的运算， 对C++程序员来说，“变量”和“对象”一般可以互换使用。 变量定义变量定义的基本形式是：首先是类型说明符，随后紧跟由一个或多个变量名组成的列表，其中变量名以逗号分隔，最后1️以分号结束。列表中每个便来匿名的类型都由类型说明敷指定，定义时还可以为一个或多个变量赋初值。 1234int sum - 0, value // sum。value 都是int sum初值为0；Sales_item item; //item的类型是Sales_item。//string 是一种库类型，表示一个可变长的字符序列。std::string book (0-123-45678-X); 何为对象： C++程序员们在很多场合都会使用对象这个名词。通常情况下，对象是指一块能存储数据并具有某种类型的空间。 一些人仅在与类有关的场景下才使用“对象”这个词。另一些人则已把命名的对象和未命名的对象区分开来，其中对象指能被程序修改的数据，而值指制度的数据。 初始值当对象在创建时获得了一个特定的值，我们说这个对象被初始化了。用于初始化便拉近的值可以使任意复杂的表达式。当一次定义了两个或多个变量时，对象的名字随着定义也就马上可以使用了。因此在同一条定义语句中，可以用箱定义跌变量值去初始化后定义的其他变量。 Example:1234//用price的值初始化discountdouble price = 109.99, discount = price * 0.16;//调用函数applyDiscount并返回值用来初始化salePricedouble salePrice = applyDiscount(price, discount); 列表初始化C++定义了初始化的好几种不同形式，这也是初始化问题复杂性的一个体现。例如，要想定义一个名为units_soid的int变量并初始化为0，以下的四条语句都可以做到：1234int units_soid = 0;int units_soid = &#123;0&#125;;int units_soid(0);int units_soid&#123;0&#125;; 作为C++新标准的一部分，永花括号来初始化变量得到了全面应用，在此之前仅在某些受限的场合下使用。这种初始化的形式被称为列表初始化。 当用于内置类型的变量时，这种初始化形式有一个重要特点：如果我们使用列表初始化切初始值存在丢失信息的风险，则编译器会报错：123long double id = 3.1415926536;int a&#123;id&#125;, b = &#123;id&#125;; // 错误：转换未执行，因为存在信息丢失的危险。int c(id), d = id; // 正确：转换执行，且确实丢失了部分值。 默认初始化如果定义变量时没有指定初始值。则变量就会被默认初始化，此时变量被赋予了‘默认值’，默认值到底是什么由变量类型决定。同时顶一边拉怪in的位置也会对此有影响。 如果是内置类型的变量未被显示初始化，它的值由定义的位置决定。定义于任何函数体之外的变量被初始化为0.一种例外情况是，定义在函数体内部的内置类型白能量将不被初始化。一个未被初始化的内置类型变量的值是未定义的。如果试图拷贝或以其他形式访问此类值将引发错误。 每个类格子决定其初始化对象的方式。而且，是否允许不精初始化就定义对象也由类型自己决定，如果类允许这种行为，它将决定对象的初始值到底是什么。 绝大多数类都至此无需显示初始化而定义对象，这样的类提供了一个合适的默认值。例如，string类规定如果没有指定初值则生成一个空串：12std::string empty; // 默认值为空 \"\"Sales_items item; // 被默认初始化的Sales_item对象。 一些类要求每个对象都显示初始化，此时如果创建了一个该类的对象而未对其做明确的初始化操作，将引发错误。 定义于函数体内的内置类型的对象如果没有初始化，则其值未定义。类的对象如果没有显示的初始化，则其值由类决定。 执行默认初始化时，内置类型的值是未定义的.这句话是有前提的，前提就是这个内置类型在哪申请的空间。12345678910111213141516171819202122232425262728#include &lt;iostream&gt;using namespace std;//,在静态存储区申请，所以初始化为0int a;//这个叫做值得初始化，3作为初始值int a_1 = 3;int main() &#123; //这个叫做有初始化值 int k = 5; //这个不是初始化，叫赋值 k = 8; //b也是内置类型，但是他在函数体申请，所以是在栈申请的空间，所以值未定义 int b; //new出来的空间都是在堆申请的，有操作系统自动分配可用空间，所以不会初始化 int *p = new int; //static申明的成员是存储在静态存储空间的，所以会初始化为0 static c; return 0;&#125; 提示：未初始化变量引发运行时故障。 未初始化的变量含有一个不确定的值，使用未初始化变量的值是一种错误的变成行为并且很难调试。尽管大多数编译器都能对一部分使用未初始化变量的行为提出警告，但严格来说编译器并未被要求检查此类错误。使用未初始化的便拉近将带来无法预计的后果。有时我们足够幸运，一访问此类对象程序就崩溃并报错，此时只要找到崩溃的位置就很容易发现变量没有初始化的问题。另外一些时候，程序会一直执行完并产生错误的结果。更\b糟糕的错误是，程序结果时对时错，无法把握。而且往无关的位置添加代码还会导致我们误以为程序对了，其实结果依旧有错。 梳理一下堆栈 定义： 栈区（stack）— 由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。 堆区（heap） — 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收 。 区别和联系： 申请方式 堆是由程序员自己申请并指明大小，在c中malloc函数 如p = (char *)malloc(10); 栈由系统自动分配，如声明在函数中一个局部变量 int b; 系统自动在栈中为b开辟空间 申请后系统的响应 栈：只要栈的剩余空间大于所申请空间，系统将为程序提供内存，否则将报异常提示栈溢出。 堆：首先应该知道操作系统有一个记录空闲内存地址的链表，当系统收到程序的申请时，会遍历改链表。寻找第一个空间大于所申请的堆节点，然后将该节点从空闲结点链表中删除，并将该结点的空间分配给程序。另外，对于大多数系统，会在这块内存空间的首地址处记录本次分配的大小，这样，代码中的delete语句才能正确的释放此内存空间。另外，由于找到的堆结点的大小不一定正海等于申请的大小，系统会自动的将多余的那部分重新放入空闲链表中。 申请大小的限制 栈：在win下，栈是像低地址扩展的数据结构，是一块连续的内存区域。这句话的意思是栈顶的地址和站的最大容量是系统预先规定好的，在win下，栈的大小是2M，如果申请的空间超过栈的剩余空间时，将提示overflow。因此，能从栈获取的空间较小。 堆：堆是像高地址扩展的数据结构，是不连续的内存区域。这是由于系统是用链表来存储的空闲内存地址的，自然是不了徐的，而链表的遍历方向是由低到高地址。堆的大小受限于计算机系统中的有小雨你内存。所以堆的空间比较灵活，也比较大。 申请效率比较 栈：由系统自动分配，速度较快。但程序员无法控制。 堆：由new分配的内存，一般速度比较慢，而且容易产生内存碎片，不过用起来最方便。 变量声明和定义的关系为了允许把程序拆分成多个逻辑部分来编写，C++语言支持分离式编译机制，该机制允许将程序分割为若干个文件，每个文件可被独立编译为了支持分离式编译，C++语言将声明和定义区分开来。声明使得名字为程序所知，一个文件如果想使用别处定义的名字则必须包含对那个名字的声明。而定义负责创建与名字关联的实体。定义还会申请存储空间，也可能会为变量赋一个初始值。如果想声明一个变量而非定义它，就在变量名前添加关键字 extern， 而且不要显式的初始化变量：12extern int i; //声明而非定义。int j; //声明并定义。 任何包含了显式初始化的声明即成为定义。我们能给出 extern 关键字标记的拜年啦给你赋一个初始值，但是这么做就抵消了extern的作用。 extern语句如果包含初始值就不再是声明，而变成定义了：1extern int i = 19; //定义。 在函数体内部，如果试图初始化一个由extern关键字修饰的变量，将引发错误。 变量能且只能被定义一次，但是可以被多次声明。 概念：静态类型 \b\bC++是一种静态类型语言，其含义是在编译节点检查类型。其中，检查类型的过程成为类型检查。我们已经知道，对象的类型决定了对象所能参与的运算。在C++语言中，编译器负责检查数据类型是否支持要执行的运算，如果试图执行类型不支持的运算，编译器将报错并且不会生成可执行文件。程序越复杂，静态类型检查越有助于发现问题，然而，前提是编译器必须知道每一个实体对象的类型。 标识符C++的标识符由字母、数字、下划线组成，其中必须以字母或下划线开头。标识符的长度没有限制，但是对大小写敏感。C++语言保留了一些名字供语言本身使用，这些名字不能被用作标识符。同时，C++也为标准库保留了一些名字。用户自定义的标识符中不能连续出现两个下划线，也不能以下划线紧连大写字母开头。此外，定义在函数体外的标识符不能以下划线开头。 变量命名规范变量命名有许多约定俗成的规范，下面的这些规范能有效提高程序的可读性： 标识符要能体现实际含义。 变啦滚名一般用小写字母，如index，不要使用Index或 INDEX。 用户自定义的类名一般以大写字母开头，如 Sale_item。 如果标识符由多个单词组成，则单词间应用明显区分。 名字的作用域不论是在圣墟的什么位置，使用到的每个名字都会执行一个特定的实体：变量、函数、类型等，同一个名字如果出现在程序的不同位置，也可以执行的是不同实体。 作用域是程序的\b一部分，在其中名字有其特定的含义。C++语言中大多数组用于都以花括号分离。同一个名字在不同的作用域中可能指向不同的实体。名字的有效区域始于名字的声明语句，以声明语句所在的作用域末端为结束。 建议：当使用变量的时候再去定义 一般来说，在对象第一次被使用的地方附近定义它是一种好的选择，因为这样做有利于更容易的找到便来那个的定义。更重要的是，当变量的定义与它第一次被使用的地方很近时，我们也会赋给它一个比较合理的初始值。 嵌套的作用域作用域能彼此包含，被包含的作用域称为内层作用域，包含着别的作用域称为外部作用域。作用域中一旦声明了某个名字，它所嵌套的所有作用域中都能访问该名字。同时，允许在内层作用域中重新定义外层作用域已有的名字：123456789101112131415161718192021/** *函数内部不宜定义与全局变量同名的新变量 */#include &lt;iostream&gt;//全局变量int reused = 42;int main(int argc, char *argv[])&#123; //块变量 int unique = 0; //输出 #1:使用全局变量reused std::cout &lt;&lt; reused &lt;&lt; \" \" &lt;&lt; unique &lt;&lt; std::endl; //覆盖全局变量reused int reused = 0; // 输出 #2:使用局部变量reused std::cout &lt;&lt; ::reused &lt;&lt; \" \" &lt;&lt; unique &lt;&lt; std::endl; //输出 #3:显式的访问全局变量reused， std::cout &lt;&lt; ::reused &lt;&lt; \" \" &lt;&lt;unique &lt;&lt; std::endl;&#125; 解释：输出#1：出现杂我jububianlaignreused定义之前，因此这条语句使用全局作用域中定义的名字reused，输出42 0.输出#2：发生咋已局部变量reused定义之后，此时局部变量reused正在作用域内，因此第二条输出语句使用的是局部变量reused而非全局变量，输出0 0.输出#3：使用域操作符 :: 来覆盖默认的作用域规则，因为全局作用域本身并没有名字，所以当作用域左侧为空时，向全局作用域发憷请求获取作用域\b操作符右侧名字对应的变量。结果是，第三条输出语句使用全局变量reused，输出42 0 建议 如果函数有可能用到某全局变量，则不宜再定义一个同名的局部变量。 复合类型复合类型 是指基于其他类型定义的类型。C++语言有几种复合类型，下面介绍 引用和指针。与我们已经掌握的变量声明相比，定义复合类型的便拉近要复杂很多。之前 提到，一条简单的声明语句由一个数据类型和紧随其后的一个变量名列表组成。其实更通用的描述是，一条声明语句由一个基本数据类型和紧随其后的一个\b声明符列表组成。每个声明符命名了一个变量并指定该变量为与基本数据类型有关的某种类型。 引用 C++11中新增了一种引用：所谓的“右值引用”，这种引用主要用于内置类。严格来说，当我们使用术语“引用”时，指的其实是“左值引用”。 引用 为对象起了另外一个名字，引用类型医用另外一种类型。通过将声明写成 &amp;d 的形式来定义引用类型，其中d是声明的变量名。123int ival = 1024;int &amp;refival = ival; //refval指向ival（ival的另一个名字）int &amp;refval2; //报错：引用必须初始化 一般在初始化变量时，初始值会被拷贝到新建的对象中，然而定义引用时，程序吧引用和它的滁州市值绑定在一起，而不是将初始值拷贝给引用。一旦初始化完成，引用将和他的初始值对象一直绑顶在一起，因为无法令引用重新把那个顶到另外一个对象，因此引用必须初始化。 引用即别名 引用并非对象，相反的，它只是为一个已经存在的对象所起的另外一个名字。 为引用赋值，实际上是把值付给了与引用绑定的对象。获取引用的值，实际上是获取了与引用绑定的对象的值。同理，以引用作为初始值，实际上是以与引用绑定的对象作为初始值： 123456789101112131415#include &lt;iostream&gt;using namespace std;int main(int argc, char *argv[]) &#123; int a = 10; int b = 20; int &amp;ra = a; ra= b; cout &lt;&lt; ra &lt;&lt; endl; // 输出10 cout &lt;&lt; a &lt;&lt; endl; // 输出20 return 0;&#125; 因为引用本身不是一个对象，所以不能定义引用的引用。 引用的定义允许在一条语句中定义多个引用，其中每个引用标识符都必须以符号&amp;开头：1234567int i = 1024, i2 = 2048; // 都是intint &amp;r = i, r2 = i2; // r是一个引用，与i绑在一起，r2是intint i3 = 1024m &amp;ri = i3; // i3是int，ri是一个引用，与i3绑定在一起int &amp;r3 = i3, &amp;r4 = i2; // r3和r4都是引用int &amp;refNum = 10; //错误//引用只能绑定在对象上，而不能与字面值或者某个表达式的计算结果绑定在一起。 指针指针是指向另外一种类型的复合类型。与引用类似，指针也实现了对其他对象的间接访问。然而指针与引用相比又有很多不同点。其一，指针本身就是一个对象，允许对指针赋值和拷贝，而且咋指针的生命周期内它可以先后指向几个不同的对象。其二，指针无需在定义时赋初值。和其他内置类型一样，在快块作用域内定义的指针如果没有被初始化，也将又有一个不确定的值。 指针通常难以理解，即使有经验的程序员也常常因为调试指针引发的错误而烦恼 定义指针类型的方法将声明符写成 x(变量) 的形式。如果在一条语句中定义了几个指针变量，每个变量都要有 。12int *pi1, *pi2, *pi3; //都是指向int类型对象的指针。double *pd1, pd2; //pd1是指向double类型对象的指针 获取对象的地址指针存放某个对象的地址，要想获取该地址，需要使用取地址符(&amp;)12int ival = 42;int *pval = &amp;ival; //pval存放变量ival的地址，或者说pval是指向val变量的指针。 第二条语句吧pval定义为一个指向int的指针，随后初始化pval另其指向名为ival的int对象。因为引用不是对象，没有实际地址，所以不能定义指向引用的指针。和引用一样，指针只能绑定在对象上，而不能与字面值或者某个表达式的计算结果绑定在一起。123456double dval;double *pd = &amp;dval; //正确：初始值是double型对象的地址。double *pd2 = pd; //正确：初始值是指向double对象的指针int *pi = pd; //错误：指针类型和pd类型不匹配pi = &amp;dval; //错误：视图把double型对象的地址赋给int型指针 因为在声明语句中指针的类型实际上被用于指定它所指向对象的类型，所以二者必须匹配。如果指针指向了一个其他类型的对象，对该对象的操作将发生错误。 指针值指针的值（即地址）应属于下列四中状态之一： 指向一个对象 指向紧邻对象所占空间的下一个位置 空指针，意味着指针没有指向任何对象 无效指针，也就是上述情况之外的其他值， 试图拷贝火以其他方式访问无效的指针豆浆引发错误。编译器并不负责检查此类错误。，这一点和试图使用未经初始化的变量是一样的。访问无效指针的后果无法预计，因此程序员必须清楚任意给定的指针是否有效。 尽管第二种和第三种形式的指针是有效的，蛋其使用同样受到限制。显然这些指针没有指向任何具体对象，所以试图访问此类指针（假定的）对象的行为不被允许。如果这样做了，后果也无法预计。 利用指针访问对象如果指针指向了一个对象，则允许使用解引用符(*)来访问对象。123int ival - 42; int *p = &amp;ival; //p存放着变量ival的地址，或者说p是指向变量ival的指针cout &lt;&lt;*p&lt;&lt;endl; //由符号*得到指针p所指的对象，输出42。 对指针解引用会得出所指的对象，因此如果给解引用的结果赋值，实际上也就是给指针所指的对象赋值。解引用操作仅适用于那些确实指向了某个对象的有效指针。 关键概念：某些符号有多重含义 像 &amp; * 这样的符号，技能用做表达式里的运算符，也能作为声明的一部分出现，符号的上下文决定了符号的\u001b意义：12345int i = 42;int &amp;r = i; //&amp;紧随类型名出现，因此是声明的一部分，所以是引用int *p; //*紧随类型名出现，因此是声明的一部分，所以是指针p = &amp;i; //&amp;出现在表达式中，是一个取地址符int &amp;r2 = *p; //&amp;是声明的一部分，*是一个解引用符。 在声明语句中， &amp; 和 * 用于组成复合类型；在表达式中，他们的角色又转变成运算符。在不同场景下出现的虽然是同一个符号，但是由于含义截然不同，所以我们完全可以吧它当做不同的符号来看待。 空指针空指针不指向任何对象，在\b试图使用一个指针之前，代码可以首先检查它是否为空，以下列出几个生成空指针的方法：123int *p1 = nullptr;int *p2 = 0;itn *p3 = NULL; 得到空指针最直接的办法就是采用字面值 nullptr来初始化指针，这也是C++11新标准刚刚引入的一种方法。nullptr是一种特殊类型的字面值，它可以被转换成任意其他的指针类型。过去的程序还会用到一个名为NULL\b的预处理变量来给指针赋值，这个变量在头文件catdlib中定义，它的值就是0。预处理变量不属于命名空间std，它由预处理器负责管理，因此我们可以直接使用预处理白能量儿无需在前面加域操作符当用到一个预处理变量时，预处理器会自动的将它替换为实际值，因此用NULL初始化指针和0初始化指针是一样的。在新标准下，现在的C++程序最好使用nullptr，同事尽量避免使用NULL。 把int变量直接赋给指针是错误的操作，即使int变量的值签好等于0也不行。12int zero = 0;ip = zero; 建议：初始化所有指针 使用未经初始化的指针是引发运行时错误的一大原因。和其他变量一样，访问未经初始化的指针所引发的后果也是无法预计的。通常这一行为将造成程序崩溃，而且一旦崩溃，要想定位到出错位置讲师特别棘手的问题。在大多数编译器环境下，如果使用了未经初始化的指针，则该指针所占内存空间的当前内容将会被看做一个地址值。访问该指针们相当于去访问一个本不存在位置上的本不存在的对象。如果指针所占内存空间中恰好有内容，而这些内容又恰好被当做了某个地址，我们就很难分清它是合法的还是非法的了。因此建议初始化所有的指针，并且在可能的情况下，尽量等定义了对象之后再定义指向他的指针，如果实在步行出指针应该指向何处，就初始化为nullptr，这样程序就能检测并知道它有没有指向任何具体的对象了。 赋值和指针指针和引用都能提供对其他对象的间接访问，然而在具体实现细节上二者有很大不同，其中最重要的一点就是引用本身并非是一个对象。一旦定义了引用，就无法另其再绑定到另外的对象，之后每次使用这个引用都是访问它最初绑定的那个对象。指针和它存放的地址之间就没有这种限制了。和其他任何变量(只要不是引用)一样，给指针赋值就是令它存放一个新的地址，从而指向一个新的对象：1234567int i = 42;int *pi = 0; //pi被初始化,没有指向任何对象int *pi2 = &amp;i; //pi2被初始化并指向了i的地址\b。int *pi3; //pi3被定义，但未被初始化，所以pi3的值不确定。pi3 = pi2; //pi3和pi2指向同一个对象ipi2 = 0; //pi2又被初始化，并\u001b不指向任何对象 其他指针操作只要指针拥有一个合法值，就能将它用在条件表达式中。和采用算数值作为条件遵循的规则类似，如果指针的值是0\b；则条件取false。12345678910int ival = 1024; // int *pi = 0; // pi是一个空指针。int *pi2 = &amp;ival; // pi2存这ival的地址、if(pi)&#123; // pi的值是0，因此条件为false //...&#125;if(pi2)&#123; // pi2的值是1024，因此条件为true //...&#125; 结论：任何非0指针对应的条件都是true。对于两个类型的合法指针，可以用相等操作符(==)或不相等操作符(!=)来比较他们，比较的结果是布尔类型。如果两个指针村法规的地址值相同，则他们相等，反之则不等。这里两个指针存放的地址值相同(两个指针相等),有三种可能，它们都为空，都指向同一个对象，或者都指向了同一个对象的下一地址。需要注意的是，一个指针指向某对象，同时另一个指针指向另外对象的下一地址，此时也有可能出现这两个指针值相同的情况，即指针相等。因为上述操作要用到指针的值，所以不论是作为条件出现还是参与比较运算，都必须使用合法指针，使用非法指针作为条件或进行比较都会引发不可预计的后果。 void* 指针void* 是一种特殊的指针类型，可用于存放任意对象的地址。一个void*的指针存放着一个地址，这一点和其他指针类似。不同的是，我们对该地址中到底是个什么类型的对象并不了解：1double obj = 3.14, *pd = &amp;obj; 利用viod*指针能做的事情比较有限：拿它和别的指针比较、作为函数的输入或输出，或者赋给另一个void*指针。不能直接操作void*所指的对象，因为我们并不知道这个对象的类型，也就无法确定能在这个对象上进行什么操作。概括来讲，以void*的视角来看内存空间也就仅仅是内存空间，没办法访问内存空间中所存的对象。 理解复合类型的声明变量的定义包括一个基本数据类型和一组声明符。子啊同一条定义语句中，虽然基本数据类型只有一个，但是声明符的形式却可以不同，也就是说，一条定义语句可能定义出不同类型的变量。12// i是一个int型的整数，p是一个int型的指针，r是一个int型的引用。int i = 1024, *p - &amp;i, &amp;r = i; 很多程序员迷惑于基本数据类型和类型修饰符的关系，其实后者不过是声明符的一部分。 定义多个变量经常有一种观点会误以为，在定义语句中，类型修饰符(*或&amp;)作用于本次定义的全部变量。造成这种错误看法的原因有很多，其中之一是我们可以把空格写在类型修饰符和变量名中间1int* p; // 合法但是容易产生误导。 我们说这种写法可能产生误导是因为int放在一起好像是这条语句中所有变量共同的类型一样，其实恰恰相反，基本数据类型是int而非int\\。 *仅仅是修饰了p而已，对该声明语句中的其它变量，它并不产生任何作用：12// p1是指向int类型的指针，p2是intint* p1, p2; 涉及指针或引用的声明，一般有两种写法，第一种把修饰符和变量标识符写在一起：1int *p1, *p2; // p1 和 p2都是指向int的指针。 这种形式着重强调变量具有的复合类型。第二种把修饰符和类型名卸载一起，并且每条语句只定义一个变量。12int* p1; // p1 是指向int的指针。int* p2; // p2 是指向int的指针。 这种形式则强调了本次声明定义了一种复合类型。 上述两种定义指针或引用的不同方法没有对错之分，关键是选择并坚持其中的一种写法，不要总是变来变去。 指向指针的指针一般来说，盛明富中修饰符的个数并没有显示。当有多个修饰符连写在一起时，按照其逻辑关系详加解释即可。以指针为例，指针是内存中的对象，像其他对象一样也有自己的地址，因此允许把指针的地址再放到另一个指针中。 通过*的个数可以区分指针的级别，也就是说，**表示指向指针的指针，***表示指向指针的指针的指针，以此类推。123int ival = 1024;int *pi = &amp;ival; // p1 指向一个int类型的数。int **ppi = &amp;p1; // p2 指向一个int类型的指针。 此处pi是指向int型数的指针，而ppi是指向int型指针的指针。解引用int型指针会得到一个int型的数，同样，解引用指向指针的指针会得到一个指针。此时为了访问最原始的对象，需要对指针的指针 做两次解引用。 指向指针的引用引用本身不是一个对象，因此不能定义指向引用的指针。但指针是对象，所以存在对指针的引用。123456789101112131415161718int main(int argc, char *argv[]) &#123; int i = 42; // p是一个int型指针。 int *p; //r是对指针p的引用 int *&amp;r = p; // r 引用 i 的指针，因此给 r 赋值 &amp;i 就是令 p 指向 i r = &amp;i; //解引用 r 得到 i，也就是p指向的对象，将i的值改为0。 *r = 0; return 0;&#125; 要理解 r 的类型到底是什么，最简单的办法就是从右向左阅读 r 的定义。离变量名最近的符号 &amp; 对变量的类型有最直接的音箱，因此r是一个引用。声明符的其余部分用以确定r引用的类型是什么，此例中的符号 * 说明 r 引用的是一个指针。最后，声明的基本数据类型部分指出 r 引用的是一个int指针。 const限定符有时我们希望定义这样一种变量，它的值不能被改变。例如，用一个变量来表示缓冲区的大小。使用变量的好处是当我们觉得缓冲区大小不再合适时，很容易对其进行调整。另一方面，也应随时警惕防止程序一不小心改变了这个值。为了满足这一要求，可以用关键字const对变量的类型加以限定。1const int bufSize = 512; // 输入缓冲区大小 这样就把bufSize定义成了一个常亮。任何试图为bufSize赋值的行为都将引发错误。1bufSize = 512; // 错误：试图像const对象写入。 因为const对象一旦创建后其值就不能再改变，所以const对象必须初始化。一如既往，初始值可以使任意复杂的表达式。123const int i = getsize(); // 正确：运行时初始化。const int j = 42; // 正确：编译时初始化。const int k; // 错误：k是一个未经初始化的常量。 初始化和const正如之前反复提到的，对象的类型决定了其上的操作。与非const类型所能参与的操作相比，const类型的对象能完成其中大部分，但也不是所有的操作都适合。主要的限制就是只能在const类型的对象上执行不改变其内容的操作。例如，const int 和普通的 int 一样都能参与算术运算，也都能转换成一个布尔值。 在不改变const对象的操作中还有一种是初始化，如果利用一个对象去初始化另外一个对象，则它们是不是const都无关紧要。123int i = 42;const int ci = i;int j = ci; 尽管ci是整型常量，但无论如何 ci 中的值还是一个整型数。ci 的常量特征仅仅在执行改变ci的操作时才会发挥作用。当用 ci 去初始化j时，根本无需在意ci是不是一个常量。拷贝一个对象的值并不会改变它，一旦拷贝完成，新的对象就和原来的对象没什么关系了。 默认状态下，const对象仅在文件内有效当以编译时初始化的方式定义一个const对象时，就如对bufSize的定义一样：1const int bufSize = 512; // 输入缓冲区大小 编译器将在编译的过程中把用到该变量的地方都替换成相对应的值。也就是说，编译器会找到代码中所有用到 bufSize 的地方，然后用 512 替换。 为了执行上述替换，编译器必须知道变量的初始值。如果程序包含多个文件，则每个用了const对象的文件都必须得能访问它的初始值才行，要做到这一点，就必须在每个用到变量的的文件中都有对它的定义。为了支持这一用法，同时避免对同一个变量的抽工夫定义，默认情况徐昂西啊，const对象被设定为仅在文件内有效。但你给多个文件中出现了同名的const变量时，其实等同于在不同文件中分别定义了独立的变量。 某些时候有这样一种const变量，它的初始值不是一个常量表达式，但又确实有必要在文件间共享。这种情况下，我们不希望编译器为每个文件分别生成独立的变量。相反，我们想让这类const对象像其他(非常量)一样工作。也就是说，只在一个文件中定义const，而在其他多个文件中声明并使用它。 而解决方法非常简单，对于const变量不管是声明还是定义都添加extern关键字，这样只需要定义一次。12345678910111213141516171819202122232425262728293031// FILE1: str.h#ifndef STR_H#define STR_H#include &lt;iostream&gt;// 在头文件中可以选择将其初始化。// extern const std::string str = \"AB\";// 也可以选择不初始化，到引用头文件的地方进行初始化。extern const std::string str;class str &#123;&#125;;#endif //STR_H// FILE2: str.cpp#include \"str.h\"extern const std::string str = \"ABCDE\";int main(int argc, char *argv[]) &#123; std::cout &lt;&lt; \"abc\"&lt;&lt; ceshi &lt;&lt; std::endl; // 输出字符为 AB std::cout &lt;&lt; \"abc\"&lt;&lt; ceshi &lt;&lt; std::endl; // 输出字符为 AB return 0;&#125; 因为str是一个常量，所以必须用 extern关键字修饰。 const的作用可以把引用绑定到const对象上，就像绑定到其他对象上一样，我们称之为对常量的引用。与普通引用不同的是，对常量的引用不能被用作修改它所绑定的对象。1234const int c1 = 1024;const int &amp;r1 = c1 // 引用及其对应的对象都应该是常量r1 = 42; // 错误， r1是常量。int &amp;r2 = c1 // 错误，非常量不能引用常量。 因为不允许直接为c1赋值，当然也就不能通过引用取改变c1，因此，对r2的初始化是错误的。假设改初始化合法，则可以通过r2来改变它引用对象的值，这显然是不正确的。 C++程序员经常把词组 “对const的引用” 简称为 “常量引用”，这一简称还是挺靠谱的，不过前提是你得时刻记着这就是个简称而已。严格来说，并不存在常量引用。因为引用不是一个对象，所以我们没法让引用本身恒定不变。事实上，由于C++语言并不允许随意改变引用所绑定的对象，所以从这层意义上理解所有的引用又都算是常量。引用的对象是常量还是非常量可以决定其所能参与的操作，却无论如何都不会影响到引用和对象的绑定关系本身。 初始化和对const的引用上一节提到，引用的类型必须与其所引用对象的类型一致，但是有两个例外。第一种例外情况是在初始化常量引用时允许用任意表达式作为初始值，只要该表达式的结果能转换成引用的类型即可。尤其，允许为一个常量引用绑定非常量的对象，字面值，甚至是一个表达式。12345int i = 42;const int &amp;r1 = i; // 允许将const引用绑定到普通int对象上。const int &amp;r2 = 42; // 正确。r1是一个常量引用。const int &amp;r3 = r1 * 2; // 正确。r2是一个常量引用int &amp;r4 = r1 * 2; // 错误。r4是一个普通的非常量引用。非const引用的右值不能是表达式 要想理解这种例外情况的原因，最简单的办法就是弄清楚当一个常量引用被绑定到另外一种类型上时到底发生了什么：12double dval = 3.14;const int &amp;ri = dval; 此处ri引用了一个int型的数，对ri的操作应该是整数运算，但dval却是一个双精度浮点而非整数。因此为了缺包让ri的绑定一个整数，编译器吧上述代码变成了如下形式：12const int temp = dval;const int &amp;ri = temp; 在这种情况下，ri做了一个临时量对象。所谓临时量对象就是当编译器需要一个空间来暂存表达式的求职结果时，临时常见的一个未命名的对象。C++程序员们常常把临时量对象简称为临时量。 接下来探讨当ri不是常量时，如果执行了类似于上面的初始化过程将带来什么样的后果。如果\bri不是常量，就允许对ri赋值，这样就会改变ri所引用对象的值。注意，此时绑定的对象是一个临时变量而非dval。程序员既然让ri引用dval，就肯定想通过ri改变\bdval的值，否则干什么要给ri赋值呢？如此看来，既然大家基本上不会想着把引用绑定到临时量上，C++语言也就把这种行为归为非法。 对const的引用可能引用一个并非const的对象必须认识到，常量引用仅对引用可参与的操作做出了限定，对于引用的对象本身是不是一个常量未做限定。因为对象也可能是个非常量，所以允许其他途径改变它的值。1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;int main(int argc, char *argv[]) &#123; int i = 42; int &amp;r1 = i; // 引用r1 绑定对象i。 const int &amp;r2 = i; // 引用r2也绑定了对象i，但因为r2是const，所以不允许通过其修改i的值 std::cout &lt;&lt; r1 &lt;&lt; std::endl; std::cout &lt;&lt; r2 &lt;&lt; std::endl; // 通过更改源的方式来改变const引用的值 r1 = 0; // r1 不是常量，i的值被修改为0；// r2 = 0; // 错误：r2 是常量引用。 std::cout &lt;&lt; r1 &lt;&lt; std::endl; std::cout &lt;&lt; r2 &lt;&lt; std::endl; // const常量不能更改 r1 = 15; const int num = i;; std::cout &lt;&lt; num &lt;&lt; std::endl; // 输出15 i = 20; r1 = 20; std::cout &lt;&lt; num &lt;&lt; std::endl; // 输出15，const常量不可更改。 return 0;&#125; r2 绑定整数i是合法的，然而，不允许通过r2修改i的值。尽管如此，i的值仍然允许通过其他途径修改，既可以直接给i复制，也可与通过像r1一样绑定到i的其他引用来修改。 指针和const与引用一样，也可以令指针指向常量或非常量。类似于常量引用。,指向敞亮的指针不能用不改变其所指对象打的值。要想存放敞亮的对象的地址，只能使用指向常量的指针：1234const double pi = 3.14; // pi是个常量，他的值不能改变。double *ptr = &amp;pi; // 错误：ptr是一个普通指针。const double *cptr = &amp;pi; // 正确：cptr可一直想一个双精度常量*cptr = 42; // 错误：不能给*cptr赋值。 2.3.2节提到，只针对的类型必须与其所指对象的类型一直，但是有两个例外、第一种类外情况是允许另一个纸箱厂凉的指针指向一个非常量对象：12double dval = 3.14; // dval是一个双精度浮点数，它的值可以改变。cptr = &amp;dval; // 正确：但是不能通过cptr改变dval的值。 和常量引用一样，指向敞亮的指针也没有规定其所指的对象必须是一个常量。所谓指向敞亮的指针仅仅要求不能通过该指针改变的对象的值，而没有规定那个对象的值不能通过其他途径改变。 试试这样想：所谓指向敞亮的指针或引用，不过是指针或引用“自以为是”罢了。它们觉得自己指向了常量，所以自觉地不去改变所指向对象的值。 const指针指针是对象而引用不是，因此就像其他对象类型一样，允许把指针本身定位常量。常量指针必须初始化，而且一旦初始化完成，则它的值（也就是存放在指针中的那个地址）就不能再改变了。把*放在const关键字之前用以说明指针是一个常量，这样的书写形式隐含着一层意味，即不变的是指针本身的值而非指向的那个值：1234int errNumb = 0;int*const curErr = &amp;errNumb // curErr将一直直系那个errNumbconst double pi = 3.14159;const double *const pip = &amp;pi; // pip是一个指向常量对象的常量指针 如同2.3.2节所讲的，要想弄清楚这些生命的含义，最行之有效的办法是从右向左阅读。此例中，离curErr最近的符号是 const，意味着curErr 本身是一个常量对象，对象的了O型由声明福的其余部分决定。声明福中的下一个符号是*，意思是curErr是一个常量指针。最后，该声明语句的基本数据类型部分确定了常量指针指向的是一个int对象。与之相似，我们也能推断出，pip是一个常量指针，它指向的对象是一个双精度浮点型常量。 指针本身是一个常量并不意味着不能通过指针修改其所指对象的值，能否这样做完全依赖于所指对象的类型，例如，pip是一个指向常量的常量指针，则不论是pip所指的对象值还是pip自己存储的那个地址都不能改变。相反的，curErr指向的是一个一般非常量整数，那么就完全可以用curErr去修改errNumb的值：123456*pip = 2.72； // 错误：pip是一个指向常量的指针if(*curErr)&#123; // 如果curErr所指的对象（也就是errNumb）的值不为0 errorHandler(); // *curErr = 0; // 正确：把curErr所指的对象的值重置&#125; 顶层const如前所述，指针本身是一个对象，它又可以指向另外一个对象。因此，指针本身是不是常量以及指针所指的是不是一个常量就是两个相互独立的问题。用名词顶层const表示指针本身是个常量，而用名词底层const表示指针所指的对象是一个常量。更一般的，顶层const可以表示任意的对象是常量，这一点对任何数据类型都适用。如算数类型、类、指针等。底层const则与指针和引用等复合类型的基本类型部分有关。比较特殊的是，指针类型既可以是顶层const也可以是底层const，这一点和其他类型相比区别明显。 123456int i = 0; int *const p1 = &amp;1; // 不能改变p1的值，这是一个顶层const。const int ci = 42; // 不能改变ci的值，这是一个顶层constconst int *p2 = &amp;ci // 允许改变p2的值，这是一个底层constconst int *const p3 = p2; // 不能修改p3的值，右边顶层const，左边是底层constconst int &amp;r = ci; // 用于声明引用的const都是底层const 当执行对象的拷贝操作时，常量是顶层const还是底层const区别明显。其中，顶层const不受什么影响：12i = ci; // 正确：拷贝ci的值，ci是一个顶层const，对此操作无影响p2 = p3; // 正确：p2和p3指向的对象类型相同，p3顶层const的部分不受影响 执行拷贝操作并不会改变被拷贝对象的值，因此，拷入和拷出的对象是否是常量都没什么影响。 另一方面，底层const的限制却不能忽视。当执行对象的拷贝操作时，拷入和拷出的对象必须具有相同的底层const资格，或者来年各个对象的数据类型必须能够转换。一般来说，非常量可以转换成常量，反之则不行：12345int *p = p3; // 错误，p3有const定义而p没有。p2 = p3; // 正确：p2和p3都是底层constp2 = &amp;1; // 正确，int*能转换成 const int*int &amp;r = ci; // 错误，普通的int&amp;不能绑定在int常量上const int &amp;r2 = 1; // 正确：const int&amp;可以绑定到一个普通int上。 p3即是顶层const也是底层const，拷贝p3时可以不在乎它是一个顶层const，但是必须说清楚它指向的对象得是一个常量。因此，不能用p3区初始化p，因为p指向的是一个普通的(非常量)整数。另一方面，p3的值可以赋给p2，是因为这两个指针都是底层const，尽管p3同时也是一个常量指针(顶层const)，仅就这次赋值而言不会有什么影响。 constexpr和常量表达式常量表达式是指值不会发生改变并且在编译过程就能得到计算结果的表达式。显然，字面值属于常量表达式。用常量表达式初始化的const对象也是常量表达式。后面将会提到。C++语言中有几种情况下是要用到常量表达式的。1234const int max_files = 20; //const int limit = max_files + 1; //int staff_size = 27; //const int sz = get_size(); // 尽管staff_size的初始值是个字面值常量，但由于它的数据类型只是一个普通int而非const_int，所以他不属于常量表达式。另一方面，尽管sz本身是一个常量，但它的具体值知道运行时才能获取到，所以也不是常量表达式。","categories":[{"name":"C++","slug":"C","permalink":"http://gmle.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://gmle.github.io/tags/C/"},{"name":"C++ Primer","slug":"C-Primer","permalink":"http://gmle.github.io/tags/C-Primer/"}]},{"title":"对C中堆内存的理解","slug":"对堆的内存分配的理解","date":"2017-10-24T05:59:18.412Z","updated":"2017-11-03T09:35:17.741Z","comments":true,"path":"2017/10/24/对堆的内存分配的理解/","link":"","permalink":"http://gmle.github.io/2017/10/24/对堆的内存分配的理解/","excerpt":"初学者对于堆栈的认识，欢迎吐槽","text":"初学者对于堆栈的认识，欢迎吐槽 C++中内存划分c++划分为五个区： 堆 堆，就是那些由new分配的内存块，他们的释放编译器不去管，由我们的应用程序去控制，一般一个new就要对应一个delete.如果程序员没有释放掉，那么在程序结束后，操作系统会自动回收。 栈 栈，就是那些由编译器在需要的时候分配，在不需要的时候自动清楚的变量的存储区。里面的变量通常是局部变量、函数参数等。 自由存储区 就是那些由malloc等分配的内存块，他和堆是十分相似的，不过它是用free来结束自己的生命的。 全局、静态存储区 全局变量和静态变量被分配到同一块内存中，在以前的C语言中，全局变量又分为初始化的和未初始化的，在C++里面没有这个区分了，他们共同占用同一块内存区。 常亮存储区 这是一块比较特殊的存储区，他们里面存放的是常量，不允许修改(当然，你要通过非正当手段也可以修改) C++内存区域中堆和栈的区别：管理方式不同： 栈是由编译器自动管理，无需我们手工控制。 堆的释放由程序员完成，容易产生内存泄漏。 空间大小不同： 堆内存很大。 栈内存可修改，但默认好像非常小。 内存碎片： 对于堆来讲，频繁的new/delete势必会造成内存空间的不连续，从而造成大量的碎片，使程序效率降低。 对于栈来讲，则不会存在这个问题。 生长方向不同： 对于堆来讲，生长方向是向上的，也就是向着内存地址增加的方向； 对于栈来讲，它的生长方式是向下的，是向着内存地址减小的方向增长。 加深理解的话： 在函数体中定义的变量通常是在栈上，定义一个变量，内存就少一点。就像一杯水喝一口 堆用malloc， calloc， realloc等分配内存的函数分配得到的就是在堆上123456789101112int a = 0; //全局初始化区 char *p1; //全局未初始化区 void main() &#123; int b; //栈 char s[] = \"abc\"; //栈 char *p2; //栈 char *p3 = \"123456\"; //123456&#123;post.content&#125;在常量区，p3在栈上 static int c = 0; //全局(静态)初始化区 p1 = (char *)malloc(10); //分配得来得10字节的区域在堆区 p2 = (char *)malloc(20); //分配得来得20字节的区域在堆区 strcpy(p1, \"123456\"); //123456&#123;post.content&#125;放在常量区，编译器可能会将它与p3所指向的\"123456\"优化成一块 &#125;","categories":[{"name":"C++","slug":"C","permalink":"http://gmle.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://gmle.github.io/tags/C/"},{"name":"内存管理","slug":"内存管理","permalink":"http://gmle.github.io/tags/内存管理/"}]},{"title":"Spark-HA的worker问题","slug":"Spark-HA的worker问题","date":"2017-10-10T03:36:26.000Z","updated":"2017-11-06T08:39:00.000Z","comments":true,"path":"2017/10/10/Spark-HA的worker问题/","link":"","permalink":"http://gmle.github.io/2017/10/10/Spark-HA的worker问题/","excerpt":"关于 HA 中 Spark worker节点连接Master的问题","text":"关于 HA 中 Spark worker节点连接Master的问题 问题：Spark Woker 不去连接ALIVE Master机器： 192.168.1.128 Master 192.168.1.129 Master Worker 192.168.1.130 Worker 启动时两个Master的状态不可控，不知道哪个是ALIVE的Master，worker节点在连接Master的时候，会判断当前Master的状态是否为ALIVE，如果为StandBy，则不继续链接，然后去寻找ALIVE，直到找到ALIVE节点的MASTER。 现在的问题是 Worker在找到StandBy节点后，并没有去寻找新的Master，导致了worker注册不到集群上，自动关闭。 原因待定。 根据一些帖子发现，如果配置了Spark on yarn ，则 Spark HA 基本没有任何作用。 错误日志 Terminal1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties17/10/09 13:05:08 INFO Worker: Registered signal handlers for [TERM, HUP, INT]17/10/09 13:05:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable17/10/09 13:05:09 INFO SecurityManager: Changing view acls to: root17/10/09 13:05:09 INFO SecurityManager: Changing modify acls to: root17/10/09 13:05:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)17/10/09 13:05:10 INFO Utils: Successfully started service 'sparkWorker' on port 39766.17/10/09 13:05:10 INFO Worker: Starting Spark worker 192.168.10.129:39766 with 4 cores, 4.0 GB RAM17/10/09 13:05:10 INFO Worker: Running Spark version 1.6.017/10/09 13:05:10 INFO Worker: Spark home: /opt/dkh/spark-1.6.0-bin-hadoop2.617/10/09 13:05:11 INFO Utils: Successfully started service 'WorkerUI' on port 8081.17/10/09 13:05:11 INFO WorkerWebUI: Started WorkerWebUI at http://192.168.10.129:808117/10/09 13:05:11 INFO Worker: Connecting to master dkm:7077...17/10/09 13:05:11 WARN Worker: Failed to connect to master dkm:7077java.io.IOException: Failed to connect to dkm/192.168.10.128:7077 at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216) at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167) at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200) at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187) at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Caused by: java.net.ConnectException: 拒绝连接: dkm/192.168.10.128:7077 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) ... 1 more17/10/09 13:05:24 INFO Worker: Retrying connection to master (attempt # 1)17/10/09 13:05:24 INFO Worker: Connecting to master dkm:7077...17/10/09 13:05:37 INFO Worker: Retrying connection to master (attempt # 2)17/10/09 13:05:37 INFO Worker: Connecting to master dkm:7077...17/10/09 13:05:50 INFO Worker: Retrying connection to master (attempt # 3)17/10/09 13:05:50 INFO Worker: Connecting to master dkm:7077...17/10/09 13:06:03 INFO Worker: Retrying connection to master (attempt # 4)17/10/09 13:06:03 INFO Worker: Connecting to master dkm:7077...17/10/09 13:06:16 INFO Worker: Retrying connection to master (attempt # 5)17/10/09 13:06:16 INFO Worker: Connecting to master dkm:7077...17/10/09 13:06:29 INFO Worker: Retrying connection to master (attempt # 6)17/10/09 13:06:29 INFO Worker: Connecting to master dkm:7077...17/10/09 13:07:47 INFO Worker: Retrying connection to master (attempt # 7)17/10/09 13:07:47 INFO Worker: Connecting to master dkm:7077...17/10/09 13:09:05 INFO Worker: Retrying connection to master (attempt # 8)17/10/09 13:09:05 INFO Worker: Connecting to master dkm:7077...17/10/09 13:10:23 INFO Worker: Retrying connection to master (attempt # 9)17/10/09 13:10:23 INFO Worker: Connecting to master dkm:7077...17/10/09 13:11:41 INFO Worker: Retrying connection to master (attempt # 10)17/10/09 13:11:41 INFO Worker: Connecting to master dkm:7077...17/10/09 13:12:59 INFO Worker: Retrying connection to master (attempt # 11)17/10/09 13:12:59 INFO Worker: Connecting to master dkm:7077...17/10/09 13:14:17 INFO Worker: Retrying connection to master (attempt # 12)17/10/09 13:14:17 INFO Worker: Connecting to master dkm:7077...17/10/09 13:15:35 INFO Worker: Retrying connection to master (attempt # 13)17/10/09 13:15:35 INFO Worker: Connecting to master dkm:7077...17/10/09 13:16:53 INFO Worker: Retrying connection to master (attempt # 14)17/10/09 13:16:53 INFO Worker: Connecting to master dkm:7077...17/10/09 13:18:11 INFO Worker: Retrying connection to master (attempt # 15)17/10/09 13:18:11 INFO Worker: Connecting to master dkm:7077...17/10/09 13:19:29 INFO Worker: Retrying connection to master (attempt # 16)17/10/09 13:19:29 INFO Worker: Connecting to master dkm:7077...17/10/09 13:20:47 ERROR Worker: All masters are unresponsive! Giving up. 既然如此，那干脆不启动第二个Master，Start-all 后，会发现集群正常，但是没有第二个Master。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://gmle.github.io/categories/Spark/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://gmle.github.io/tags/Shell/"},{"name":"Spark","slug":"Spark","permalink":"http://gmle.github.io/tags/Spark/"},{"name":"Linux","slug":"Linux","permalink":"http://gmle.github.io/tags/Linux/"}]},{"title":"关于Spark环境变量问题","slug":"关于Spark环境变量问题","date":"2017-09-13T02:07:56.000Z","updated":"2017-11-06T08:44:11.000Z","comments":true,"path":"2017/09/13/关于Spark环境变量问题/","link":"","permalink":"http://gmle.github.io/2017/09/13/关于Spark环境变量问题/","excerpt":"记录一次因为spark内置环境问题引发的惨案","text":"记录一次因为spark内置环境问题引发的惨案 问题：Spark在spark-env.sh中的环境变量不生效 错误日志 Terminal12345678[root@ceshi3 sbin]# ./start-slaves.sh - /usr/local/spark-1.6.0-bin-hadoop2.6/conf/spark-env.sh: line 9: export: `/usr/local/spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar': not a valid identifier- ceshi3: /usr/local/spark-1.6.0-bin-hadoop2.6/conf/spark-env.sh: line 9: export: `/usr/local/spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar': not a valid identifier- ceshi3: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-ceshi3.out- ceshi3: failed to launch org.apache.spark.deploy.worker.Worker:- ceshi3: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/spark-class: line 87: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/java: 没有那个文件或目录- ceshi3: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/spark-class: line 87: exec: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/java: cannot execute: 没有那个文件或目录- ceshi3: full log in /usr/local/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-ceshi3.out 发现启动worker的时候会出现错误：12- ceshi3: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/spark-class: line 87: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/java: 没有那个文件或目录- ceshi3: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/spark-class: line 87: exec: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/java: cannot execute: 没有那个文件或目录 这个bin/java明明是$ JAVA_HOME 的，为什么会变为 $SPARK_HOME 呢 既然启动报错，而且报的是 $JAVA_HOME，那就要看几个东西:一个是正常的系统变量配置，再一个就是在要启动的服务里是否使用了这个配置变量，再确认下自己的配置是否已经有了。 查看 spark 关于环境变量的配置文件1234567891011export SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=dk31:2181,dk32:2181,dk34:2181 -Dspark.deploy.zookeeper.dir=/spark\"export JAVA_HOME=$&#123;JAVA_HOME&#125;export HADOOP_HOME=/usr/local/hadoop-2.6.0export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport SCALA_HOME=/usr/local/scala-2.10.4export SPARK_WORKER_MEMORY=4gexport SPARK_EXECUTOR_MEMORY=2gexport SPARK_DRIVER_MEMORY=1gexport SPARK_WORKER_CORES=4export SPARK_CLASSPATH=/usr/local/spark-1.6.0-bin-hadoop2.6/lib/mysql-connector-java.jarexport SPARK_CLASSPATH=$SPARK_CLASSPATH:$CLASSPATH 发现 $JAVA_HOME 变量是取的系统变量·，但是系统变量为什么取不到？ 查了下：在脚本中使用export, 只在脚本中有效，退出这个脚本，设置的变量就没有了。由于spark-class使用了 spark-env.sh 在使用的时候 已经取不到该值，所以无效了。但是想不通为什么会变成 $SPARK_HOME 的变量","categories":[{"name":"Spark","slug":"Spark","permalink":"http://gmle.github.io/categories/Spark/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://gmle.github.io/tags/Shell/"},{"name":"Spark","slug":"Spark","permalink":"http://gmle.github.io/tags/Spark/"},{"name":"Linux","slug":"Linux","permalink":"http://gmle.github.io/tags/Linux/"}]},{"title":"Maven一键安装 centos平台","slug":"yum一键安装maven","date":"2017-09-12T02:44:47.000Z","updated":"2017-09-12T02:48:22.000Z","comments":true,"path":"2017/09/12/yum一键安装maven/","link":"","permalink":"http://gmle.github.io/2017/09/12/yum一键安装maven/","excerpt":"maven一键安装","text":"maven一键安装 添加maven的仓库1wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo yum 安装1yum -y install apache-maven","categories":[{"name":"linux","slug":"linux","permalink":"http://gmle.github.io/categories/linux/"}],"tags":[{"name":"Centos","slug":"Centos","permalink":"http://gmle.github.io/tags/Centos/"},{"name":"maven","slug":"maven","permalink":"http://gmle.github.io/tags/maven/"}]},{"title":"C++标准库","slug":"C++标准库","date":"2017-08-04T09:17:18.000Z","updated":"2017-08-04T09:55:45.000Z","comments":true,"path":"2017/08/04/C++标准库/","link":"","permalink":"http://gmle.github.io/2017/08/04/C++标准库/","excerpt":"C++标准库可以分为两个部分： 标准函数库：继承自C语言； 面向对象库：是类及其相关函数的集合；","text":"C++标准库可以分为两个部分： 标准函数库：继承自C语言； 面向对象库：是类及其相关函数的集合； 标准函数库：输入 / 输出I/O ； 字符串和字符处理； 数学； 时间、日期和本地化； 动态分配； 其他； 宽字符函数；面向对象类： 标准的C++ I/O类； String类； 数值类； STL容器类； STL算法； STL函数对象； STL迭代器； STL分配器； 本地化库； 异常处理类； 杂项支持库；","categories":[{"name":"C++","slug":"C","permalink":"http://gmle.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://gmle.github.io/tags/C/"}]},{"title":"Bash/Shell调用MySQL并忽略警告","slug":"Bash调用MySql","date":"2017-06-23T06:09:03.000Z","updated":"2017-06-25T09:26:19.000Z","comments":true,"path":"2017/06/23/Bash调用MySql/","link":"","permalink":"http://gmle.github.io/2017/06/23/Bash调用MySql/","excerpt":"Shell对MySQL的调用与脚本中如何写 Shell脚本如何搞定 MySQL的增删改查","text":"Shell对MySQL的调用与脚本中如何写 Shell脚本如何搞定 MySQL的增删改查 用Shell对mysql操作非常的简单我们利用 mysql 命令去操作数据库里面的所有东西。 shell脚本1234567891011121314# 一坨一坨的运行：mysql -uroot -p123456 -e \"select * from tmp_test where tmp_name = 'a';select * from tmp_test where tmp_name = 'b';select * from tmp_test where tmp_id = 1;select tmp_name from tmp_test where tmp_id = 2;quit\"# 赋值：id=$(mysql -uroot -p123456 -e \"SELECT tmp_id from tmp_test WHERE tmp_name = 'a';\")# 会发现还有字段名字，加参数去掉字段名，只保留我们要查询的：id=$(mysql -uroot -p123456 -Bse \"SELECT tmp_id from tmp_test WHERE tmp_name = 'a';\") 过后我们会发现每次查询之后会出现警告，每次都出：1mysql: [Warning] Using a password on the command line interface can be insecure. MySQL 版本 5.6+ 的安全策略MySQL5.6版本向上有一个密码安全问题，即在命令行输入密码会出现警告： 12mysql -uroot -proot mysql: [Warning] Using a password on the command line interface can be insecure. 读取配置文件的参数也不可以，这样我们 需要指定一个mysql的配置文件作为mysql的配置输入进去： cnf配置文件my.cnf123#!/bin/bash[mysql]password=root 然后再在脚本中调用：123#!/bin/bash# 继续赋值，这样就不会出现警告信息：id=$(mysql --defaults-file=./my.cnf -uroot -Bse \"SELECT tmp_id from tmp_test WHERE tmp_name = 'a';\")","categories":[{"name":"Shell","slug":"Shell","permalink":"http://gmle.github.io/categories/Shell/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://gmle.github.io/tags/Shell/"},{"name":"Bash","slug":"Bash","permalink":"http://gmle.github.io/tags/Bash/"},{"name":"MySQL","slug":"MySQL","permalink":"http://gmle.github.io/tags/MySQL/"}]},{"title":"在Centos6.5下升级python至3.6.0","slug":"centos6.5下python2.6.6升级至3.6","date":"2017-05-11T02:16:11.000Z","updated":"2017-05-11T07:57:48.000Z","comments":true,"path":"2017/05/11/centos6.5下python2.6.6升级至3.6/","link":"","permalink":"http://gmle.github.io/2017/05/11/centos6.5下python2.6.6升级至3.6/","excerpt":"此次升级保留旧版本的环境。","text":"此次升级保留旧版本的环境。 配置系统环境安装开发工具1yum groupinstall -y developement 安装python3解码支持包1yum install -y zlib-devel openssl-devel sqlite-devel bzip2-devel 准备更新版本验证原有的python版本1python -V python 2.6.6 下载python3.6.0包1wget http://www.python.org/ftp/python/3.6.0/Python-3.6.0.tar.xz 解压编译python安装包解压12xz -d Python-3.6.0.tar.xztar -xvf Python-3.6.0.tar 编译123cd Python-3.6.0# 配置安装路径./configure --prefix=/usr/local 如果出现编译错误可能是因为gcc gcc-c++版本太低或者未安装，使用代码 1yum -y install gcc gcc-c++ 进行安装，然后重新编译./configure 执行安装1make &amp;&amp; make altinstall 建立软连接(就是快捷方式)12mv /usr/bin/python /usr/bin/python2.6.6 ##你的python版本可能不同ln -s /usr/local/bin/python3.6 /usr/bin/python 重新验证python版本， 1python -V python3.6.0 yum指令会报错，将其重新指向旧版本的python1vi /usr/bin/yum 将文件的头部#！/usr/bin/python改为#！/usr/bin/python2.6.6 安装新pip1$ wget https://pypi.python.org/packages/source/p/pip/pip-1.3.1.tar.gz --no-check-certificate 解压安装pip1234chmod +x pip-1.3.1.tar.gztar xzvf pip-1.3.1.tar.gzcd pip-1.3.1python setup.py install 查看pip安装1pip -V pip 1.3.1 from /usr/local/lib/python3.6/site-packages/pip-1.3.1-py3.6.egg (python 3.6)","categories":[{"name":"Centos","slug":"Centos","permalink":"http://gmle.github.io/categories/Centos/"}],"tags":[{"name":"Centos","slug":"Centos","permalink":"http://gmle.github.io/tags/Centos/"},{"name":"Python","slug":"Python","permalink":"http://gmle.github.io/tags/Python/"}]},{"title":"MacOS下配置Hadoop和Spark","slug":"MacOS安装Hadoop&Spark","date":"2017-05-02T07:49:44.000Z","updated":"2017-05-03T06:37:38.000Z","comments":true,"path":"2017/05/02/MacOS安装Hadoop&Spark/","link":"","permalink":"http://gmle.github.io/2017/05/02/MacOS安装Hadoop&Spark/","excerpt":"首先，准备MacOS环境略过Java、Scala、Python的环境安装，从Hadoop和Spark说起","text":"首先，准备MacOS环境略过Java、Scala、Python的环境安装，从Hadoop和Spark说起 安装Hadoop安装Hadoop，最简单的安装方式：1brew install hadoop 找到安装目录安装完成后，找到Hadoop配置文件目录： 1cd /usr/local/Cellar/hadoop/2.7.3/libexec/etc/hadoop 修改core-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/Cellar/hadoop/2.7.3/libexec/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:8020&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 修改hdfs-site.xml1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/Cellar/hadoop/2.7.3/libexec/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/Cellar/hadoop/2.7.3/libexec/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 添加环境变量123#Hadoop environment configs export HADOOP_HOME=/usr/local/Cellar/hadoop/2.7.3/libexec export PATH=$PATH:$&#123;HADOOP_HOME&#125;/bin 格式化HDFS12cd /usr/local/Cellar/hadoop/2.7.3/bin ./hdfs namenode -format 启动Hadoop12cd /usr/local/Cellar/hadoop/2.7.3/sbin ./start-all.sh 在终端输入 jps 查看java进程1231206 DataNode 1114 NameNode 1323 SecondaryNameNode 安装SparkSpark的安装也是使用 brew1brew install apache-spark 找到安装目录找到Spark配置文件目录1cd /usr/local/Cellar/apache-spark/2.1.0/libexec/conf 修改spark-env.sh1234cp spark-env.sh.template spark-env.shvi spark-env.shexport SPARK_HOME=/usr/local/Cellar/apache-spark/2.1.0/libexec export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_102.jdk/Contents/Home 加入环境变量12export SPARK_HOME=/usr/local/Cellar/apache-spark/2.1.0/libexec export PATH=$PATH:$&#123;SPARK_HOME&#125;/bin 启动Spark12cd /usr/local/Cellar/apache-spark/1.6.0/bin ./start-all.sh 查看进程12345678910jps6052 Worker6022 Master6728 Jps5546 NameNode5739 SecondaryNameNode5947 NodeManager5630 DataNode5855 ResourceManager 配置Pycharm开发spark应用打开Pycharm（我的python版本是2.7）新建xxxx，新建类：一个简单的wordcount12345678910from pyspark import SparkContextlogFile = \"/Users/admin/Desktop/BackUp\"sc = SparkContext(\"local\",\"Simple App\")logData = sc.textFile(logFile).cache()numAs = logData.filter(lambda s: 'a' in s).count()numBs = logData.filter(lambda s: 'b' in s).count()print(\"Lines with a: %i, lines with b: %i\"%(numAs, numBs)) F4打开当前可运行代码的配置项Environment Variables 选项填写：1PYTHONPATH /usr/local/Cellar/apache-spark/2.1.0/libexec/python 至此，环境完成。","categories":[{"name":"MacOS","slug":"MacOS","permalink":"http://gmle.github.io/categories/MacOS/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"MacOS","slug":"MacOS","permalink":"http://gmle.github.io/tags/MacOS/"},{"name":"Spark","slug":"Spark","permalink":"http://gmle.github.io/tags/Spark/"}]},{"title":"HBase的列簇","slug":"HBase的列簇","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:38:13.000Z","comments":true,"path":"2016/09/19/HBase的列簇/","link":"","permalink":"http://gmle.github.io/2016/09/19/HBase的列簇/","excerpt":"对于HBase中的列簇，新手都会有这样的问题 HBase的列有列族前缀和列组成， 那么一个表中列族要怎么设定？ 是只设定一个？还是越多越好？还是根据什么设定几个？","text":"对于HBase中的列簇，新手都会有这样的问题 HBase的列有列族前缀和列组成， 那么一个表中列族要怎么设定？ 是只设定一个？还是越多越好？还是根据什么设定几个？ 何时用HBase1.系统需要适应不同种类的数据格式和数据源，不能预先严格定义模式，需要处理大规模数据； 2.不强调数据之间的关系，所要存储的数据是半结构化或非结构化的； 3.数据非常稀疏； 4.想要更好的进行扩展； 比如谷歌就将BigTable用来存储网页的索引数据，索引数据就很好的满足了上面的几点要求。 HBase的结构表、行、列和单元格 先做一个简单的总结：最基本的单位是列（column），一列或者多列组成一行（row），并且由唯一的行键（row key）来确定存储。 一个表中有很多行，每一列可能有多个版本，在每一个单元格（Cell）中存储了不同的值。 HBase的行与行之间是有序的，按照row key的字典序进行排序，行键是唯一的，在一个表里只出现一次，否则就是在更新同一行，行键可以是任意的字节数组。 一行由若干列组成，其中的某些列又可以构成一个列族（column family），一个列族的所有列存储在同一个底层的存储文件里，这个文件称之为HFile。 列族需要在创建表的时候就定义好，数量也不宜过多。 列族名必须由可打印字符组成，创建表的时候不需要定义好列。 对列的引用格式通常为family：qualifier，qualifier也可以是任意的字节数组。 同一个列族里qualifier的名称应该唯一，否则就是在更新同一列，列的数量没有限制，可以有数百万个。 列值也没有类型和长度限定。 HBase会对row key的长度做检查，默认应该小于65536。 一个可视化的HBase表如下： Timestamp代表时间戳，默认由系统指定，用户也可以显示设置。使用不同的时间戳来区分不同的版本。一个单元格的不同版本的值按照时间戳降序排列在一起，在读取的时候优先取最新的值。用户可以指定每个值能保存的最大版本数. HBase的存取模式如下（表，行键，列族，列，时间戳）-&gt; 值。即一个表中的某一行键的某一列族的某一列的某一个版本的值唯一。 行数据的存取操作是原子的，可以读取任意数目的列。目前还不支持跨行事务和跨表事务。 同一列族下的数据压缩在一起，访问控制磁盘和内存都在列族层面进行。 2.自动分区 HBase中扩展和负载均衡的基本单元称作region，region本质上是以行键排序的连续存储空间。如果region过大，系统就会把它们动态拆分，相反的，就把多个region合并，以减少存储文件数量。 一个表最开始只有一个region，用户开始向表中插入数据时，系统会检查region大小，确保不会超过配置的最大值，如果超过，会从region中行键的中间值一分为二，将该region分为大小大致相等的两个region。 3.存储格式 HFile：HBase中KeyValue数据的存储格式。HFile是Hadoop的二进制格式文件。 HLog：HBase中WAL（Write-Ahead-Log，预写式日志）文件的存储格式，物理上是Hadoop的Sequence File。 HFile的格式如下图： HFile文件的长度可变，唯一固定的是File Info和Trailer。Trailer存储指向其他块的指针，它在持久化数据到文件结束时写入的，写入后，该文件就会变成不可变的数据存储文件。数据块（data blocks）中存储key-values，可以看做是一个MapFile。当block关闭操作时，第一个key会被写入index中，index文件在hfile关闭操作时写入。 KeyValue的具体格式如下图： 上图中，keytype有四种类型，分别是Put、Delete、 DeleteColumn和DeleteFamily。RowLength为2个字节，Row长度不固定，ColumnFamilyLength为2个字节，ColumnFamily长度不固定，ColumnQualifier长度不固定，TimeStamp为4个字节，KeyType为1个字节。之所以不记录ColumnQualifier的长度是因为可以通过其他字段计算得到。 HBase常用操作：List；Create；Put；Scan；Get；Delete；Disable；Drop； 列簇的设计1、列簇的设计需要根据你的业务。那些可能被反复修改的数据表尽量使用单列簇。 每个列簇在HDFS都有一个独立的HFILE，当某个ROWKEY的某个列簇数据被冲刷时，这个ROWKEY连带的其他列簇数据也会被一起冲刷，I/O负担很大。 APACHE官方也提倡多列簇的设计方案，单列簇性能是最高的。 而持久型数据，也就是一次写入，从不修改的数据，可以使用多列簇，原理相同，但目前任然提倡单列簇设计模式 2、多列簇的效率问题参照1 3、所谓列簇分组，就相当于关系习惯数据库中，两个表被纵向合并，形成一张双列簇的表","categories":[{"name":"HBase","slug":"HBase","permalink":"http://gmle.github.io/categories/HBase/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"HBase","slug":"HBase","permalink":"http://gmle.github.io/tags/HBase/"},{"name":"列族","slug":"列族","permalink":"http://gmle.github.io/tags/列族/"}]},{"title":"MapReduce架构及原理","slug":"MapReduce架构原理","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:37:15.000Z","comments":true,"path":"2016/09/19/MapReduce架构原理/","link":"","permalink":"http://gmle.github.io/2016/09/19/MapReduce架构原理/","excerpt":"Hadoop中MapReduce的架构以及原理。 MapReduce介绍 MapReduce 编程模型 Google提出的框架 主要用于搜索领域 一种分布式计算模型框架解决海量数据的计算问题 MapReduce将整个并行计算过程抽象到两个函数 map（映射）：对一些独立元素组成的列表的每一个元素进行指定的操作，可以高度并行 Reduce（化简）：队一个列表的元素进行合并 一个简单的MapReduce程序只需要指定map()、reduce()、input和output，剩下的事由框架来执行。","text":"Hadoop中MapReduce的架构以及原理。 MapReduce介绍 MapReduce 编程模型 Google提出的框架 主要用于搜索领域 一种分布式计算模型框架解决海量数据的计算问题 MapReduce将整个并行计算过程抽象到两个函数 map（映射）：对一些独立元素组成的列表的每一个元素进行指定的操作，可以高度并行 Reduce（化简）：队一个列表的元素进行合并 一个简单的MapReduce程序只需要指定map()、reduce()、input和output，剩下的事由框架来执行。 MapReduce特点- 高容错 - 高扩展 - 编程简单 - 适合大数据离线批量处理 Map任务处理* 读取输入文件内容，解析成key，value对。对输入文件的没一行，解析成key，value对。每一个键值对调用一次map函数 * 写自己的逻辑，处理输入的key，value，转换成新的key，value输出 * 对输出的key、value进行分区 * 对不同分区的数据，按照key进行排序】分组。相同的key的value放到一个集合中。 reduce 任务处理* 对多个map任务的输出，按照不用的分区，通过网络copy到不同的reduce节点 * 对多个map任务的输出进行合并、排序。写reduce函数自己的逻辑，对哦输入的key、value处理，转换成新的key、value输出。 * 把reduce的输出保存到文件中 MapReduce键值对格式：因为会有不同的结果，所以Reduce的 v2 会是数组的形式存储多个值。 MR过程中各个角色的作用：* jobClient：提交作业 * jobTracker：初始化作业，分配作业，TaskTracker与其进行通信，协调监控整个作业 * TaskTracker：定期与JobTracker通信，执行Map任务和Reduce任务 * HDFS：保存作业的数据、配置、jar包、结果等。 作业提交流程* 提交作业准备 * 编写自己的MR程序 * 配置作业，保罗输入输出路径等等 * 提交作业 *配置完成后，通过JobClient提交作业 * 具体功能 * 与JobTracker通信得到一个jar的存储路径和JobId * 输入输出路径检查、讲job jar拷贝到的HDFS * 写job.xml、真正提交作业。 作业初始化* 客户端提交作业后，jobTracker会讲作业加入到队列，然后进行调度，默认的是FIFO方式 * 具体功能 * 作业初始化主要是指 JobInProgress中完成的 * 读取分片信息 * 创建task包括Map和Reduce创建task包括Map和Reduce任务 ## 任务分配 * TaskTracker 与JobTracker之间的通信和任务分配是通过心跳机制实现的 * TaskTracker会主动定期向JobTracker发送报告 询问是否有任务要做， 如果有，就会申请到任务 任务执行* 如果TaskTracker拿到任务，会将所有信息拷贝到本地，包括代码、配置、分片信息等 * TarkTacker中的localizeJob()方法会被调用进行本地化，拷贝job.jar,jobconf.job.xml到本地 * TaskTracker调用launchTaskForJob()方法加载启动任务","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://gmle.github.io/tags/MapReduce/"},{"name":"架构","slug":"架构","permalink":"http://gmle.github.io/tags/架构/"},{"name":"原理","slug":"原理","permalink":"http://gmle.github.io/tags/原理/"}]},{"title":"Hadoop与HDFS的一些常用命令","slug":"HSFS中文件的一些操作命令","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:37:41.000Z","comments":true,"path":"2016/09/19/HSFS中文件的一些操作命令/","link":"","permalink":"http://gmle.github.io/2016/09/19/HSFS中文件的一些操作命令/","excerpt":"Hadoop与HDFS文件操作的一些常用命令，记个笔记。 Hadoop启动","text":"Hadoop与HDFS文件操作的一些常用命令，记个笔记。 Hadoop启动 12sudo ./sbin/start-dfs.shsudo ./sbin/start-yarn.sh 启动之后的访问地址查看任务的运行情况http://localhost:8088/ 查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件。http://localhost:50070 HDFS命令查看HDFS基本统计信息1sudo ./bin/hdfs dfsadmin -report 格式化namenode1sudo ./bin/hdfs namenode -format HSFS文件操作列出跟目录下所有的文件1sudo ./bin/hdfs dfs -ls / path1sudo ./bin/hdfs dfs -rm -r /path 增加一个path目录1sudo ./bin/hdfs dfs -mkdir /path 列出跟目录下所有的文件1sudo ./bin/hdfs dfs -ls / 递归显示path下的所有文件1sudo ./bin/hdfs dfs -lsr /path 将本地文件或目录localSrc上传到HDFS中的dest路径1sudo ./bin/hdfs dfs –put /localSrc /dest 与-put命令相同1sudo ./bin/hdfs dfs –copyFromLocal /localSrc /dest 显示文件内容到标准输出上。1sudo ./bin/hdfs dfs –cat /filename","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://gmle.github.io/tags/HDFS/"},{"name":"常用命令","slug":"常用命令","permalink":"http://gmle.github.io/tags/常用命令/"}]},{"title":"HBase的配置(伪分布式)","slug":"HBase的配置(伪分布式)","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:38:09.000Z","comments":true,"path":"2016/09/19/HBase的配置(伪分布式)/","link":"","permalink":"http://gmle.github.io/2016/09/19/HBase的配置(伪分布式)/","excerpt":"","text":"HBase的CRUD ###源码链接https://github.com/gmle/hbase","categories":[{"name":"HBase","slug":"HBase","permalink":"http://gmle.github.io/categories/HBase/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"HBase","slug":"HBase","permalink":"http://gmle.github.io/tags/HBase/"}]},{"title":"hadoop常见面试题目（不定期更新）","slug":"hadoop常见面试题目","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:38:19.000Z","comments":true,"path":"2016/09/19/hadoop常见面试题目/","link":"","permalink":"http://gmle.github.io/2016/09/19/hadoop常见面试题目/","excerpt":"整理一些面试题，以便与日后查看（不定期更新） 2016.04.21 – 更新下面哪个程序负责 HDFS 数据存储？ a)NameNode b)Jobtracker c)Datanode d)secondaryNameNode e)tasktracker","text":"整理一些面试题，以便与日后查看（不定期更新） 2016.04.21 – 更新下面哪个程序负责 HDFS 数据存储？ a)NameNode b)Jobtracker c)Datanode d)secondaryNameNode e)tasktracker HDfS 中的 block 默认保存几份？ a)3 份 b)2 份 c)1 份 d)不确定 下列哪个程序通常与 NameNode 在一个节点启动？ a)SecondaryNameNode b)DataNode c)TaskTracker d)Jobtracker &gt;此题分析： hadoop的集群是基于master/slave模式，namenode和jobtracker属于master，datanode和tasktracker属于slave，master只有一个，而slave有多个SecondaryNameNode内存需求和NameNode在一个数量级上，所以通常secondary NameNode（运行在单独的物理机器上）和NameNode运行在不同的机器上。 JobTracker和TaskTracker； JobTracker 对应于 NameNode； TaskTracker 对应于 DataNode； DataNode 和NameNode 是针对数据存放来而言的； JobTracker和TaskTracker是对于MapReduce执行而言的。 mapreduce中几个主要概念，mapreduce整体上可以分为这么几条执行线索：obclient，JobTracker与TaskTracker。 1、JobClient会在用户端通过JobClient类将应用已经配置参数打包成jar文件存储到hdfs，并把路径提交到Jobtracker,然后由JobTracker创建每一个Task（即MapTask和ReduceTask）并将它们分发到各个TaskTracker服务中去执行。 2、JobTracker是一个master服务，软件启动之后JobTracker接收Job，负责调度Job的每一个子任务task运行于TaskTracker上，并监控它们，如果发现有失败的task就重新运行它。一般情况应该把JobTracker部署在单独的机器上。 3、TaskTracker是运行在多个节点上的slaver服务。TaskTracker主动与JobTracker通信，接收作业，并负责直接执行每一个任务。TaskTracker都需要运行在HDFS的DataNode上。 Hadoop 作者 答案C Doug cutting a)Martin Fowler b)Kent Beck c)Doug cutting 下列哪项通常是集群的最主要瓶颈: a)CPU b)网络 c)磁盘IO d)内存 &gt;该题解析： 首先集群的目的是为了节省成本，用廉价的pc机，取代小型机及大型机。小型机和大型机有什么特点？ 1.cpu处理能力强 2.内存够大 所以集群的瓶颈不可能是a和d 3.网络是一种稀缺资源，但是并不是瓶颈。 4.由于大数据面临海量数据，读写数据都需要io，然后还要冗余数据，hadoop一般备3份数据，所以IO就会打折扣。 关于 SecondaryNameNode 哪项是正确的？ a)它是 NameNode 的热备 b)它对内存没有要求 c)它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 d)SecondaryNameNode 应与 NameNode 部署到一个节点。 下列哪项可以作为集群的管理？ a)Puppet b)Pdsh c)Cloudera Manager d)Zookeeper Cloudera 提供哪几种安装 CDH 的方法？ a)Cloudera manager b)Tarball c)Yum d)Rpm Ganglia 不仅可以进行监控，也可以进行告警。（ 正确） 分析：此题的目的是考Ganglia的了解。严格意义上来讲是正确。ganglia作为一款最常用的Linux环境中的监控软件，它擅长的的是从节点中按照用户的需求以较低的代价采集数据。但是ganglia在预警以及发生事件后通知用户上并不擅长。最新的ganglia已经有了部分这方面的功能。但是更擅长做警告的还有Nagios。Nagios，就是一款精于预警、通知的软件。通过将Ganglia和Nagios组合起来，把Ganglia采集的数据作为Nagios的数据源，然后利用Nagios来发送预警通知，可以完美的实现一整套监控管理的系统。 Block Size 是不可以修改的。（错误 ） 分析：它是可以被修改的Hadoop的基础配置文件是hadoop-default.xml，默认建立一个Job的时候会建立Job的Config，Config首先读入hadoop-default.xml的配置，然后再读入hadoop-site.xml的配置（这个文件初始的时候配置为空），hadoop-site.xml中主要配置需要覆盖的hadoop-default.xml的系统级配置。 Nagios 不可以监控 Hadoop 集群，因为它不提供 Hadoop 支持。（错误 ） 分析：Nagios是集群监控工具，而且是云计算三大利器之一 如果 NameNode 意外终止，SecondaryNameNode 会接替它使集群继续工作。（错误 ） 分析：SecondaryNameNode是帮助恢复，而不是替代，如何恢复，可以查看 Cloudera CDH 是需要付费使用的。（错误 ） 分析：第一套付费产品是Cloudera Enterpris，Cloudera Enterprise在美国加州举行的 Hadoop 大会 (Hadoop Summit) 上公开，以若干私有管理、监控、运作工具加强 Hadoop 的功能。收费采取合约订购方式，价格随用的 Hadoop 叢集大小变动。 Hadoop 是 Java 开发的，所以 MapReduce 只支持 Java 语言编写。（错误 ） 分析：rhadoop是用R语言开发的，MapReduce是一个框架，可以理解是一种思想，可以使用其他语言开发。 Hadoop 支持数据的随机读写。（错 ） 分析：lucene是支持随机读写的，而hdfs只支持随机读。但是HBase可以来补救。HBase提供随机读写，来解决Hadoop不能处理的问题。HBase自底层设计开始即聚焦于各种可伸缩性问题：表可以很“高”，有数十亿个数据行；也可以很“宽”，有数百万个列；水平分区并在上千个普通商用机节点上自动复制。表的模式是物理存储的直接反映，使系统有可能提高高效的数据结构的序列化、存储和检索。 NameNode 负责管理 metadata，client 端每次读写请求，它都会从磁盘中读取或则会写入 metadata 信息并反馈 client 端。（错误） 此题分析： NameNode 不需要从磁盘读取 metadata，所有数据都在内存中，硬盘上的只是序列化的结果，只有每次 namenode 启动的时候才会读取。 1）文件写入 Client向NameNode发起文件写入的请求。 NameNode根据文件大小和文件块配置情况，返回给Client它所管理部分DataNode的信息。 Client将文件划分为多个Block，根据DataNode的地址信息，按顺序写入到每一个DataNode块中。 2）文件读取 Client向NameNode发起文件读取的请求。 NameNode 本地磁盘保存了 Block 的位置信息。（ 个人认为正确，欢迎提出其它意见） 分析：DataNode是文件存储的基本单元，它将Block存储在本地文件系统中，保存了Block的Meta-data，同时周期性地将所有存在的Block信息发送给NameNode。NameNode返回文件存储的DataNode的信息。 Client读取文件信息。 DataNode 通过长连接与 NameNode 保持通信。（ ） 这个有分歧：具体正在找这方面的有利资料。下面提供资料可参考。 首先明确一下概念： （1）.长连接 Client方与Server方先建立通讯连接，连接建立后不断开，然后再进行报文发送和接收。这种方式下由于通讯连接一直存在，此种方式常用于点对点通讯。 （2）.短连接 Client方与Server每进行一次报文收发交易时才进行通讯连接，交易完毕后立即断开连接。此种方式常用于一点对多点通讯，比如多个Client连接一个Server. Hadoop 自身具有严格的权限管理和安全措施保障集群正常运行。（错误 ） hadoop只能阻止好人犯错，但是不能阻止坏人干坏事 Slave 节点要存储数据，所以它的磁盘越大越好。（ 错误） 分析：一旦Slave节点宕机，数据恢复是一个难题 hadoop dfsadmin –report 命令用于检测 HDFS 损坏块。（错误 ） Hadoop 默认调度器策略为 FIFO（正确 ） 集群内每个节点都应该配 RAID，这样避免单磁盘损坏，影响整个节点运行。（错误 ） 分析：首先明白什么是RAID，可以参考百科磁盘阵列。这句话错误的地方在于太绝对，具体情况具体分析。题目不是重点，知识才是最重要的。因为hadoop本身就具有冗余能力，所以如果不是很严格不需要都配备RAID。具体参考第二题。 因为 HDFS 有多个副本，所以 NameNode 是不存在单点问题的。（错误 ） 每个 map 槽就是一个线程。（错误 ） 分析：首先我们知道什么是map 槽,map 槽-&gt;map slotmap slot 只是一个逻辑值 ( org.apache.hadoop.mapred.TaskTracker.TaskLauncher.numFreeSlots )，而不是对应着一个线程或者进程 每个 map 槽就是一个线程。（错误 ） 分析：首先我们知道什么是map 槽,map 槽-&gt;map slotmap slot 只是一个逻辑值 ( org.apache.hadoop.mapred.TaskTracker.TaskLauncher.numFreeSlots )，而不是对应着一个线程或者进程 Mapreduce 的 input split 就是一个 block。（错误 ） Hadoop 环境变量中的 HADOOP_HEAPSIZE 用于设置所有 Hadoop 守护线程的内存。它默认是 200 GB。（ 错误） hadoop为各个守护进程（namenode,secondarynamenode,jobtracker,datanode,tasktracker）统一分配的内存在hadoop-env.sh中设置，参数为HADOOP_HEAPSIZE，默认为1000M。 NameNode 的 Web UI 端口是 50030，它通过 jetty 启动的 Web 服务。（错误 ） DataNode 首次加入 cluster 的时候，如果 log 中报告不兼容文件版本，那需要 NameNode执行“Hadoop namenode -format”操作格式化磁盘。（错误 ） 分析： 首先明白介绍，什么ClusterID ClusterID 添加了一个新的标识符ClusterID用于标识集群中所有的节点。当格式化一个Namenode，需要提供这个标识符或者自动生成。这个ID可以被用来格式化加入集群的其他Namenode。 二次整理 有的同学问题的重点不是上面分析内容：内容如下： 这个报错是说明 DataNode 所装的Hadoop版本和其它节点不一致，应该检查DataNode的Hadoop版本","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"面试","slug":"面试","permalink":"http://gmle.github.io/tags/面试/"}]},{"title":"HBase的数据模型","slug":"HBase的数据模型","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:38:01.000Z","comments":true,"path":"2016/09/19/HBase的数据模型/","link":"","permalink":"http://gmle.github.io/2016/09/19/HBase的数据模型/","excerpt":"目录介绍概览行列簇时间戳 此篇为参考文章","text":"目录介绍概览行列簇时间戳 此篇为参考文章 介绍HBase 的数据模型是继 Bigtable 数据模型的之后的克隆版，特别适用于密集的数据系统。 也就是HBase是通过Google的BigTable演变而来 概览看简单点，HBase可以概括成一个Map &gt;Map&lt;byte[], Map&lt;byte[], Map&lt;byte[], Map&lt;Long, byte[]&gt;&gt;&gt; 第一个Map是映射从 row keys 到 column families。 第二个Map是映射从 column families 到他们的 column keys。 第三个Map是映射从 column keys 到他们的 timestamps 。 最后，最后的Map映射 timestamps 到一个单一的值。 keys 一般为 字符串 strings， timestamps 是一个长整型 longs。而值则为一个不解释的字节数组。列的 keys 总在其 families 后面，表现如：family:key。因为一个 family 映射到另一个其他的map，这在理论上允许一个 family 包含无限个 column keys。因此，为了获取一个值，用户需要使用三个 keys 来 get ： row key+column key+timestamp -&gt; value 行键+列键+时间戳 -&gt; 值 行HBase 以数组格式来处理 row key，但 row key 本身是有字符串的形式表现。row key Map 一个特性就是以一个词典顺序来保存。例如，从1到100的数字，就是按照 1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99 这样的方式来保存。 要想以自然顺序来保存整型数，row keys 必须在左边以0填充。利用这一点，row key Map 的功能可以通过提供一个 scaner 来增强， scaner 带有一个 start row key 和 一个 stop row key。例如，如果 row keys 是日期格式 YYYYMMDD，获取 2008年7月整个月的内容，就是打开一个 scaner （20080700到20080800）。它并不关心指定的 row keys 存在与否，唯一要关心的，就是这个调用不会返回 stop row key，因此，stop row key 必须给 scaner 指定好。 列簇在 Hbase中，列成员重组具有同一性质的数据，并不限制数据类型。簇是表模式的一部分，为每行保存同种数据。与 froms rows to forws 不同的是 column keys 可以是稀少的。例如，row “20080702” 可以拥有自己的“info:”成员，该成员下有如下几个 column keys：info:aaainfo:bbbinfo:ccc同样，row “20080703”仅有：info:12342在使用 column keys 的时候，开发者必须要非常小心。因为长度为0 的 key是允许的，这说明，在前面的例子中，数据可以被插入到 column key “info:” 中。我们强烈推荐，仅仅在没有其他的 keys 指定时使用空的 key。同样，由于一个成员中的数据是同一种类的，参考性能与时间戳 ，很多属性可以指定。 时间戳根据成员的配置情况，HBase的值可以是以多版本的方式保存。缺省情况下，HBase将每个新值的时间戳设置为当前时间 milliseconds，并且当一个 cell 被请求时，返回最新的版本。开发者可以在插入数据时自定义时间戳，然后再通过指定这个时间戳来重新获取该值。Family Attributes成员属性可以为每一个簇指定下面的属性：","categories":[{"name":"HBase","slug":"HBase","permalink":"http://gmle.github.io/categories/HBase/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"HBase","slug":"HBase","permalink":"http://gmle.github.io/tags/HBase/"},{"name":"数据模型","slug":"数据模型","permalink":"http://gmle.github.io/tags/数据模型/"}]},{"title":"Scala中的 Option、Some、None、Null、Nil、Nothing","slug":"Scala中的几个类型","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:37:09.000Z","comments":true,"path":"2016/09/19/Scala中的几个类型/","link":"","permalink":"http://gmle.github.io/2016/09/19/Scala中的几个类型/","excerpt":"Scala的一些类型。","text":"Scala的一些类型。 Scala的Option类型：Option可以存储任意类型的值，而Option的实例就是 Some 和 None 对象实例。 Some 和 None 都是 Option 的子类，而且都是 final 类型，所以没有派生子类。 Option的数据存取Option的数据存取就是对 Some 对象的操作。 12345678910class optionTest extends TestCase&#123; val capitals = Map(\"france\" -&gt; \"paris\", \"japan\" -&gt; \"tokyo\") def test1() &#123; val a = capitals get \"france\" val b = capitals get \"a\" println(a) //Some（paris） println(b) //None &#125; 结果对象类型为Some或者None 当程序给你回传Some的时候，代表这个函数成功的给了你一个String， 而你可以通过get()函数拿到那个String； 如果程序返回的是None，也就是没有String返回的时候，如果还要调用，则会抛出异常：NoSuchElementExpection。 Scala的Null类型：Null是所有AnyRef的子类，在Scala的类型系统中，AnyRef是Any的子类，同是Any子类的还有AnyVal。 对应Java值类型的所有类型都是AnyVal的子类。 所以Null可以赋值给虽有的引用类型，而不能赋值给值类型，这个java的语义是相同的。 null是Null的唯一对象。 Scala的Nothing类型：Nothing是所有类型的子类，也是Null的子类，Nothing没有对象，但是可以用来定义类型。 例如，如果一个方法抛出异常，则异常的返回值类型就是Nothing（不会返回） 1234def get(index:Int):Int = &#123; if(x&lt;0) throw new Expection(...) else ...&#125; if是表达式，必然有返回值，返回值必然会有类型。 如果x&lt;0抛出异常，返回值的类型为Nothing，Nothing也是Int的子类， 所以，if表达式的返回类型为Int，get方法的返回值类型也为Int Scala的Nil类型：Nil是一个空List,定义为 List[Nothing]，根据List的定义 LIst[+A]，所有的Nil是虽有List[T]的子类。","categories":[{"name":"Scala","slug":"Scala","permalink":"http://gmle.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://gmle.github.io/tags/Scala/"},{"name":"Option","slug":"Option","permalink":"http://gmle.github.io/tags/Option/"},{"name":"Some","slug":"Some","permalink":"http://gmle.github.io/tags/Some/"},{"name":"None","slug":"None","permalink":"http://gmle.github.io/tags/None/"}]},{"title":"Hadoop_HDFS底层架构","slug":"对HDFS的一些认识","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:38:42.000Z","comments":true,"path":"2016/09/19/对HDFS的一些认识/","link":"","permalink":"http://gmle.github.io/2016/09/19/对HDFS的一些认识/","excerpt":"这是我对HDFS的认知与了解，并结合网上查阅的资料进行的整合。 HDFS设计基础与目标 硬件错误是常态，因此需要冗余。 错误检测和快速、自动的恢复是HDFS最核心的架构目标 小文件不适合存储，适合存储超大文件 流式数据访问","text":"这是我对HDFS的认知与了解，并结合网上查阅的资料进行的整合。 HDFS设计基础与目标 硬件错误是常态，因此需要冗余。 错误检测和快速、自动的恢复是HDFS最核心的架构目标 小文件不适合存储，适合存储超大文件 流式数据访问 流式数据访问，即数据劈来给你读取而非随机读写，Hadoop需要的是数据分析而不是事务处理 大规模数据集 简单一致性模型，为了降低系统复杂度，对文件采用一次写多次读的理念：文件一经写入，关闭，再也不能修改。 程序采用 “数据就近” 原则分配节点执行。 HDFS的底层架构分布式文件系统 优点： - 传统文件系统最大问题是容量和吞吐来那个的限制 - 多用户多应用的并行读写是分布式文件系统产生的根源 - 扩充存储空间的成本低廉 - 物理层存储的分布式 - 基于科户籍/服务器模式 - 通常情况下基于操作系统的本地文件系统 HDFS体系结构 NameNode 管理文件系统的命名空间 记录每个文件数据快再各个DataNode上的文职和副本信息 协调客户端对文件的访问 协调客户端对文件的访问 记录命名空间内的改动或空间本身属性的改动 NameNode使用事务日志记录HDFS元数据的变化，使用映像文件存储文件系统的命名空间，包括文件映射，文件属性等 DataNode 负责所在物理节点的存储管理 一次写入，多次读取（不能修改） 文件由数据块组成，典型块的大小 0-1.0之间版本大小-&gt; 64M 1-2.x 大小为 128M 数据块尽量散布到各个节点内 事务日志 映像文件 SecondaryNameNode HDFS的高可用性 HDFS集群中NameNode存在单点故障。 躲雨只有一个NameNode的集群，如果NameNode及其出现意外downtime，那么整个集群将无法使用，知道NameNode重新启动","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://gmle.github.io/tags/HDFS/"}]},{"title":"正则表达式","slug":"正则表达式","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:38:36.000Z","comments":true,"path":"2016/09/19/正则表达式/","link":"","permalink":"http://gmle.github.io/2016/09/19/正则表达式/","excerpt":"正则表达式的基础使用。 正则表达式 --&gt; 专门勇于操作字符串； 可以理解为 ‘正确的规则’。 好处：用了一些符号来代表一些代码，书写起来更为简单。 弊端：可读性差，而且要把符号学完 但是底层也是要配合代码实现的。","text":"正则表达式的基础使用。 正则表达式 --&gt; 专门勇于操作字符串； 可以理解为 ‘正确的规则’。 好处：用了一些符号来代表一些代码，书写起来更为简单。 弊端：可读性差，而且要把符号学完 但是底层也是要配合代码实现的。 正则表达式再每个语言中都大同小异。具体语法查看相应的API 入门学习正则表达式的最好方法是从例子开始，理解例子之后再自己对例子进行修改，实验。 符号学习1、了解一下常见的正则符号： 1.1 [] ：【判断字符位上的内容】 1.2 与定义字符：都带着反斜线 .： 任意字符 \\\\d : 数字 [0-9] \\\\D : 非数字 [^0-9] \\\\w : 单词字符[a-zA-Z_0-9] 1.3 边界字符 ^ : 行开头 $ : 行结尾 \\b : 单词边界 1.4 数量词：必须结合内容 X? : x内容出现零次或一次 X* : x内容出现零次或多次 X+ : x内容出现一次或多次 X(n) : x内容出现n此 X(n,) : x内容出现至少n此 X(n,m) : x内容出现n到m此 常见功能操作字符串String类 匹配 结果为 true | false就是String类中的machers方法 12345678910111213141516171819202122232425package cn.lesion.Regular;import java.util.regex.Pattern;/** * Created in Intellij IDEA . * Author: 王乐. * Date : 16-5-17. * * 匹配手机号 */public class phoneBoolen &#123; public static void main(String[] args) &#123; //手机号 String phone = \"17093548877\"; //定义规则 String phone_Regex = \"^((13[0-9])|(15[^4,\\\\D])|(18[0,5-9]))\\\\d&#123;8&#125;$\"; System.out.println(phone+\"::\"+phone.matches(phone_Regex)); &#125;&#125; 切割 结果为 String[] 字符串数组利用String中的方法 Splite12345678910111213141516171819202122232425262728293031323334353637383940package cn.lesion.Regular;/** * Created in Intellij IDEA . * Author: 王乐. * Date : 16-5-17. */public class splitStr &#123; public static void main(String[] args) &#123;// String str = \"zhagnsan lisi wangwu\";// String regex = \" +\";// String str = \"zhangsan,lisi,wangwu\";// String regex = \",\"// String str = \"zhangsan.lisi.wangwu\";// String regex = \"\\\\.\"; //正则规则的复用：想复用 先封装 正则封装用小括号完成 //封装完成之后有编号，从1开始。规则中被小括号封装的称之为组，直接通过编号就可以调用对应的组 //调用方式直接写已有的组的编号前面加上\\\\ 如 ()\\\\1，在使用已有的第一组内容。原则：必须要先有组 String str = \"zhangsan@@@@lisi###wangwu\"; String regex = \"(.)\\\\1+\"; String[] strs = str.split(regex); for (String s: strs) &#123; System.out.println(\"--\"+s+\"--\"); &#125; &#125;&#125; 替换 结果是一个新的字符串利用String中的方法 replaceAll1234567891011121314151617181920212223242526272829303132package cn.lesion.Regular;/** * Created in Intellij IDEA . * Author: 王乐. * Date : 16-5-17. */public class replace &#123; public static void main(String[] args) &#123;/* String str = \"ajkls@@@@@fbxcv@@@@@@opi\"; //在参数列表中，其他参数钥匙用之前参数中规则的组，需要使用$组编号 str = str.replaceAll(\"(.)\\\\1+\", \"$1\");*//* String str = \"13623372344\"; //136****2344 str = str.replaceAll(\"(\\\\d&#123;3&#125;)\\\\d&#123;4&#125;(\\\\d&#123;4&#125;)\", \"$1****$2\");*/ String str = \"lzxiujcbvil1111111111111111111adgfadkgji23984567289345079\"; //替换N个以上的数字 str = str.replaceAll(\"\\\\d&#123;4,&#125;\", \"***\"); System.out.println(str); &#125;&#125; 获取 结果为 String[] 字符串数组其它三个功能内部最终是用的都是Pattern正则表达式对象。现在需要其他功能时，字符串String类中没有对象的方法。Pattern对象的使用原理 将正则表达式字符串编译成正则对象 pattern 通过pattern对象获取macher对象（匹配器对象） 通过匹配器对象对字符串进行规则的匹配，结果都在匹配器中 通过匹配器对象的功能获取结果 范例：123456//编译为正则对象Pattern p = Pattern.compile(\"a*b\");//返回一个匹配器对象Matcher m = p.matcher(\"aaaaaab\");//判断boolean b = m.matches(); 代码：12345678910111213141516171819202122232425262728293031323334353637383940414243package cn.lesion.Regular;import java.util.regex.Matcher;import java.util.regex.Pattern;/** * Created in Intellij IDEA . * Author: 王乐. * Date : 16-5-17. * * 取出 */public class takeOutWords &#123; public static void main(String[] args) &#123; String str = \"da jia zhu yi le, ming tian fang jia le!\"; //取出由三个字母组成的单词 String regex = \"\\\\b[a-zA-Z]&#123;3&#125;\\\\b\"; //将正则表达式转换为对象 Pattern p = Pattern.compile(regex); //和要操作的字符串关联，获取对应的匹配器对象 Matcher m = p.matcher(str); while (m.find()) &#123; //第一种取值方式 System.out.println(m.group()); //第二种取值方式 System.out.println(str.substring(m.start(),m.end())); &#125; &#125;&#125;","categories":[{"name":"正则表达式","slug":"正则表达式","permalink":"http://gmle.github.io/categories/正则表达式/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://gmle.github.io/tags/Java/"},{"name":"正则表达式","slug":"正则表达式","permalink":"http://gmle.github.io/tags/正则表达式/"},{"name":"JavaSE","slug":"JavaSE","permalink":"http://gmle.github.io/tags/JavaSE/"}]},{"title":"HBase的Java API","slug":"HBase的JavaAPI详解","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:37:57.000Z","comments":true,"path":"2016/09/19/HBase的JavaAPI详解/","link":"","permalink":"http://gmle.github.io/2016/09/19/HBase的JavaAPI详解/","excerpt":"HBase的常用Java API","text":"HBase的常用Java API HBase常用操作：List；Create；Put；Scan；Get；Delete；Disable；Drop； Java API的HBase操作实现可以查看前面几篇文章了解下HBase的体系结构和HBase数据视图。 连接HBase，配置必要的配置文件HBaseConfiguration是每一个hbase client都会使用到的对象，它代表的是HBase配置信息。它有两种构造方式：12public HBaseConfiguration()public HBaseConfiguration(final Configuration c) 默认的构造方式会尝试从hbase-default.xml和hbase-site.xml中读取配置。也可以指定位置去读取1234public static Configuration config = new Configuration(); static &#123; config.addResource(\"/conf/hbase-site.xml\"); &#125; 如果classpath没有这两个文件，就需要你自己设置配置。 123456Configuration config = new Configuration();config.set(“hbase.zookeeper.quorum”, “zkServer”);config.set(“hbase.zookeeper.property.clientPort”, “2181″);\\iguration config = new HBaseConfiguration(config); 建表首先加入配置文件 创建表是通过HBaseAdmin对象来操作的。HBaseAdmin负责表的META信息处理。HBaseAdmin提供了createTable这个方法： 1public void createTable(HTableDescriptor desc) HTableDescriptor 代表的是表的schema, 提供的方法中比较有用的有setMaxFileSize，指定最大的region size setMemStoreFlushSize 指定memstore flush到HDFS上的文件大小 增加列簇，也就是增加family。通过 addFamily方法1public void addFamily(final HColumnDescriptor family) HColumnDescriptor 代表的是column的schema，提供的方法比较常用的有 setTimeToLive:指定最大的TTL,单位是ms,过期数据会被自动删除。 setInMemory:指定是否放在内存中，对小表有用，可用于提高效率。默认关闭 setBloomFilter:指定是否使用BloomFilter,可提高随机查询效率。默认关闭 setCompressionType:设定数据压缩类型。默认无压缩。 setMaxVersions:指定数据最大保存的版本个数。默认为3。 简单的例子–&gt; 创建含有两个列簇的表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package cn.lesion.MyPractice;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.client.HBaseAdmin;import java.io.IOException;/** * Created in Intellij IDEA . * Author: 王乐. * Date : 16-5-4. */public class Create &#123; //定义静态常量 public static Configuration config = new Configuration(); static &#123; config.addResource(\"/conf/hbase-site.xml\"); &#125; public static void createTable(String tableName)&#123; System.out.println(\"开始建表\"); try &#123; HBaseAdmin admin = new HBaseAdmin(config); //如果创建的这个表存在的话，删除此表 if (admin.tableExists(tableName)) &#123; //禁用此表 admin.disableTable(tableName); //删除这个表 System.out.println(\"表已经存在，开始删除这个存在的表...\"); admin.deleteTable(tableName); System.out.println(\"删除表 \"+tableName+\" 成功！\"); &#125; //对象：列族（Column Family) HTableDescriptor tableDescriptor = new HTableDescriptor(tableName); tableDescriptor.addFamily(new HColumnDescriptor(\"info\")); tableDescriptor.addFamily(new HColumnDescriptor(\"message\")); //创建含有两个列簇的表 admin.createTable(tableDescriptor); &#125; catch (IOException e) &#123; e.printStackTrace(); System.out.println(\"Not Found config\"); &#125; &#125; public static void main(String[] args) &#123; createTable(\"mytable\"); &#125;&#125; 删除表删除表也是通过HBaseAdmin来操作。删除表之前首先要disable表。这是一个非常耗时的操作，所以不建议频繁删除表。删除表之前要先禁用这个表：方法：disableTable然后进行删除操作：方法:delete 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package cn.lesion.operateTable;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.MasterNotRunningException;import org.apache.hadoop.hbase.ZooKeeperConnectionException;import org.apache.hadoop.hbase.client.HBaseAdmin;import java.io.IOException;/** * Created in Intellij IDEA . * Author : 王乐. * Date : 2016.05.03.21. */public class Drop_Table &#123; private static Configuration config = new Configuration(); //Loading configuration files on startup！ static &#123; //add Create_Table config config.addResource(\"/conf/hbase-site.xml\"); &#125; public static void main(String[] args) &#123; dropTable(\"hbase_test1\"); &#125; /** * According to table's name to Drop table * @param tableName */ public static void dropTable(String tableName) &#123; try &#123; HBaseAdmin admin = new HBaseAdmin(config); System.out.println(\"Deleting “ \" + tableName + \" ”...\"); admin.disableTable(tableName); admin.deleteTable(tableName); System.out.println(\"Table“ \" + tableName + \" ”delete success\"); &#125; catch (MasterNotRunningException e) &#123; System.out.println(\"HBase service no start！\"); &#125; catch (ZooKeeperConnectionException e) &#123; System.out.println(\"Not connected to zookeeper!\"); &#125; catch (IOException e) &#123; System.out.println(\"Not found config!\");; &#125; &#125;&#125; 查询数据查询分为单条随机查询和批量查询。 单条查询是通过rowkey在table中查询某一行的数据。HTable提供了get方法来完成单条查询。 批量查询是通过制定一段rowkey的范围来查询。HTable提供了个getScanner方法来完成批量查询。 12public Result get(final Get get)public ResultScanner getScanner(final Scan scan) Get对象包含了一个Get查询需要的信息。它的构造方法有两种： 12public Get(byte [] row)public Get(byte [] row, RowLock rowLock) Rowlock是为了保证读写的原子性，你可以传递一个已经存在Rowlock，否则HBase会自动生成一个新的rowlock。 Scan对象提供了默认构造函数，一般使用默认构造函数。 setMaxVersions:指定最大的版本个数。如果不带任何参数调用setMaxVersions,表示取所有的版本。如果不掉用setMaxVersions,只会取到最新的版本。 setTimeRange:指定最大的时间戳和最小的时间戳，只有在此范围内的cell才能被获取。 setTimeStamp:指定时间戳。 setFilter:指定Filter来过滤掉不需要的信息 Scan特有的方法： setStartRow:指定开始的行。如果不调用，则从表头开始。 setStopRow:指定结束的行（不含此行）。 setBatch:指定最多返回的Cell数目。用于防止一行中有过多的数据，导致OutofMemory错误。 ResultScanner是Result的一个容器，每次调用ResultScanner的next方法，会返回Result.12public Result next() throws IOException;public Result [] next(int nbRows) throws IOException; Result代表是一行的数据。常用方法有： getRow:返回rowkey raw:返回所有的key value数组。 getValue:按照column来获取cell的值 示例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169package cn.lesion.operateTable;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.KeyValue;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;import org.apache.hadoop.hbase.filter.Filter;import org.apache.hadoop.hbase.filter.FilterList;import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.ArrayList;import java.util.List;/** * Created in Intellij IDEA . * Author : 王乐. * Date : 2016.05.03.21. */public class Query_Data &#123; private static Configuration config = new Configuration(); //Loading configuration files on startup！ static &#123; //add Create_Table config config.addResource(\"/conf/hbase-site.xml\"); &#125; public static void main(String[] args) &#123; // QueryAll(\"hbase_test\"); // QueryByRowKey(\"hbase_test\"); // QueryByCondition(\"hbase_test\"); QueryByMultiCondition(\"hbase_test\"); &#125; /** * Query all data * @param tableName */ public static void QueryAll(String tableName) &#123; HTablePool pool = new HTablePool(config, 1000); HTableInterface table = pool.getTable(tableName); try &#123; ResultScanner rs = table.getScanner(new Scan()); for (Result r : rs) &#123; System.out.println(\"get the rowkey:\" + new String(r.getRow())); for (KeyValue keyValue : r.raw()) &#123; System.out.println(\"列：\" + new String(keyValue.getFamily()) + \"====value:\" + new String(keyValue.getValue())); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 单条件查询,根据rowkey查询唯一一条记录 * Single condition query,According to rowkey query data * @param tableName */ public static void QueryByRowKey(String tableName) &#123; HTablePool pool = new HTablePool(config, 1000); HTableInterface table = pool.getTable(tableName); try &#123; // Add the rowkey or Can be defined in the parameter Get scan = new Get(\"112233bbbcccc\".getBytes());// 根据rowkey查询 Result r = table.get(scan); System.out.println(\"获得到rowkey:\" + new String(r.getRow())); for (KeyValue keyValue : r.raw()) &#123; System.out.println(\"列：\" + new String(keyValue.getFamily()) + \"====值:\" + new String(keyValue.getValue())); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 单条件查询，查询多条记录 * Single condition query, query multiple records * @param tableName */ public static void QueryByCondition(String tableName) &#123; try &#123; HTablePool pool = new HTablePool(config, 1000); HTableInterface table = pool.getTable(tableName); // 当列column1的值为aaa时进行查询 //Query when the value of the column column1 is aaa Filter filter = new SingleColumnValueFilter(Bytes .toBytes(\"column1\"), null, CompareOp.EQUAL, Bytes .toBytes(\"aaa\")); Scan s = new Scan(); s.setFilter(filter); ResultScanner rs = table.getScanner(s); for (Result r : rs) &#123; System.out.println(\"Get rowkey:\" + new String(r.getRow())); for (KeyValue keyValue : r.raw()) &#123; System.out.println(\"column：\" + new String(keyValue.getFamily()) + \"====value:\" + new String(keyValue.getValue())); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 组合条件查询 * Combination condition query * @param tableName */ public static void QueryByMultiCondition(String tableName) &#123; try &#123; HTablePool pool = new HTablePool(config, 1000); HTableInterface table = pool.getTable(tableName); List&lt;Filter&gt; filters = new ArrayList&lt;Filter&gt;(); Filter filter1 = new SingleColumnValueFilter(Bytes .toBytes(\"column1\"), null, CompareOp.EQUAL, Bytes .toBytes(\"aaa\")); filters.add(filter1); Filter filter2 = new SingleColumnValueFilter(Bytes .toBytes(\"column2\"), null, CompareOp.EQUAL, Bytes .toBytes(\"bbb\")); filters.add(filter2); Filter filter3 = new SingleColumnValueFilter(Bytes .toBytes(\"column3\"), null, CompareOp.EQUAL, Bytes .toBytes(\"ccc\")); filters.add(filter3); FilterList filterList1 = new FilterList(filters); Scan scan = new Scan(); scan.setFilter(filterList1); ResultScanner rs = table.getScanner(scan); for (Result r : rs) &#123; System.out.println(\"Get rowkey:\" + new String(r.getRow())); for (KeyValue keyValue : r.raw()) &#123; System.out.println(\"column：\" + new String(keyValue.getFamily()) + \"====value:\" + new String(keyValue.getValue())); &#125; &#125; rs.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 插入数据HTable通过put方法来插入数据。 12public void put(final Put put) throws IOExceptionpublic void put(final List puts) throws IOException 可以传递单个批Put对象或者List put对象来分别实现单条插入和批量插入。 Put提供了3种构造方式： 123public Put(byte [] row)public Put(byte [] row, RowLock rowLock)public Put(Put putToCopy) Put常用的方法有： add:增加一个Cell setTimeStamp:指定所有cell默认的timestamp,如果一个Cell没有指定timestamp,就会用到这个值。如果没有调用，HBase会将当前时间作为未指定timestamp的cell的timestamp. setWriteToWAL: WAL是Write Ahead Log的缩写，指的是HBase在插入操作前是否写Log。默认是打开，关掉会提高性能，但是如果系统出现故障(负责插入的Region Server挂掉)，数据可能会丢失。 另外HTable也有两个方法也会影响插入的性能 setAutoFlash: AutoFlush指的是在每次调用HBase的Put操作，是否提交到HBase Server。默认是true,每次会提交。如果此时是单条插入，就会有更多的IO,从而降低性能. setWriteBufferSize: Write Buffer Size在AutoFlush为false的时候起作用，默认是2MB,也就是当插入数据超过2MB,就会自动提交到Server 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package cn.lesion.operateTable;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.HTableInterface;import org.apache.hadoop.hbase.client.HTablePool;import org.apache.hadoop.hbase.client.Put;import java.io.IOException;/** * Created in Intellij IDEA . * Author : 王乐. * Date : 2016.05.03.19. */public class Insert_Data &#123; private static Configuration config = new Configuration(); //Loading configuration files on startup！ static &#123; //add Create_Table config config.addResource(\"/conf/hbase-site.xml\"); &#125; public static void main(String[] args) &#123; // Create a new table named is 'abc' insertData(\"mytest\"); &#125; /** * According to tableName to insert data * @param tableName */ private static void insertData(String tableName) &#123; System.out.println(\"start insert data ......\"); HTablePool pool = new HTablePool(config, 1000); //Not Mandatory conversion to HTable pool.getable return HTableInterface HTableInterface table = pool.getTable(tableName); // 一个PUT代表一行数据，再NEW一个PUT表示第二行数据,每行一个唯一的ROWKEY，此处rowkey为put构造方法中传入的值 // one put == a row data, if new a 'put' it's a second row data // This rowkey is unique. Put put = new Put(\"1\".getBytes()); // 本行数据的第一列 // first data --&gt; first column put.addColumn(\"name\".getBytes(), null, \"王乐\".getBytes()); // 本行数据的第三列 // second column put.addColumn(\"age\".getBytes(), null, \"19\".getBytes()); // 本行数据的第三列 // third column put.addColumn(\"sex\".getBytes(), null, \"男\".getBytes()); try &#123; table.put(put); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; System.out.println(\"end insert data ......\"); &#125;&#125; 删除数据HTable 通过delete方法来删除数据。1public void delete(final Delete delete) Delete构造方法有： 123public Delete(byte [] row)public Delete(byte [] row, long timestamp, RowLock rowLock)public Delete(final Delete d) Delete常用方法有 deleteFamily/deleteColumns:指定要删除的family或者column的数据。如果不调用任何这样的方法，将会删除整行。 注意：如果某个Cell的timestamp高于当前时间，这个Cell将不会被删除，仍然可以查出来。 示例：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package cn.lesion.operateTable;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.client.Delete;import org.apache.hadoop.hbase.client.HTable;import java.io.IOException;import java.util.ArrayList;import java.util.List;/** * Created in Intellij IDEA . * Author : 王乐. * Date : 2016.05.03.21. */public class Delete_Data &#123; private static Configuration config = new Configuration(); //Loading configuration files on startup！ static &#123; //add Create_Table config config.addResource(\"/conf/hbase-site.xml\"); &#125; public static void main(String[] args) &#123; deleteRow(\"hbase_test\",\"1\"); &#125; /** * According to table's rowkey to delete data * @param tablename * @param rowkey */ public static void deleteRow(String tablename, String rowkey) &#123; try &#123; HTable table = new HTable(config, tablename); List&lt;Delete&gt; list = new ArrayList&lt;Delete&gt;(); Delete d1 = new Delete(rowkey.getBytes()); list.add(d1); table.delete(list); System.out.println(\"Delete row success!\"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 切分表HBaseAdmin提供split方法来将table 进行split. 1public void split(final String tableNameOrRegionName) 如果提供的tableName，那么会将table所有region进行split ;如果提供的region Name，那么只会split这个region. 由于split是一个异步操作，我们并不能确切的控制region的个数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public void split(String tableName,int number,int timeout) throws Exception &#123; private static Configuration config = new Configuration(); //Loading configuration files on startup！ static &#123; //add Create_Table config config.addResource(\"/conf/hbase-site.xml\"); &#125; HBaseAdmin hAdmin = new HBaseAdmin(config); HTable hTable = new HTable(config,tableName); int oldsize = 0; t = System.currentTimeMillis(); while(true)&#123; int size = hTable.getRegionsInfo().size(); logger.info(“the region number=”+size); if(size&gt;=number ) break; if(size!=oldsize)&#123; hAdmin.split(hTable.getTableName()); oldsize = size; &#125; else if(System.currentTimeMillis()-t&gt;timeout)&#123; break; &#125; Thread.sleep(1000*10); &#125;&#125;","categories":[{"name":"HBase","slug":"HBase","permalink":"http://gmle.github.io/categories/HBase/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"HBase","slug":"HBase","permalink":"http://gmle.github.io/tags/HBase/"},{"name":"Java","slug":"Java","permalink":"http://gmle.github.io/tags/Java/"},{"name":"API","slug":"API","permalink":"http://gmle.github.io/tags/API/"}]},{"title":"Hadoop_玩转 HDFS之 ACL","slug":"玩转HDFS-ACL","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:38:39.000Z","comments":true,"path":"2016/09/19/玩转HDFS-ACL/","link":"","permalink":"http://gmle.github.io/2016/09/19/玩转HDFS-ACL/","excerpt":"Hadoop从2.4.0版本开始支持hdfs的ACL， 通俗的讲就是文件访问控制权限 下面对其进行一些测试： unnamed user (file owner) 文件的拥有者 unnamed group (file group) 文件的所属组 named user 除了文件的拥有者和拥有组之外，的其它用户 named group 除了文件的拥有者和拥有组之外，的其它用户 mask 权限掩码，用于过滤named user和named group的权限","text":"Hadoop从2.4.0版本开始支持hdfs的ACL， 通俗的讲就是文件访问控制权限 下面对其进行一些测试： unnamed user (file owner) 文件的拥有者 unnamed group (file group) 文件的所属组 named user 除了文件的拥有者和拥有组之外，的其它用户 named group 除了文件的拥有者和拥有组之外，的其它用户 mask 权限掩码，用于过滤named user和named group的权限 一、启用ACL启用ACL功能 修改hdfs-site.xml 增加如下属性 开启ACL12345678&lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 修改core-site.xml 设置用户组默认权限.1234&lt;property&gt; &lt;name&gt;fs.permissions.umask-mode&lt;/name&gt; &lt;value&gt;002&lt;/value&gt;&lt;/property&gt; 一个访问控制列表（ACL）是一组ACL词目(entries)的集合，每个ACL词目会指定一个用户/组，并赋予读/写/执行上等权限。例如：123456user::rw- user:bruce:rwx #effective:r-- group::r-x #effective:r-- group:sales:rwx #effective:r-- mask::r-- other::r-- 这里面，没有命名的用户/组即该文件的基本所属用户/组。每一个ACL都有一个掩码(mask)，如果用户不提供掩码，那么该掩码会自动根据所有ACL条目的并集来获得(属主除外）。在该文件上运行chmod会改变掩码的权限。由于掩码用于过滤，这有效地限制了权限的扩展ACL条目，而不是仅仅改变组条目，并可能丢失的其他扩展ACL条目。 定义默认 （default）ACL条目，新的子文件和目录会自动继承默认的ACL条目设置，而只有目录会有默认的ACL条目。例如： 123456789user::rwx group::r-x other::r-x default:user::rwx default:user:bruce:rwx #effective:r-x default:group::r-x default:group:sales:rwx #effective:r-x default:mask::r-x default:other::r-x ACL相关的文件API：1234567891011public void modifyAclEntries(Path path, List aclSpec) throws IOException;public void removeAclEntries(Path path, List aclSpec) throws IOException;public void public void removeDefaultAcl(Path path) throws IOException;public void removeAcl(Path path) throws IOException;public void setAcl(Path path, List aclSpec) throws IOException;public AclStatus getAclStatus(Path path) throws IOException; 命令行命令：显示文件和目录的访问控制列表。如果一个目录有默认的ACL，getfacl也可以显示默认的ACL设置。1hdfs dfs -getfacl [-R] path 设置文件和目录的ACL1hdfs dfs -setfacl [-R] [-b|-k -m|-x acl_spec path]|[--set acl_spec path] 12345-R: Use this option to recursively list ACLs for all files and directories.-b: Revoke all permissions except the base ACLs for user, groups and others.-k: Remove the default ACL.-m: Add new permissions to the ACL with this option. Does not affect existing permissions.-x: Remove only the ACL specified. 当ls的权限位输出以+结束时，那么该文件或目录正在启用一个ACL。1hdfs dfs -ls args 实际使用：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# 默认只有基本的权限控制hdfs dfs -getfacl /data# file: /data# owner: hive# group: hadoopuser::rwxgroup::r-xother::r-x#递归显示/data下所有文件的ACLhdfs dfs -getfacl -R /data# file: /data# owner: hive# group: hadoopuser::rwxgroup::r-xother::r-x# file: /data/test.zero# owner: hive# group: hadoopuser::rw-group::r--other::r--# file: /data/test.zero.2# owner: hive# group: hadoopuser::rw-group::r--other::r--#添加一个用户ACL条目hdfs dfs -setfacl -m user:hbase:rw- /data/test.zero#添加一个组ACL条目和一个用户ACL条目（如果设置一个未命名条目，可以用user::r-x，group::r-w或者other::r-x等来设置）hdfs dfs -setfacl -m group:crm:--x,user:app1:rwx /data/test.zero.2#移除一个ACL条目hdfs dfs -setfacl -x user:app1 /data/test.zero.2#“+”已开启了ACL功能hdfs dfs -ls -R /data-rw-rwxr--+ 3 hive hadoop 1073741824 2014-12-21 15:32 /data/test.zero-rw-r-xr--+ 3 hive hadoop 1073741824 2014-12-21 15:50 /data/test.zero.2# 查看当前ACL，此时mask已经被生成hdfs dfs -getfacl -R /data/test.zero.2# file: /data/test.zero.2# owner: hive# group: hadoopuser::rw-group::r--group:crm:--xmask::r-xother::r--hdfs dfs -getfacl /data/test.zero.2# 为data目录添加default权限hdfs dfs -setfacl -m default:user:debugo:rwx /datahdfs dfs -mkdir /data/d1hdfs dfs -getfacl /data/d1user::rwxuser:debugo:rwx #effective:r-xgroup::r-xmask::r-xother::r-xdefault:user::rwxdefault:user:debugo:rwxdefault:group::r-xdefault:mask::rwxdefault:other::r-x#可以看出，default虽然继承给了d1，但是被mask=r-x所过滤，所以这里还需要设置mask。此时debugo用户的权限可以被正常访问。hdfs dfs -setfacl -m mask::rwx /data/d1hdfs dfs -getfacl /data/d1# file: /data/d1# owner: hdfs# group: hadoopuser::rwxuser:debugo:rwxgroup::r-xmask::rwxother::r-x","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://gmle.github.io/tags/HDFS/"},{"name":"ACL","slug":"ACL","permalink":"http://gmle.github.io/tags/ACL/"}]},{"title":"HBase入门 — 操作数据表","slug":"HBase入门_操作数据","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:37:52.000Z","comments":true,"path":"2016/09/19/HBase入门_操作数据/","link":"","permalink":"http://gmle.github.io/2016/09/19/HBase入门_操作数据/","excerpt":"HBase中数据表的增删改查 其实底层还是最普通的MR(MapReduce)操作。 Source源码地址 GitHub/hbase","text":"HBase中数据表的增删改查 其实底层还是最普通的MR(MapReduce)操作。 Source源码地址 GitHub/hbase Compile environmentOracle JDK_1.8 create table:Position: cn.lesion.operateTable.Create_Table;123public class Create_Table &#123; ...&#125; drop tablePosition: cn.lesion.operateTable.Drop_Table;123public class Drop_Table &#123; ...&#125; delete dataPosition: cn.lesion.operateTable.Delete_Data;123public class Delete_Data &#123; ...&#125; Insert DataPosition: cn.lesion.operateTable.Insert_Data;123public class Insert_Data &#123; ...&#125; Query DataPosition: cn.lesion.operateTable.Query_Data;123public class Query_Data &#123; ...&#125;","categories":[{"name":"HBase","slug":"HBase","permalink":"http://gmle.github.io/categories/HBase/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"HBase","slug":"HBase","permalink":"http://gmle.github.io/tags/HBase/"},{"name":"CRUD","slug":"CRUD","permalink":"http://gmle.github.io/tags/CRUD/"}]},{"title":"Hadoop_Yarn架构详解","slug":"Yarn架构详解","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:37:01.000Z","comments":true,"path":"2016/09/19/Yarn架构详解/","link":"","permalink":"http://gmle.github.io/2016/09/19/Yarn架构详解/","excerpt":"Yarn架构概述 Yarn Shell操作 Yarn 高可用的配置 Yarn 架构概述* 直接源于MRv1的缺陷(原MapReduce框架的不足) * JobTracker是集群事务的集中处理点，存在单点故障(一个失败，全部受影响) * JobTracker需要完成的任务太多，既要维护job的状态又要维护job的task的状态，造成过多的资源消耗(扩展性受限)","text":"Yarn架构概述 Yarn Shell操作 Yarn 高可用的配置 Yarn 架构概述* 直接源于MRv1的缺陷(原MapReduce框架的不足) * JobTracker是集群事务的集中处理点，存在单点故障(一个失败，全部受影响) * JobTracker需要完成的任务太多，既要维护job的状态又要维护job的task的状态，造成过多的资源消耗(扩展性受限) * 在taskTracker端，用map/reduce task作为资源的表示过于简单，没有考虑到cpu、内存等资源情况，当把两个需要消耗大内存的task调度到一起，很容易出现OOM * 把资源强制划分为map/reduce slot,当只有map task时，reduce slot不能用；当只有reduce task时，map slot不能用，容易造成资源利用不足。(多计算框架各自为战，数据共享困难). Hadoop Yarn基本架构* Yarn 各模块组成 * ResourceManager * 处理客户端请求 * 启动/监控ApplicationMaster * 监控NodeManager * 资源分配与调度 * NodeManager * 单个节点上的资源管理 * 处理来自ResourceManager的名年龄 * 处理来自ApplicationMaster的命令 * ApplicationMaster * 数据切分 * 为应用程序申请资源，并分配给内部任务 * 任务监控与容错 * Yarn的容错 * ResourceManager * 存在单点故障 * 2.X版本基于ZooKeeper实现HA * NodeManager * 失败后，RM将失败任务告诉对应的AM * AM决定如何处理失败的任务 * ApplicationMaster * 失败后，由RM负责重启，AM需处理内部任务的容错问题 - Hadoop Yarn调度框架 * 双层电镀框架 * RM将资源分配给AM * AM讲资源进一步分配给各个Task * 基于资源预留的调度策略 * 资源不足时，会为Task预留，知道资源充足 * 与“all or nothing”策略不同（Apache Mesos） * HadoopYarn资源调度器 * 多类型资源调度 * 采用DRF算法 * 目前只支持CPU和内存两种资源 * 提供多种资源调度器 * FIFO：先进先出 * Fair Scheduler：公平调度器 * Capacity Scheduler：容量调度器 * 多租户资源调度器 * 支持资源按比例分配 * 支持层级队列划分方式 * 支持资源抢占 * Hadoop Yarn 的资源隔离方案 * 支持内存和CPU两种资源隔离 * 内存是一种 “决定生死`” 的资源(集群内存不够，内存一处，任务崩溃) * CPU是一种 “你更想快慢” 的资源 * 内存隔离 * 基于县城监控的方案 * 基于Cgroups的方案 * CPU隔离 * 默认不对CPu资源进行隔离 * 基于Cgroups的方案 * Hadoop Yarn资源调度语义 * 支持的语义 * 请求某个铁定节点/机架上的特定资源量 * 讲某些节点加入或移除黑名单，不再自己分配这些节点上的资源 * 请求归还某些资源 * 部支持的语义 * 请求任意节点/机架上的特定资源量 * 请求一组或几组符合某种特质的资源 * 超细粒度资源 * 动态调整Container资源 * 运行在Yarn上的计算框架 * 离线计算框架：MapReduce（处理海量数据） * DAG计算框架：Tez * 流式计算框架：Storm：（处理流式数据） * 内存计算框架：Spark（因为基于内存处理，所以特别快） * 图计算框架：Giraph、GraphLib 运行在Yarn上的计算框架(MapReduce, Spark)AM* 框架 * 离线计算框架：MapReduce（处理海量数据） * DAG计算框架：Tez * 流式计算框架：Storm：（处理流式数据） * 内存计算框架：Spark（因为基于内存处理，所以特别快） * 图计算框架：Giraph、GraphLib * Yarn应用程序类型 * 长应用程序 * Service、HTTP Server等 * 短应用程序 * MR job、Spark Job等。 Yarn 的发展前景* 服务自动化部署（集群一键安装） * 调度框架的完善 * 支持更多的资源类型（网络、磁盘等） * 支持更多的调度语义 * 长作业的在线升级 * storm的在吸纳升级 * Container资源动态调整 * 容错机制 * ResourceMapager自身容错 * NOdeManager宕机，任务不受影响 * ApplicationMaster个性化容错","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"Yarn","slug":"Yarn","permalink":"http://gmle.github.io/tags/Yarn/"}]},{"title":"Hadoop_对HDFS中文件的操作","slug":"HDFS的文件操作","date":"2016-09-19T00:53:20.000Z","updated":"2017-05-03T06:37:48.000Z","comments":true,"path":"2016/09/19/HDFS的文件操作/","link":"","permalink":"http://gmle.github.io/2016/09/19/HDFS的文件操作/","excerpt":"Hadoop没有当前目录的概念，当然也就没有 “cd” 命令 HDFS文件操作的方法。 命令行方式上一篇文章已经写到命令行方式操作HDFS，不再多说。进入传送门～","text":"Hadoop没有当前目录的概念，当然也就没有 “cd” 命令 HDFS文件操作的方法。 命令行方式上一篇文章已经写到命令行方式操作HDFS，不再多说。进入传送门～ 根据上一篇文章，写入两个文件：Web查看文件的方式：text1 : hello world.text2 : hello hadoop. Java API操作HDFS新建一个Maven项目，不再细说1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.lesion&lt;/groupId&gt; &lt;artifactId&gt;bigData&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;!-- log4j日志包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.0-beta9&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Hadoop开发包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 新建一个类，写入如下代码：123456789101112131415161718192021222324252627282930package cn.lesion.data;/** * Created in Intellij IDEA . * Author : 王乐. * Date : 2016.04.19.20. * * 说明：读取hdfs中的文件内容 */import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import java.io.InputStream;import java.net.URI;public class Hello &#123; public static void main(String[] args) throws Exception &#123; //hdfs的地址 String uri = \"hdfs://lele:9000/\"; Configuration config = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), config); // 显示在hdfs的/tmp/input下指定文件的内容 InputStream is = fs.open(new Path(\"/user/test1.txt\")); IOUtils.copyBytes(is, System.out, 1024, true); &#125;&#125; 运行之后：Hello","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://gmle.github.io/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://gmle.github.io/tags/HDFS/"},{"name":"HDFS操作","slug":"HDFS操作","permalink":"http://gmle.github.io/tags/HDFS操作/"}]}]}