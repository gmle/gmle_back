<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>忘lelele</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://gmle.github.io/"/>
  <updated>2017-11-03T07:53:18.000Z</updated>
  <id>http://gmle.github.io/</id>
  
  <author>
    <name>忘了</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>C++ Primer 读书笔记</title>
    <link href="http://gmle.github.io/2017/10/25/cpp_primer/"/>
    <id>http://gmle.github.io/2017/10/25/cpp_primer/</id>
    <published>2017-10-25T06:21:39.585Z</published>
    <updated>2017-11-03T07:53:18.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>做点什么吧</code></pre><a id="more"></a><h1 id="第一章-开始"><a href="#第一章-开始" class="headerlink" title="第一章 - 开始"></a>第一章 - 开始</h1><ul><li>最近感觉自己很浮躁，看看书吧。</li><li>我之前学过Java，对于C++这种面向对象的语言也有一定的了解</li><li>之前也模模糊糊的看过一点C++的东西，也不全面，我现在尝试以一个没有学过c语言的人去学习C++，并且拜读圣经，希望能得到不错的成果。</li><li>本书采用的C++版本为 C++11</li></ul><h2 id="1-1-编写一个简单的C-程序"><a href="#1-1-编写一个简单的C-程序" class="headerlink" title="1.1 编写一个简单的C++程序"></a>1.1 编写一个简单的C++程序</h2><ul><li>就像Java有程序入口一样，C++程序也有入口，它们的入口函数都是 <strong>main</strong> 函数。</li><li>在执行成程序的时候，系统会调用 <strong>main</strong> 来运行程序。</li></ul><p>Example：</p><pre class=" language-c++"><code class="language-c++">int main{  return 0;}</code></pre><p>我保留了Java的书写习惯，将起始的大括号放在了 main 函数的一行，当然我感觉这样写好看一些吧（也可能是习惯）。</p><p>这是一个C++里最简单的函数，这段代码的作用是返回给操作系统一个值：0 。</p><p>C++函数构成包括四个部分：</p><ul><li>返回类型<ul><li>main函数的返回值类型必须为 <strong>int</strong>，即整数类型。 int类型是一种内置类型。</li></ul></li><li>函数名<ul><li>主函数名字为main函数，自定义的函数名字可以自行取。</li></ul></li><li>参数列表<ul><li>本例中没有带有任何参数。</li></ul></li><li>函数体<ul><li>大括号括起来的语句块即为函数体，此函数体中只包含一条语句。</li><li>此语句是结束词函数的执行，并向调用者返回一个值；此返回值类型必须与函数的返回值类型相同</li></ul></li></ul><hr><blockquote><p>重要概念：类型</p></blockquote><pre><code>    因为Java是从C++演变而来，所以对于类型也有很深的认识，因为没有学过C++，没有具体了解到C++的所有类型，不方便发表自己的看法。(其实类型好像没什么看法)    类型是程序设计的最基本的概念之一，一种类型不仅仅定义了数据元素的内容，还定义了这类数据上可以进行的运算    程序所处理的数据都保存在变量中，而每个变量都有自己的类型。</code></pre><hr><h3 id="编译、运行程序"><a href="#编译、运行程序" class="headerlink" title="编译、运行程序"></a>编译、运行程序</h3><pre><code>编写好程序之后 我们就需要去编译它。</code></pre><p>编译环境我用的是<a href="http://cmake.org" target="_blank" rel="external">CMake</a>；CMake的使用，我参考了这份资料<a href="http://pan.baidu.com/s/1hrC3Ale" target="_blank" rel="external">CMake实践</a></p><h2 id="1-2-输入输出"><a href="#1-2-输入输出" class="headerlink" title="1.2 输入输出"></a>1.2 输入输出</h2><p>C++并没有定义输入输出语句，但是它有一个全面的标准库(std)来提供IO机制以及其他操作。</p><h3 id="标准输入输出对象"><a href="#标准输入输出对象" class="headerlink" title="标准输入输出对象"></a>标准输入输出对象</h3><pre><code>本示例使用 iostream 库，iostream 库中包含两个基础类型 istream 和 ostream，分别表示输入流和输出流。‘流’想要表达的是：随着时间的推移，自复式顺序生成或者消耗的。</code></pre><p>标准库定义了四个IO对象。</p><ul><li><p>为了处理输入，我们使用一个名为<strong>cin</strong>的istream类型的对象。这个对象成为标准输入。</p></li><li><p>为了处理输出，我们使用一个名为<strong>cout</strong>的ostream类型的对象。这个对象成为标准输出。</p></li><li><p>为了处理警告和错误消息，我们使用一个名为<strong>cerr</strong>的ostream类型的对象，我们称之为标准错误</p></li><li>clog则用来输出程序运行时的一般性信息。</li></ul><p>一个使用 IO 库的程序</p><pre class=" language-c++"><code class="language-c++">#include (iostream)int main(){    std::cout << " Enter two numbers: "<<std::endl;    int v1 = 0, v2 = 0;    std::cin >> v1 >> v2;    std::cout << "The sum of " << v1 << " and " << v2 << " is " <<v1+v2 << " ." <<std::endl;    return 0;}</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;做点什么吧
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="C++" scheme="http://gmle.github.io/categories/C/"/>
    
    
      <category term="C++" scheme="http://gmle.github.io/tags/C/"/>
    
      <category term="C++ Primer" scheme="http://gmle.github.io/tags/C-Primer/"/>
    
  </entry>
  
  <entry>
    <title>对C中堆内存的理解</title>
    <link href="http://gmle.github.io/2017/10/24/%E5%AF%B9%E5%A0%86%E7%9A%84%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>http://gmle.github.io/2017/10/24/对堆的内存分配的理解/</id>
    <published>2017-10-24T05:59:18.412Z</published>
    <updated>2017-10-24T05:59:38.670Z</updated>
    
    <content type="html"><![CDATA[<pre><code>初学者对于堆栈的认识，欢迎吐槽</code></pre><a id="more"></a><h2 id="C-中内存划分"><a href="#C-中内存划分" class="headerlink" title="C++中内存划分"></a>C++中内存划分</h2><p>c++划分为五个区：</p><ul><li>堆<ul><li>堆，就是那些由new分配的内存块，他们的释放编译器不去管，由我们的应用程序去控制，一般一个new就要对应一个delete.如果程序员没有释放掉，那么在程序结束后，操作系统会自动回收。</li></ul></li><li>栈<ul><li>栈，就是那些由编译器在需要的时候分配，在不需要的时候自动清楚的变量的存储区。里面的变量通常是局部变量、函数参数等。</li></ul></li><li>自由存储区<ul><li>就是那些由malloc等分配的内存块，他和堆是十分相似的，不过它是用free来结束自己的生命的。</li></ul></li><li>全局、静态存储区<ul><li>全局变量和静态变量被分配到同一块内存中，在以前的C语言中，全局变量又分为初始化的和未初始化的，在C++里面没有这个区分了，他们共同占用同一块内存区。</li></ul></li><li>常亮存储区<ul><li>这是一块比较特殊的存储区，他们里面存放的是常量，不允许修改(当然，你要通过非正当手段也可以修改)</li></ul></li></ul><h2 id="C-内存区域中堆和栈的区别："><a href="#C-内存区域中堆和栈的区别：" class="headerlink" title="C++内存区域中堆和栈的区别："></a>C++内存区域中堆和栈的区别：</h2><h3 id="管理方式不同："><a href="#管理方式不同：" class="headerlink" title="管理方式不同："></a>管理方式不同：</h3><ul><li>栈是由编译器自动管理，无需我们手工控制。</li><li>堆的释放由程序员完成，容易产生内存泄漏。</li></ul><h3 id="空间大小不同："><a href="#空间大小不同：" class="headerlink" title="空间大小不同："></a>空间大小不同：</h3><ul><li>堆内存很大。</li><li>栈内存可修改，但默认好像非常小。</li></ul><h3 id="内存碎片："><a href="#内存碎片：" class="headerlink" title="内存碎片："></a>内存碎片：</h3><ul><li>对于堆来讲，频繁的new/delete势必会造成内存空间的不连续，从而造成大量的碎片，使程序效率降低。</li><li>对于栈来讲，则不会存在这个问题。</li></ul><h3 id="生长方向不同："><a href="#生长方向不同：" class="headerlink" title="生长方向不同："></a>生长方向不同：</h3><ul><li>对于堆来讲，生长方向是向上的，也就是向着内存地址增加的方向；</li><li>对于栈来讲，它的生长方式是向下的，是向着内存地址减小的方向增长。<ul><li>加深理解的话：<ul><li>在函数体中定义的变量通常是在栈上，定义一个变量，内存就少一点。就像一杯水喝一口</li><li>堆用malloc， calloc， realloc等分配内存的函数分配得到的就是在堆上<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> a = <span class="number">0</span>; <span class="comment">//全局初始化区  </span></div><div class="line"><span class="keyword">char</span> *p1; <span class="comment">//全局未初始化区  </span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </div><div class="line">    <span class="keyword">int</span> b; <span class="comment">//栈  </span></div><div class="line">    <span class="keyword">char</span> s[] = <span class="string">"abc"</span>;          <span class="comment">//栈  </span></div><div class="line">    <span class="keyword">char</span> *p2;                  <span class="comment">//栈  </span></div><div class="line">    <span class="keyword">char</span> *p3 = <span class="string">"123456"</span>;       <span class="comment">//123456&#123;post.content&#125;在常量区，p3在栈上  </span></div><div class="line">    <span class="keyword">static</span> <span class="keyword">int</span> c = <span class="number">0</span>;          <span class="comment">//全局(静态)初始化区  </span></div><div class="line">    p1 = (<span class="keyword">char</span> *)<span class="built_in">malloc</span>(<span class="number">10</span>);   <span class="comment">//分配得来得10字节的区域在堆区  </span></div><div class="line">    p2 = (<span class="keyword">char</span> *)<span class="built_in">malloc</span>(<span class="number">20</span>);   <span class="comment">//分配得来得20字节的区域在堆区  </span></div><div class="line">    <span class="built_in">strcpy</span>(p1, <span class="string">"123456"</span>);      <span class="comment">//123456&#123;post.content&#125;放在常量区，编译器可能会将它与p3所指向的"123456"优化成一块  </span></div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;初学者对于堆栈的认识，欢迎吐槽
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="c++" scheme="http://gmle.github.io/categories/c/"/>
    
    
      <category term="c++" scheme="http://gmle.github.io/tags/c/"/>
    
      <category term="内存管理" scheme="http://gmle.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>关于Spark环境变量问题</title>
    <link href="http://gmle.github.io/2017/10/10/Spark-HA%E7%9A%84worker%E9%97%AE%E9%A2%98/"/>
    <id>http://gmle.github.io/2017/10/10/Spark-HA的worker问题/</id>
    <published>2017-10-10T03:36:26.000Z</published>
    <updated>2017-10-10T03:53:09.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题：Spark-Woker-不去连接ALIVE-Master"><a href="#问题：Spark-Woker-不去连接ALIVE-Master" class="headerlink" title="问题：Spark Woker 不去连接ALIVE Master"></a>问题：Spark Woker 不去连接ALIVE Master</h2><p>机器：</p><ul><li>192.168.1.128 Master</li><li>192.168.1.129 Master Worker</li><li><p>192.168.1.130 Worker</p><p>启动时两个Master的状态不可控，不知道哪个是ALIVE的Master，worker节点在连接Master的时候，会判断当前Master的状态是否为ALIVE，如果为StandBy，则不继续链接，然后去寻找ALIVE，直到找到ALIVE节点的MASTER。</p><p>现在的问题是 Worker在找到StandBy节点后，并没有去寻找新的Master，导致了worker注册不到集群上，自动关闭。</p><p>原因待定。</p><p>根据一些帖子发现，如果配置了Spark on yarn ，则 Spark HA 基本没有任何作用。</p></li></ul><a id="more"></a><p>错误日志</p><ul><li>Terminal<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line">Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties</div><div class="line">17/10/09 13:05:08 INFO Worker: Registered signal handlers for [TERM, HUP, INT]</div><div class="line">17/10/09 13:05:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</div><div class="line">17/10/09 13:05:09 INFO SecurityManager: Changing view acls to: root</div><div class="line">17/10/09 13:05:09 INFO SecurityManager: Changing modify acls to: root</div><div class="line">17/10/09 13:05:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)</div><div class="line">17/10/09 13:05:10 INFO Utils: Successfully started service 'sparkWorker' on port 39766.</div><div class="line">17/10/09 13:05:10 INFO Worker: Starting Spark worker 192.168.10.129:39766 with 4 cores, 4.0 GB RAM</div><div class="line">17/10/09 13:05:10 INFO Worker: Running Spark version 1.6.0</div><div class="line">17/10/09 13:05:10 INFO Worker: Spark home: /opt/dkh/spark-1.6.0-bin-hadoop2.6</div><div class="line">17/10/09 13:05:11 INFO Utils: Successfully started service 'WorkerUI' on port 8081.</div><div class="line">17/10/09 13:05:11 INFO WorkerWebUI: Started WorkerWebUI at http://192.168.10.129:8081</div><div class="line">17/10/09 13:05:11 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:05:11 WARN Worker: Failed to connect to master dkm:7077</div><div class="line">java.io.IOException: Failed to connect to dkm/192.168.10.128:7077</div><div class="line">at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)</div><div class="line">at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)</div><div class="line">at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)</div><div class="line">at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)</div><div class="line">at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)</div><div class="line">at java.util.concurrent.FutureTask.run(FutureTask.java:262)</div><div class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</div><div class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</div><div class="line">at java.lang.Thread.run(Thread.java:745)</div><div class="line">Caused by: java.net.ConnectException: 拒绝连接: dkm/192.168.10.128:7077</div><div class="line">at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</div><div class="line">at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)</div><div class="line">at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)</div><div class="line">at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)</div><div class="line">at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)</div><div class="line">at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)</div><div class="line">at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)</div><div class="line">at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)</div><div class="line">at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)</div><div class="line">... 1 more</div><div class="line">17/10/09 13:05:24 INFO Worker: Retrying connection to master (attempt # 1)</div><div class="line">17/10/09 13:05:24 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:05:37 INFO Worker: Retrying connection to master (attempt # 2)</div><div class="line">17/10/09 13:05:37 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:05:50 INFO Worker: Retrying connection to master (attempt # 3)</div><div class="line">17/10/09 13:05:50 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:06:03 INFO Worker: Retrying connection to master (attempt # 4)</div><div class="line">17/10/09 13:06:03 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:06:16 INFO Worker: Retrying connection to master (attempt # 5)</div><div class="line">17/10/09 13:06:16 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:06:29 INFO Worker: Retrying connection to master (attempt # 6)</div><div class="line">17/10/09 13:06:29 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:07:47 INFO Worker: Retrying connection to master (attempt # 7)</div><div class="line">17/10/09 13:07:47 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:09:05 INFO Worker: Retrying connection to master (attempt # 8)</div><div class="line">17/10/09 13:09:05 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:10:23 INFO Worker: Retrying connection to master (attempt # 9)</div><div class="line">17/10/09 13:10:23 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:11:41 INFO Worker: Retrying connection to master (attempt # 10)</div><div class="line">17/10/09 13:11:41 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:12:59 INFO Worker: Retrying connection to master (attempt # 11)</div><div class="line">17/10/09 13:12:59 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:14:17 INFO Worker: Retrying connection to master (attempt # 12)</div><div class="line">17/10/09 13:14:17 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:15:35 INFO Worker: Retrying connection to master (attempt # 13)</div><div class="line">17/10/09 13:15:35 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:16:53 INFO Worker: Retrying connection to master (attempt # 14)</div><div class="line">17/10/09 13:16:53 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:18:11 INFO Worker: Retrying connection to master (attempt # 15)</div><div class="line">17/10/09 13:18:11 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:19:29 INFO Worker: Retrying connection to master (attempt # 16)</div><div class="line">17/10/09 13:19:29 INFO Worker: Connecting to master dkm:7077...</div><div class="line">17/10/09 13:20:47 ERROR Worker: All masters are unresponsive! Giving up.</div></pre></td></tr></table></figure></li></ul><p>既然如此，那干脆不启动第二个Master，Start-all 后，会发现集群正常，但是没有第二个Master。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题：Spark-Woker-不去连接ALIVE-Master&quot;&gt;&lt;a href=&quot;#问题：Spark-Woker-不去连接ALIVE-Master&quot; class=&quot;headerlink&quot; title=&quot;问题：Spark Woker 不去连接ALIVE Master&quot;&gt;&lt;/a&gt;问题：Spark Woker 不去连接ALIVE Master&lt;/h2&gt;&lt;p&gt;机器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;192.168.1.128 Master&lt;/li&gt;
&lt;li&gt;192.168.1.129 Master Worker&lt;/li&gt;
&lt;li&gt;&lt;p&gt;192.168.1.130 Worker&lt;/p&gt;
&lt;p&gt;启动时两个Master的状态不可控，不知道哪个是ALIVE的Master，worker节点在连接Master的时候，会判断当前Master的状态是否为ALIVE，如果为StandBy，则不继续链接，然后去寻找ALIVE，直到找到ALIVE节点的MASTER。&lt;/p&gt;
&lt;p&gt;现在的问题是 Worker在找到StandBy节点后，并没有去寻找新的Master，导致了worker注册不到集群上，自动关闭。&lt;/p&gt;
&lt;p&gt;原因待定。&lt;/p&gt;
&lt;p&gt;根据一些帖子发现，如果配置了Spark on yarn ，则 Spark HA 基本没有任何作用。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://gmle.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://gmle.github.io/tags/Spark/"/>
    
      <category term="Shell" scheme="http://gmle.github.io/tags/Shell/"/>
    
      <category term="Linux" scheme="http://gmle.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>关于Spark环境变量问题</title>
    <link href="http://gmle.github.io/2017/09/13/%E5%85%B3%E4%BA%8ESpark%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%97%AE%E9%A2%98/"/>
    <id>http://gmle.github.io/2017/09/13/关于Spark环境变量问题/</id>
    <published>2017-09-13T02:07:56.000Z</published>
    <updated>2017-09-17T04:01:17.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题：Spark在spark-env-sh中的环境变量不生效"><a href="#问题：Spark在spark-env-sh中的环境变量不生效" class="headerlink" title="问题：Spark在spark-env.sh中的环境变量不生效"></a>问题：Spark在spark-env.sh中的环境变量不生效</h2><a id="more"></a><p>错误日志</p><ul><li>Terminal<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@ceshi3 sbin]# ./start-slaves.sh </div><div class="line">- /usr/local/spark-1.6.0-bin-hadoop2.6/conf/spark-env.sh: line 9: export: `/usr/local/spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar': not a valid identifier</div><div class="line">- ceshi3: /usr/local/spark-1.6.0-bin-hadoop2.6/conf/spark-env.sh: line 9: export: `/usr/local/spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar': not a valid identifier</div><div class="line">- ceshi3: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-ceshi3.out</div><div class="line">- ceshi3: failed to launch org.apache.spark.deploy.worker.Worker:</div><div class="line">- ceshi3:   /usr/local/spark-1.6.0-bin-hadoop2.6/bin/spark-class: line 87: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/java: 没有那个文件或目录</div><div class="line">- ceshi3:   /usr/local/spark-1.6.0-bin-hadoop2.6/bin/spark-class: line 87: exec: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/java: cannot execute: 没有那个文件或目录</div><div class="line">- ceshi3: full log in /usr/local/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-ceshi3.out</div></pre></td></tr></table></figure></li></ul><p>发现启动worker的时候会出现错误：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">- ceshi3:   /usr/local/spark-1.6.0-bin-hadoop2.6/bin/spark-class: line 87: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/java: 没有那个文件或目录</div><div class="line">- ceshi3:   /usr/local/spark-1.6.0-bin-hadoop2.6/bin/spark-class: line 87: exec: /usr/local/spark-1.6.0-bin-hadoop2.6/bin/java: cannot execute: 没有那个文件或目录</div></pre></td></tr></table></figure></p><p>这个bin/java明明是$ JAVA_HOME 的，为什么会变为 $SPARK_HOME 呢</p><p>既然启动报错，而且报的是 $JAVA_HOME，那就要看几个东西:一个是正常的系统变量配置，再一个就是在要启动的服务里是否使用了这个配置变量，再确认下自己的配置是否已经有了。</p><h2 id="查看-spark-关于环境变量的配置文件"><a href="#查看-spark-关于环境变量的配置文件" class="headerlink" title="查看 spark 关于环境变量的配置文件"></a>查看 spark 关于环境变量的配置文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=dk31:2181,dk32:2181,dk34:2181 -Dspark.deploy.zookeeper.dir=/spark"</div><div class="line">export JAVA_HOME=$&#123;JAVA_HOME&#125;</div><div class="line">export HADOOP_HOME=/usr/local/hadoop-2.6.0</div><div class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</div><div class="line">export SCALA_HOME=/usr/local/scala-2.10.4</div><div class="line">export SPARK_WORKER_MEMORY=4g</div><div class="line">export SPARK_EXECUTOR_MEMORY=2g</div><div class="line">export SPARK_DRIVER_MEMORY=1g</div><div class="line">export SPARK_WORKER_CORES=4</div><div class="line">export SPARK_CLASSPATH=/usr/local/spark-1.6.0-bin-hadoop2.6/lib/mysql-connector-java.jar</div><div class="line">export SPARK_CLASSPATH=$SPARK_CLASSPATH:$CLASSPATH</div></pre></td></tr></table></figure><p>发现 $JAVA_HOME 变量是取的系统变量·，但是系统变量为什么取不到？</p><p>查了下：在脚本中使用export, 只在脚本中有效，退出这个脚本，设置的变量就没有了。<br>由于spark-class使用了 spark-env.sh 在使用的时候 已经取不到该值，所以无效了。<br>但是想不通为什么会变成 $SPARK_HOME 的变量</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题：Spark在spark-env-sh中的环境变量不生效&quot;&gt;&lt;a href=&quot;#问题：Spark在spark-env-sh中的环境变量不生效&quot; class=&quot;headerlink&quot; title=&quot;问题：Spark在spark-env.sh中的环境变量不生效&quot;&gt;&lt;/a&gt;问题：Spark在spark-env.sh中的环境变量不生效&lt;/h2&gt;
    
    </summary>
    
      <category term="Shell" scheme="http://gmle.github.io/categories/Shell/"/>
    
    
      <category term="Spark" scheme="http://gmle.github.io/tags/Spark/"/>
    
      <category term="Shell" scheme="http://gmle.github.io/tags/Shell/"/>
    
      <category term="Linux" scheme="http://gmle.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Maven一键安装 centos平台</title>
    <link href="http://gmle.github.io/2017/09/12/yum%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85maven/"/>
    <id>http://gmle.github.io/2017/09/12/yum一键安装maven/</id>
    <published>2017-09-12T02:44:47.000Z</published>
    <updated>2017-09-12T02:48:22.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>maven一键安装</code></pre><a id="more"></a><h2 id="添加maven的仓库"><a href="#添加maven的仓库" class="headerlink" title="添加maven的仓库"></a>添加maven的仓库</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo</div></pre></td></tr></table></figure><h2 id="yum-安装"><a href="#yum-安装" class="headerlink" title="yum 安装"></a>yum 安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum -y install apache-maven</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;maven一键安装
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="linux" scheme="http://gmle.github.io/categories/linux/"/>
    
    
      <category term="Centos" scheme="http://gmle.github.io/tags/Centos/"/>
    
      <category term="maven" scheme="http://gmle.github.io/tags/maven/"/>
    
  </entry>
  
  <entry>
    <title>C++标准库</title>
    <link href="http://gmle.github.io/2017/08/04/C++%E6%A0%87%E5%87%86%E5%BA%93/"/>
    <id>http://gmle.github.io/2017/08/04/C++标准库/</id>
    <published>2017-08-04T09:17:18.000Z</published>
    <updated>2017-08-04T09:22:20.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="C-标准库可以分为两个部分："><a href="#C-标准库可以分为两个部分：" class="headerlink" title="C++标准库可以分为两个部分："></a>C++标准库可以分为两个部分：</h2><ul><li>标准函数库：继承自C语言；</li><li>面向对象库：是类及其相关函数的集合；</li></ul><a id="more"></a><h2 id="标准函数库："><a href="#标准函数库：" class="headerlink" title="标准函数库："></a>标准函数库：</h2><h4 id="输入-输出I-O-；"><a href="#输入-输出I-O-；" class="headerlink" title="输入 / 输出I/O ；"></a>输入 / 输出I/O ；</h4><ul><li>字符串和字符处理；</li><li>数学；</li><li>时间、日期和本地化；</li><li>动态分配；</li><li>其他；</li><li>宽字符函数；<h2 id="面向对象类："><a href="#面向对象类：" class="headerlink" title="面向对象类："></a>面向对象类：</h2></li><li>标准的C++ I/O类；</li><li>String类；</li><li>数值类；</li><li>STL容器类；</li><li>STL算法；</li><li>STL函数对象；</li><li>STL迭代器；</li><li>STL分配器；</li><li>本地化库；</li><li>异常处理类；</li><li>杂项支持库；</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;C-标准库可以分为两个部分：&quot;&gt;&lt;a href=&quot;#C-标准库可以分为两个部分：&quot; class=&quot;headerlink&quot; title=&quot;C++标准库可以分为两个部分：&quot;&gt;&lt;/a&gt;C++标准库可以分为两个部分：&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;标准函数库：继承自C语言；&lt;/li&gt;
&lt;li&gt;面向对象库：是类及其相关函数的集合；&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="C++" scheme="http://gmle.github.io/categories/C/"/>
    
    
      <category term="C++" scheme="http://gmle.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Bash/Shell调用MySQL并忽略警告</title>
    <link href="http://gmle.github.io/2017/06/23/Bash%E8%B0%83%E7%94%A8MySql/"/>
    <id>http://gmle.github.io/2017/06/23/Bash调用MySql/</id>
    <published>2017-06-23T06:09:03.000Z</published>
    <updated>2017-06-25T09:26:19.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>Shell对MySQL的调用与脚本中如何写</code></pre><h2 id="Shell脚本如何搞定-MySQL的增删改查"><a href="#Shell脚本如何搞定-MySQL的增删改查" class="headerlink" title="Shell脚本如何搞定 MySQL的增删改查"></a>Shell脚本如何搞定 MySQL的增删改查</h2><a id="more"></a><p>用Shell对mysql操作非常的简单<br>我们利用 mysql 命令去操作数据库里面的所有东西。</p><ul><li>shell脚本<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># 一坨一坨的运行：</span></div><div class="line">mysql -uroot -p123456 <span class="_">-e</span> <span class="string">"</span></div><div class="line">select * from tmp_test where tmp_name = 'a';</div><div class="line">select * from tmp_test where tmp_name = 'b';</div><div class="line">select * from tmp_test where tmp_id = 1;</div><div class="line">select tmp_name from tmp_test where tmp_id = 2;</div><div class="line">quit</div><div class="line">"</div><div class="line"></div><div class="line"><span class="comment"># 赋值：</span></div><div class="line">id=$(mysql -uroot -p123456 <span class="_">-e</span> <span class="string">"SELECT tmp_id from tmp_test WHERE tmp_name = 'a';"</span>)</div><div class="line"><span class="comment"># 会发现还有字段名字，加参数去掉字段名，只保留我们要查询的：</span></div><div class="line">id=$(mysql -uroot -p123456 -Bse <span class="string">"SELECT tmp_id from tmp_test WHERE tmp_name = 'a';"</span>)</div></pre></td></tr></table></figure></li></ul><p>过后我们会发现每次查询之后会出现警告，每次都出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mysql: [Warning] Using a password on the <span class="built_in">command</span> line interface can be insecure.</div></pre></td></tr></table></figure></p><h2 id="MySQL-版本-5-6-的安全策略"><a href="#MySQL-版本-5-6-的安全策略" class="headerlink" title="MySQL 版本 5.6+ 的安全策略"></a>MySQL 版本 5.6+ 的安全策略</h2><p>MySQL5.6版本向上有一个密码安全问题，即在命令行输入密码会出现警告：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mysql -uroot -proot  </div><div class="line">mysql: [Warning] Using a password on the <span class="built_in">command</span> line interface can be insecure.</div></pre></td></tr></table></figure><p>读取配置文件的参数也不可以，这样我们 需要指定一个mysql的配置文件作为mysql的配置输入进去：</p><p>cnf配置文件<br>my.cnf<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line">[mysql]</div><div class="line">password=root</div></pre></td></tr></table></figure></p><p>然后再在脚本中调用：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"><span class="comment"># 继续赋值，这样就不会出现警告信息：</span></div><div class="line">id=$(mysql --defaults-file=./my.cnf -uroot -Bse <span class="string">"SELECT tmp_id from tmp_test WHERE tmp_name = 'a';"</span>)</div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;Shell对MySQL的调用与脚本中如何写
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Shell脚本如何搞定-MySQL的增删改查&quot;&gt;&lt;a href=&quot;#Shell脚本如何搞定-MySQL的增删改查&quot; class=&quot;headerlink&quot; title=&quot;Shell脚本如何搞定 MySQL的增删改查&quot;&gt;&lt;/a&gt;Shell脚本如何搞定 MySQL的增删改查&lt;/h2&gt;
    
    </summary>
    
      <category term="Shell" scheme="http://gmle.github.io/categories/Shell/"/>
    
    
      <category term="Shell" scheme="http://gmle.github.io/tags/Shell/"/>
    
      <category term="Bash" scheme="http://gmle.github.io/tags/Bash/"/>
    
      <category term="MySQL" scheme="http://gmle.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>在Centos6.5下升级python至3.6.0</title>
    <link href="http://gmle.github.io/2017/05/11/centos6.5%E4%B8%8Bpython2.6.6%E5%8D%87%E7%BA%A7%E8%87%B33.6/"/>
    <id>http://gmle.github.io/2017/05/11/centos6.5下python2.6.6升级至3.6/</id>
    <published>2017-05-11T02:16:11.000Z</published>
    <updated>2017-05-11T02:33:41.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="此次升级保留旧版本的环境。"><a href="#此次升级保留旧版本的环境。" class="headerlink" title="此次升级保留旧版本的环境。"></a>此次升级保留旧版本的环境。</h2><a id="more"></a><h2 id="配置系统环境"><a href="#配置系统环境" class="headerlink" title="配置系统环境"></a>配置系统环境</h2><h3 id="安装开发工具"><a href="#安装开发工具" class="headerlink" title="安装开发工具"></a>安装开发工具</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum groupinstall -y developement</div></pre></td></tr></table></figure><h3 id="安装python3解码支持包"><a href="#安装python3解码支持包" class="headerlink" title="安装python3解码支持包"></a>安装python3解码支持包</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum install -y zlib-devel openssl-devel sqlite-devel bzip2-devel</div></pre></td></tr></table></figure><h2 id="准备更新版本"><a href="#准备更新版本" class="headerlink" title="准备更新版本"></a>准备更新版本</h2><h3 id="验证原有的python版本"><a href="#验证原有的python版本" class="headerlink" title="验证原有的python版本"></a>验证原有的python版本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python -V</div></pre></td></tr></table></figure><p>python 2.6.6</p><h3 id="下载python3-6-0包"><a href="#下载python3-6-0包" class="headerlink" title="下载python3.6.0包"></a>下载python3.6.0包</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget http://www.python.org/ftp/python/3.6.0/Python-3.6.0.tar.xz</div></pre></td></tr></table></figure><h3 id="解压编译python安装包"><a href="#解压编译python安装包" class="headerlink" title="解压编译python安装包"></a>解压编译python安装包</h3><h4 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">xz -d Python-3.6.0.tar.xz</div><div class="line">tar -xvf Python-3.6.0.tar</div></pre></td></tr></table></figure><h4 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cd Python-3.6.0</div><div class="line"><span class="meta">#</span> 配置安装路径</div><div class="line">./configure --prefix=/usr/local</div></pre></td></tr></table></figure><ul><li><p>如果出现编译错误可能是因为gcc gcc-c++版本太低或者未安装，使用代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum -y install gcc gcc-c++</div></pre></td></tr></table></figure></li><li><p>进行安装，然后重新编译./configure</p><h3 id="执行安装"><a href="#执行安装" class="headerlink" title="执行安装"></a>执行安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">make &amp;&amp; make altinstall</div></pre></td></tr></table></figure></li></ul><h3 id="建立软连接-就是快捷方式"><a href="#建立软连接-就是快捷方式" class="headerlink" title="建立软连接(就是快捷方式)"></a>建立软连接(就是快捷方式)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mv /usr/bin/python /usr/bin/python2.6.6    ##你的python版本可能不同</div><div class="line">ln -s /usr/local/bin/python3.6 /usr/bin/python</div></pre></td></tr></table></figure><ul><li><p>重新验证python版本，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python -V</div></pre></td></tr></table></figure></li><li><p>python3.6.0</p></li></ul><h3 id="yum指令会报错，将其重新指向旧版本的python"><a href="#yum指令会报错，将其重新指向旧版本的python" class="headerlink" title="yum指令会报错，将其重新指向旧版本的python"></a>yum指令会报错，将其重新指向旧版本的python</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vi /usr/bin/yum</div></pre></td></tr></table></figure><ul><li>将文件的头部#！/usr/bin/python改为#！/usr/bin/python2.6.6</li></ul><h3 id="安装新pip"><a href="#安装新pip" class="headerlink" title="安装新pip"></a>安装新pip</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span> wget https://pypi.python.org/packages/source/p/pip/pip-1.3.1.tar.gz --no-check-certificate</div></pre></td></tr></table></figure><h4 id="解压安装pip"><a href="#解压安装pip" class="headerlink" title="解压安装pip"></a>解压安装pip</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">chmod +x pip-1.3.1.tar.gz</div><div class="line">tar xzvf pip-1.3.1.tar.gz</div><div class="line">cd pip-1.3.1</div><div class="line">python setup.py install</div></pre></td></tr></table></figure><h4 id="查看pip安装"><a href="#查看pip安装" class="headerlink" title="查看pip安装"></a>查看pip安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip -V</div></pre></td></tr></table></figure><ul><li>pip 1.3.1 from /usr/local/lib/python3.6/site-packages/pip-1.3.1-py3.6.egg (python 3.6)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;此次升级保留旧版本的环境。&quot;&gt;&lt;a href=&quot;#此次升级保留旧版本的环境。&quot; class=&quot;headerlink&quot; title=&quot;此次升级保留旧版本的环境。&quot;&gt;&lt;/a&gt;此次升级保留旧版本的环境。&lt;/h2&gt;
    
    </summary>
    
      <category term="Centos" scheme="http://gmle.github.io/categories/Centos/"/>
    
    
      <category term="Centos" scheme="http://gmle.github.io/tags/Centos/"/>
    
      <category term="Python" scheme="http://gmle.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>MacOS下配置Hadoop和Spark</title>
    <link href="http://gmle.github.io/2017/05/02/MacOS%E5%AE%89%E8%A3%85Hadoop&amp;Spark/"/>
    <id>http://gmle.github.io/2017/05/02/MacOS安装Hadoop&amp;Spark/</id>
    <published>2017-05-02T07:49:44.000Z</published>
    <updated>2017-05-03T06:37:38.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="首先，准备MacOS环境"><a href="#首先，准备MacOS环境" class="headerlink" title="首先，准备MacOS环境"></a>首先，准备MacOS环境</h2><pre><code>略过Java、Scala、Python的环境安装，从Hadoop和Spark说起</code></pre><a id="more"></a><h2 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h2><p>安装Hadoop，最简单的安装方式：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">brew install hadoop</div></pre></td></tr></table></figure></p><h4 id="找到安装目录"><a href="#找到安装目录" class="headerlink" title="找到安装目录"></a>找到安装目录</h4><pre><code>安装完成后，找到Hadoop配置文件目录：</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/Cellar/hadoop/2.7.3/libexec/etc/hadoop</div></pre></td></tr></table></figure><h4 id="修改core-site-xml"><a href="#修改core-site-xml" class="headerlink" title="修改core-site.xml"></a>修改core-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/Cellar/hadoop/2.7.3/libexec/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><h4 id="修改hdfs-site-xml"><a href="#修改hdfs-site-xml" class="headerlink" title="修改hdfs-site.xml"></a>修改hdfs-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/Cellar/hadoop/2.7.3/libexec/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/Cellar/hadoop/2.7.3/libexec/tmp/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><h4 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a>添加环境变量</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Hadoop environment configs  </span></div><div class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/Cellar/hadoop/2.7.3/libexec  </div><div class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$&#123;HADOOP_HOME&#125;</span>/bin</div></pre></td></tr></table></figure><h4 id="格式化HDFS"><a href="#格式化HDFS" class="headerlink" title="格式化HDFS"></a>格式化HDFS</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/Cellar/hadoop/2.7.3/bin  </div><div class="line">./hdfs namenode -format</div></pre></td></tr></table></figure><h4 id="启动Hadoop"><a href="#启动Hadoop" class="headerlink" title="启动Hadoop"></a>启动Hadoop</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/Cellar/hadoop/2.7.3/sbin  </div><div class="line">./start-all.sh</div></pre></td></tr></table></figure><p>在终端输入 jps 查看java进程<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">1206 DataNode  </div><div class="line">1114 NameNode  </div><div class="line">1323 SecondaryNameNode</div></pre></td></tr></table></figure></p><h2 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h2><p>Spark的安装也是使用 brew<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">brew install apache-spark</div></pre></td></tr></table></figure></p><h4 id="找到安装目录-1"><a href="#找到安装目录-1" class="headerlink" title="找到安装目录"></a>找到安装目录</h4><p>找到Spark配置文件目录<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/Cellar/apache-spark/2.1.0/libexec/conf</div></pre></td></tr></table></figure></p><h4 id="修改spark-env-sh"><a href="#修改spark-env-sh" class="headerlink" title="修改spark-env.sh"></a>修改spark-env.sh</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cp spark-env.sh.template spark-env.sh</div><div class="line">vi spark-env.sh</div><div class="line"><span class="built_in">export</span> SPARK_HOME=/usr/<span class="built_in">local</span>/Cellar/apache-spark/2.1.0/libexec  </div><div class="line"><span class="built_in">export</span> JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_102.jdk/Contents/Home</div></pre></td></tr></table></figure><h4 id="加入环境变量"><a href="#加入环境变量" class="headerlink" title="加入环境变量"></a>加入环境变量</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> SPARK_HOME=/usr/<span class="built_in">local</span>/Cellar/apache-spark/2.1.0/libexec  </div><div class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin</div></pre></td></tr></table></figure><h4 id="启动Spark"><a href="#启动Spark" class="headerlink" title="启动Spark"></a>启动Spark</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd /usr/local/Cellar/apache-spark/1.6.0/bin  </div><div class="line">./start-all.sh</div></pre></td></tr></table></figure><h4 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">jps</div><div class="line"></div><div class="line">6052 Worker</div><div class="line">6022 Master</div><div class="line">6728 Jps</div><div class="line">5546 NameNode</div><div class="line">5739 SecondaryNameNode</div><div class="line">5947 NodeManager</div><div class="line">5630 DataNode</div><div class="line">5855 ResourceManager</div></pre></td></tr></table></figure><h2 id="配置Pycharm开发spark应用"><a href="#配置Pycharm开发spark应用" class="headerlink" title="配置Pycharm开发spark应用"></a>配置Pycharm开发spark应用</h2><p>打开Pycharm（我的python版本是2.7）<br>新建xxxx，新建类：一个简单的wordcount<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</div><div class="line"></div><div class="line">logFile = <span class="string">"/Users/admin/Desktop/BackUp"</span></div><div class="line">sc = SparkContext(<span class="string">"local"</span>,<span class="string">"Simple App"</span>)</div><div class="line">logData = sc.textFile(logFile).cache()</div><div class="line"></div><div class="line">numAs = logData.filter(<span class="keyword">lambda</span> s: <span class="string">'a'</span> <span class="keyword">in</span> s).count()</div><div class="line">numBs = logData.filter(<span class="keyword">lambda</span> s: <span class="string">'b'</span> <span class="keyword">in</span> s).count()</div><div class="line"></div><div class="line">print(<span class="string">"Lines with a: %i, lines with b: %i"</span>%(numAs, numBs))</div></pre></td></tr></table></figure></p><p>F4打开当前可运行代码的配置项<br>Environment Variables 选项填写：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">PYTHONPATH    /usr/<span class="built_in">local</span>/Cellar/apache-spark/2.1.0/libexec/python</div></pre></td></tr></table></figure></p><p>至此，环境完成。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;首先，准备MacOS环境&quot;&gt;&lt;a href=&quot;#首先，准备MacOS环境&quot; class=&quot;headerlink&quot; title=&quot;首先，准备MacOS环境&quot;&gt;&lt;/a&gt;首先，准备MacOS环境&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;略过Java、Scala、Python的环境安装，从Hadoop和Spark说起
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="MacOS" scheme="http://gmle.github.io/categories/MacOS/"/>
    
    
      <category term="Hadoop" scheme="http://gmle.github.io/tags/Hadoop/"/>
    
      <category term="MacOS" scheme="http://gmle.github.io/tags/MacOS/"/>
    
      <category term="Spark" scheme="http://gmle.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce架构及原理</title>
    <link href="http://gmle.github.io/2016/09/19/MapReduce%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"/>
    <id>http://gmle.github.io/2016/09/19/MapReduce架构原理/</id>
    <published>2016-09-19T00:53:20.000Z</published>
    <updated>2017-05-03T06:37:15.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>Hadoop中MapReduce的架构以及原理。</code></pre><h2 id="MapReduce介绍"><a href="#MapReduce介绍" class="headerlink" title="MapReduce介绍"></a>MapReduce介绍</h2><ul><li>MapReduce 编程模型<ul><li>Google提出的框架 主要用于搜索领域</li><li>一种分布式计算模型框架解决海量数据的计算问题</li><li>MapReduce将整个并行计算过程抽象到两个函数<ul><li>map（映射）：对一些独立元素组成的列表的每一个元素进行指定的操作，可以高度并行</li><li>Reduce（化简）：队一个列表的元素进行合并</li></ul></li><li>一个简单的MapReduce程序只需要指定map()、reduce()、input和output，剩下的事由框架来执行。</li></ul></li></ul><a id="more"></a><h2 id="MapReduce特点"><a href="#MapReduce特点" class="headerlink" title="MapReduce特点"></a>MapReduce特点</h2><pre><code>- 高容错- 高扩展- 编程简单- 适合大数据离线批量处理</code></pre><h2 id="Map任务处理"><a href="#Map任务处理" class="headerlink" title="Map任务处理"></a>Map任务处理</h2><pre><code>* 读取输入文件内容，解析成key，value对。对输入文件的没一行，解析成key，value对。每一个键值对调用一次map函数* 写自己的逻辑，处理输入的key，value，转换成新的key，value输出* 对输出的key、value进行分区* 对不同分区的数据，按照key进行排序】分组。相同的key的value放到一个集合中。</code></pre><h2 id="reduce-任务处理"><a href="#reduce-任务处理" class="headerlink" title="reduce 任务处理"></a>reduce 任务处理</h2><pre><code>* 对多个map任务的输出，按照不用的分区，通过网络copy到不同的reduce节点* 对多个map任务的输出进行合并、排序。写reduce函数自己的逻辑，对哦输入的key、value处理，转换成新的key、value输出。* 把reduce的输出保存到文件中</code></pre><p>MapReduce键值对格式：<br><img src="http://7xt0cb.com2.z0.glb.clouddn.com/mapreduce键值对格式.png" alt="键值对格式 "><br>因为会有不同的结果，所以Reduce的 v2 会是数组的形式存储多个值。</p><h2 id="MR过程中各个角色的作用："><a href="#MR过程中各个角色的作用：" class="headerlink" title="MR过程中各个角色的作用："></a>MR过程中各个角色的作用：</h2><pre><code>* jobClient：提交作业* jobTracker：初始化作业，分配作业，TaskTracker与其进行通信，协调监控整个作业* TaskTracker：定期与JobTracker通信，执行Map任务和Reduce任务* HDFS：保存作业的数据、配置、jar包、结果等。</code></pre><h2 id="作业提交流程"><a href="#作业提交流程" class="headerlink" title="作业提交流程"></a>作业提交流程</h2><pre><code>* 提交作业准备    * 编写自己的MR程序    * 配置作业，保罗输入输出路径等等* 提交作业    *配置完成后，通过JobClient提交作业* 具体功能    * 与JobTracker通信得到一个jar的存储路径和JobId    * 输入输出路径检查、讲job jar拷贝到的HDFS    * 写job.xml、真正提交作业。</code></pre><h2 id="作业初始化"><a href="#作业初始化" class="headerlink" title="作业初始化"></a>作业初始化</h2><pre><code>* 客户端提交作业后，jobTracker会讲作业加入到队列，然后进行调度，默认的是FIFO方式* 具体功能    * 作业初始化主要是指 JobInProgress中完成的    * 读取分片信息    * 创建task包括Map和Reduce创建task包括Map和Reduce任务</code></pre><p>##　任务分配</p><pre><code>* TaskTracker 与JobTracker之间的通信和任务分配是通过心跳机制实现的* TaskTracker会主动定期向JobTracker发送报告 询问是否有任务要做， 如果有，就会申请到任务</code></pre><h2 id="任务执行"><a href="#任务执行" class="headerlink" title="任务执行"></a>任务执行</h2><pre><code>* 如果TaskTracker拿到任务，会将所有信息拷贝到本地，包括代码、配置、分片信息等* TarkTacker中的localizeJob()方法会被调用进行本地化，拷贝job.jar,jobconf.job.xml到本地* TaskTracker调用launchTaskForJob()方法加载启动任务</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;Hadoop中MapReduce的架构以及原理。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;MapReduce介绍&quot;&gt;&lt;a href=&quot;#MapReduce介绍&quot; class=&quot;headerlink&quot; title=&quot;MapReduce介绍&quot;&gt;&lt;/a&gt;MapReduce介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;MapReduce 编程模型&lt;ul&gt;
&lt;li&gt;Google提出的框架 主要用于搜索领域&lt;/li&gt;
&lt;li&gt;一种分布式计算模型框架解决海量数据的计算问题&lt;/li&gt;
&lt;li&gt;MapReduce将整个并行计算过程抽象到两个函数&lt;ul&gt;
&lt;li&gt;map（映射）：对一些独立元素组成的列表的每一个元素进行指定的操作，可以高度并行&lt;/li&gt;
&lt;li&gt;Reduce（化简）：队一个列表的元素进行合并&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;一个简单的MapReduce程序只需要指定map()、reduce()、input和output，剩下的事由框架来执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://gmle.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://gmle.github.io/tags/Hadoop/"/>
    
      <category term="MapReduce" scheme="http://gmle.github.io/tags/MapReduce/"/>
    
      <category term="架构" scheme="http://gmle.github.io/tags/%E6%9E%B6%E6%9E%84/"/>
    
      <category term="原理" scheme="http://gmle.github.io/tags/%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop_Yarn架构详解</title>
    <link href="http://gmle.github.io/2016/09/19/Yarn%E6%9E%B6%E6%9E%84%E8%AF%A6%E8%A7%A3/"/>
    <id>http://gmle.github.io/2016/09/19/Yarn架构详解/</id>
    <published>2016-09-19T00:53:20.000Z</published>
    <updated>2017-05-03T06:37:01.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>Yarn架构概述Yarn Shell操作Yarn 高可用的配置</code></pre><h2 id="Yarn-架构概述"><a href="#Yarn-架构概述" class="headerlink" title="Yarn 架构概述"></a>Yarn 架构概述</h2><pre><code>* 直接源于MRv1的缺陷(原MapReduce框架的不足)    * JobTracker是集群事务的集中处理点，存在单点故障(一个失败，全部受影响)    * JobTracker需要完成的任务太多，既要维护job的状态又要维护job的task的状态，造成过多的资源消耗(扩展性受限)</code></pre><a id="more"></a><pre><code>* 在taskTracker端，用map/reduce task作为资源的表示过于简单，没有考虑到cpu、内存等资源情况，当把两个需要消耗大内存的task调度到一起，很容易出现OOM* 把资源强制划分为map/reduce slot,当只有map task时，reduce slot不能用；当只有reduce task时，map slot不能用，容易造成资源利用不足。(多计算框架各自为战，数据共享困难).</code></pre><h3 id="Hadoop-Yarn基本架构"><a href="#Hadoop-Yarn基本架构" class="headerlink" title="Hadoop Yarn基本架构"></a>Hadoop Yarn基本架构</h3><pre><code>* Yarn 各模块组成    * ResourceManager        * 处理客户端请求        * 启动/监控ApplicationMaster        * 监控NodeManager        * 资源分配与调度    * NodeManager        * 单个节点上的资源管理        * 处理来自ResourceManager的名年龄        * 处理来自ApplicationMaster的命令    * ApplicationMaster        * 数据切分        * 为应用程序申请资源，并分配给内部任务        * 任务监控与容错* Yarn的容错    * ResourceManager        * 存在单点故障        * 2.X版本基于ZooKeeper实现HA    * NodeManager        * 失败后，RM将失败任务告诉对应的AM        * AM决定如何处理失败的任务    * ApplicationMaster        * 失败后，由RM负责重启，AM需处理内部任务的容错问题- Hadoop Yarn调度框架    * 双层电镀框架        * RM将资源分配给AM        * AM讲资源进一步分配给各个Task    * 基于资源预留的调度策略        * 资源不足时，会为Task预留，知道资源充足        * 与“all or nothing”策略不同（Apache Mesos）    * HadoopYarn资源调度器        * 多类型资源调度            * 采用DRF算法            * 目前只支持CPU和内存两种资源        * 提供多种资源调度器            * FIFO：先进先出            * Fair Scheduler：公平调度器            * Capacity Scheduler：容量调度器            * 多租户资源调度器                * 支持资源按比例分配                * 支持层级队列划分方式                * 支持资源抢占    * Hadoop Yarn 的资源隔离方案        * 支持内存和CPU两种资源隔离            * 内存是一种 “决定生死`” 的资源(集群内存不够，内存一处，任务崩溃)            * CPU是一种 “你更想快慢” 的资源        * 内存隔离            * 基于县城监控的方案            * 基于Cgroups的方案        * CPU隔离            * 默认不对CPu资源进行隔离            * 基于Cgroups的方案    * Hadoop Yarn资源调度语义        * 支持的语义            * 请求某个铁定节点/机架上的特定资源量            * 讲某些节点加入或移除黑名单，不再自己分配这些节点上的资源            * 请求归还某些资源        * 部支持的语义            * 请求任意节点/机架上的特定资源量            * 请求一组或几组符合某种特质的资源            * 超细粒度资源            * 动态调整Container资源* 运行在Yarn上的计算框架    * 离线计算框架：MapReduce（处理海量数据）    * DAG计算框架：Tez    * 流式计算框架：Storm：（处理流式数据）    * 内存计算框架：Spark（因为基于内存处理，所以特别快）    * 图计算框架：Giraph、GraphLib</code></pre><h3 id="运行在Yarn上的计算框架-MapReduce-Spark-AM"><a href="#运行在Yarn上的计算框架-MapReduce-Spark-AM" class="headerlink" title="运行在Yarn上的计算框架(MapReduce, Spark)AM"></a>运行在Yarn上的计算框架(MapReduce, Spark)AM</h3><pre><code>* 框架    * 离线计算框架：MapReduce（处理海量数据）    * DAG计算框架：Tez    * 流式计算框架：Storm：（处理流式数据）    * 内存计算框架：Spark（因为基于内存处理，所以特别快）    * 图计算框架：Giraph、GraphLib* Yarn应用程序类型    * 长应用程序        * Service、HTTP Server等    * 短应用程序        * MR job、Spark Job等。</code></pre><h3 id="Yarn-的发展前景"><a href="#Yarn-的发展前景" class="headerlink" title="Yarn 的发展前景"></a>Yarn 的发展前景</h3><pre><code>* 服务自动化部署（集群一键安装）* 调度框架的完善    * 支持更多的资源类型（网络、磁盘等）    * 支持更多的调度语义* 长作业的在线升级    * storm的在吸纳升级    * Container资源动态调整* 容错机制    * ResourceMapager自身容错    * NOdeManager宕机，任务不受影响    * ApplicationMaster个性化容错</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;Yarn架构概述
Yarn Shell操作
Yarn 高可用的配置
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Yarn-架构概述&quot;&gt;&lt;a href=&quot;#Yarn-架构概述&quot; class=&quot;headerlink&quot; title=&quot;Yarn 架构概述&quot;&gt;&lt;/a&gt;Yarn 架构概述&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;* 直接源于MRv1的缺陷(原MapReduce框架的不足)
    * JobTracker是集群事务的集中处理点，存在单点故障(一个失败，全部受影响)
    * JobTracker需要完成的任务太多，既要维护job的状态又要维护job的task的状态，造成过多的资源消耗(扩展性受限)
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://gmle.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://gmle.github.io/tags/Hadoop/"/>
    
      <category term="Yarn" scheme="http://gmle.github.io/tags/Yarn/"/>
    
  </entry>
  
  <entry>
    <title>HBase的Java API</title>
    <link href="http://gmle.github.io/2016/09/19/HBase%E7%9A%84JavaAPI%E8%AF%A6%E8%A7%A3/"/>
    <id>http://gmle.github.io/2016/09/19/HBase的JavaAPI详解/</id>
    <published>2016-09-19T00:53:20.000Z</published>
    <updated>2017-05-03T06:37:57.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>HBase的常用Java API</code></pre><a id="more"></a><h2 id="HBase常用操作："><a href="#HBase常用操作：" class="headerlink" title="HBase常用操作："></a>HBase常用操作：</h2><p>List；<br>Create；<br>Put；<br>Scan；<br>Get；<br>Delete；<br>Disable；<br>Drop；</p><h1 id="Java-API的HBase操作实现"><a href="#Java-API的HBase操作实现" class="headerlink" title="Java API的HBase操作实现"></a>Java API的HBase操作实现</h1><p>可以查看前面几篇文章了解下HBase的体系结构和HBase数据视图。</p><h2 id="连接HBase，配置必要的配置文件"><a href="#连接HBase，配置必要的配置文件" class="headerlink" title="连接HBase，配置必要的配置文件"></a>连接HBase，配置必要的配置文件</h2><p>HBaseConfiguration是每一个hbase client都会使用到的对象，它代表的是HBase配置信息。它有两种构造方式：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">HBaseConfiguration</span><span class="params">()</span></span></div><div class="line"><span class="keyword">public</span> <span class="title">HBaseConfiguration</span><span class="params">(<span class="keyword">final</span> Configuration c)</span></div></pre></td></tr></table></figure></p><p>默认的构造方式会尝试从hbase-default.xml和hbase-site.xml中读取配置。<br>也可以指定位置去读取<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> Configuration config = <span class="keyword">new</span> Configuration();</div><div class="line">  <span class="keyword">static</span> &#123;</div><div class="line">      config.addResource(<span class="string">"/conf/hbase-site.xml"</span>);</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p><p>如果classpath没有这两个文件，就需要你自己设置配置。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Configuration config = <span class="keyword">new</span> Configuration();</div><div class="line"></div><div class="line">config.set(“hbase.zookeeper.quorum”, “zkServer”);</div><div class="line">config.set(“hbase.zookeeper.property.clientPort”, “<span class="number">2181</span>″);\</div><div class="line"></div><div class="line">iguration config = <span class="keyword">new</span> HBaseConfiguration(config);</div></pre></td></tr></table></figure><h2 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h2><p>首先加入配置文件</p><p>创建表是通过HBaseAdmin对象来操作的。HBaseAdmin负责表的META信息处理。HBaseAdmin提供了createTable这个方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">(HTableDescriptor desc)</span></span></div></pre></td></tr></table></figure><p>HTableDescriptor 代表的是表的schema, 提供的方法中比较有用的有setMaxFileSize，指定最大的region size</p><p>setMemStoreFlushSize 指定memstore flush到HDFS上的文件大小</p><p>增加列簇，也就是增加family。<br>通过 addFamily方法<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addFamily</span><span class="params">(<span class="keyword">final</span> HColumnDescriptor family)</span></span></div></pre></td></tr></table></figure></p><p>HColumnDescriptor 代表的是column的schema，提供的方法比较常用的有</p><p>setTimeToLive:指定最大的TTL,单位是ms,过期数据会被自动删除。</p><p>setInMemory:指定是否放在内存中，对小表有用，可用于提高效率。默认关闭</p><p>setBloomFilter:指定是否使用BloomFilter,可提高随机查询效率。默认关闭</p><p>setCompressionType:设定数据压缩类型。默认无压缩。</p><p>setMaxVersions:指定数据最大保存的版本个数。默认为3。</p><h3 id="简单的例子–-gt-创建含有两个列簇的表"><a href="#简单的例子–-gt-创建含有两个列簇的表" class="headerlink" title="简单的例子–&gt; 创建含有两个列簇的表"></a>简单的例子–&gt; 创建含有两个列簇的表</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> cn.lesion.MyPractice;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HColumnDescriptor;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HTableDescriptor;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.HBaseAdmin;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created in Intellij IDEA .</div><div class="line"> * Author: 王乐.</div><div class="line"> * Date  : 16-5-4.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Create</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="comment">//定义静态常量</span></div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Configuration config = <span class="keyword">new</span> Configuration();</div><div class="line"></div><div class="line">    <span class="keyword">static</span> &#123;</div><div class="line">        config.addResource(<span class="string">"/conf/hbase-site.xml"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">(String tableName)</span></span>&#123;</div><div class="line"></div><div class="line">        System.out.println(<span class="string">"开始建表"</span>);</div><div class="line"></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(config);</div><div class="line"></div><div class="line">            <span class="comment">//如果创建的这个表存在的话，删除此表</span></div><div class="line">            <span class="keyword">if</span> (admin.tableExists(tableName)) &#123;</div><div class="line">                <span class="comment">//禁用此表</span></div><div class="line">                admin.disableTable(tableName);</div><div class="line">                <span class="comment">//删除这个表</span></div><div class="line">                System.out.println(<span class="string">"表已经存在，开始删除这个存在的表..."</span>);</div><div class="line">                admin.deleteTable(tableName);</div><div class="line">                System.out.println(<span class="string">"删除表 "</span>+tableName+<span class="string">" 成功！"</span>);</div><div class="line"></div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="comment">//对象：列族（Column Family)</span></div><div class="line">            HTableDescriptor tableDescriptor = <span class="keyword">new</span> HTableDescriptor(tableName);</div><div class="line">            tableDescriptor.addFamily(<span class="keyword">new</span> HColumnDescriptor(<span class="string">"info"</span>));</div><div class="line">            tableDescriptor.addFamily(<span class="keyword">new</span> HColumnDescriptor(<span class="string">"message"</span>));</div><div class="line"></div><div class="line">            <span class="comment">//创建含有两个列簇的表</span></div><div class="line">            admin.createTable(tableDescriptor);</div><div class="line"></div><div class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">            System.out.println(<span class="string">"Not Found config"</span>);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        createTable(<span class="string">"mytable"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h2><p>删除表也是通过HBaseAdmin来操作。<br>删除表之前首先要disable表。这是一个非常耗时的操作，所以不建议频繁删除表。<br>删除表之前要先禁用这个表：方法：disableTable<br>然后进行删除操作：方法:delete</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> cn.lesion.operateTable;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.MasterNotRunningException;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.ZooKeeperConnectionException;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.HBaseAdmin;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created in Intellij IDEA .</div><div class="line"> * Author : 王乐.</div><div class="line"> * Date : 2016.05.03.21.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Drop_Table</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Configuration config = <span class="keyword">new</span> Configuration();</div><div class="line"></div><div class="line">    <span class="comment">//Loading configuration files on startup！</span></div><div class="line">    <span class="keyword">static</span> &#123;</div><div class="line">        <span class="comment">//add Create_Table config</span></div><div class="line">        config.addResource(<span class="string">"/conf/hbase-site.xml"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        dropTable(<span class="string">"hbase_test1"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * According to table's name to Drop table</div><div class="line">     * <span class="doctag">@param</span> tableName</div><div class="line">     */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dropTable</span><span class="params">(String tableName)</span> </span>&#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(config);</div><div class="line"></div><div class="line">            System.out.println(<span class="string">"Deleting “ "</span> + tableName + <span class="string">" ”..."</span>);</div><div class="line"></div><div class="line">            admin.disableTable(tableName);</div><div class="line">            admin.deleteTable(tableName);</div><div class="line"></div><div class="line">            System.out.println(<span class="string">"Table“ "</span> + tableName + <span class="string">" ”delete success"</span>);</div><div class="line"></div><div class="line">        &#125; <span class="keyword">catch</span> (MasterNotRunningException e) &#123;</div><div class="line"></div><div class="line">            System.out.println(<span class="string">"HBase service no start！"</span>);</div><div class="line"></div><div class="line">        &#125; <span class="keyword">catch</span> (ZooKeeperConnectionException e) &#123;</div><div class="line"></div><div class="line">            System.out.println(<span class="string">"Not connected to zookeeper!"</span>);</div><div class="line"></div><div class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line"></div><div class="line">            System.out.println(<span class="string">"Not found config!"</span>);;</div><div class="line"></div><div class="line">        &#125;</div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h2><p>查询分为单条随机查询和批量查询。</p><p>单条查询是通过rowkey在table中查询某一行的数据。HTable提供了get方法来完成单条查询。</p><p>批量查询是通过制定一段rowkey的范围来查询。HTable提供了个getScanner方法来完成批量查询。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> Result <span class="title">get</span><span class="params">(<span class="keyword">final</span> Get get)</span></span></div><div class="line"><span class="keyword">public</span> ResultScanner <span class="title">getScanner</span><span class="params">(<span class="keyword">final</span> Scan scan)</span></div></pre></td></tr></table></figure><p>Get对象包含了一个Get查询需要的信息。它的构造方法有两种：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">Get</span><span class="params">(<span class="keyword">byte</span> [] row)</span></span></div><div class="line"><span class="keyword">public</span> <span class="title">Get</span><span class="params">(<span class="keyword">byte</span> [] row, RowLock rowLock)</span></div></pre></td></tr></table></figure><p>Rowlock是为了保证读写的原子性，你可以传递一个已经存在Rowlock，否则HBase会自动生成一个新的rowlock。</p><p>Scan对象提供了默认构造函数，一般使用默认构造函数。</p><p>setMaxVersions:指定最大的版本个数。如果不带任何参数调用setMaxVersions,表示取所有的版本。如果不掉用setMaxVersions,只会取到最新的版本。</p><p>setTimeRange:指定最大的时间戳和最小的时间戳，只有在此范围内的cell才能被获取。</p><p>setTimeStamp:指定时间戳。</p><p>setFilter:指定Filter来过滤掉不需要的信息</p><p>Scan特有的方法：</p><p>setStartRow:指定开始的行。如果不调用，则从表头开始。</p><p>setStopRow:指定结束的行（不含此行）。</p><p>setBatch:指定最多返回的Cell数目。用于防止一行中有过多的数据，导致OutofMemory错误。</p><p>ResultScanner是Result的一个容器，每次调用ResultScanner的next方法，会返回Result.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> Result <span class="title">next</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</div><div class="line"><span class="keyword">public</span> Result [] next(<span class="keyword">int</span> nbRows) <span class="keyword">throws</span> IOException;</div></pre></td></tr></table></figure></p><p>Result代表是一行的数据。常用方法有：</p><p>getRow:返回rowkey</p><p>raw:返回所有的key value数组。</p><p>getValue:按照column来获取cell的值</p><p>示例：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> cn.lesion.operateTable;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.KeyValue;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.*;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.filter.Filter;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.filter.FilterList;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.filter.SingleColumnValueFilter;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created in Intellij IDEA .</div><div class="line"> * Author : 王乐.</div><div class="line"> * Date : 2016.05.03.21.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Query_Data</span> </span>&#123;</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Configuration config = <span class="keyword">new</span> Configuration();</div><div class="line"></div><div class="line">    <span class="comment">//Loading configuration files on startup！</span></div><div class="line">    <span class="keyword">static</span> &#123;</div><div class="line">        <span class="comment">//add Create_Table config</span></div><div class="line">        config.addResource(<span class="string">"/conf/hbase-site.xml"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        <span class="comment">// QueryAll("hbase_test");</span></div><div class="line">        <span class="comment">// QueryByRowKey("hbase_test");</span></div><div class="line">        <span class="comment">// QueryByCondition("hbase_test");</span></div><div class="line">        QueryByMultiCondition(<span class="string">"hbase_test"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * Query all data</div><div class="line">     * <span class="doctag">@param</span> tableName</div><div class="line">     */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">QueryAll</span><span class="params">(String tableName)</span> </span>&#123;</div><div class="line">        HTablePool pool = <span class="keyword">new</span> HTablePool(config, <span class="number">1000</span>);</div><div class="line">        HTableInterface table = pool.getTable(tableName);</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            ResultScanner rs = table.getScanner(<span class="keyword">new</span> Scan());</div><div class="line">            <span class="keyword">for</span> (Result r : rs) &#123;</div><div class="line">                System.out.println(<span class="string">"get the rowkey:"</span> + <span class="keyword">new</span> String(r.getRow()));</div><div class="line">                <span class="keyword">for</span> (KeyValue keyValue : r.raw()) &#123;</div><div class="line">                    System.out.println(<span class="string">"列："</span> + <span class="keyword">new</span> String(keyValue.getFamily())</div><div class="line">                            + <span class="string">"====value:"</span> + <span class="keyword">new</span> String(keyValue.getValue()));</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * 单条件查询,根据rowkey查询唯一一条记录</div><div class="line">     * Single condition query,According to rowkey query data</div><div class="line">     * <span class="doctag">@param</span> tableName</div><div class="line">     */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">QueryByRowKey</span><span class="params">(String tableName)</span> </span>&#123;</div><div class="line"></div><div class="line">        HTablePool pool = <span class="keyword">new</span> HTablePool(config, <span class="number">1000</span>);</div><div class="line">        HTableInterface table = pool.getTable(tableName);</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            <span class="comment">// Add the rowkey  or Can be defined in the parameter</span></div><div class="line">            Get scan = <span class="keyword">new</span> Get(<span class="string">"112233bbbcccc"</span>.getBytes());<span class="comment">// 根据rowkey查询</span></div><div class="line">            Result r = table.get(scan);</div><div class="line">            System.out.println(<span class="string">"获得到rowkey:"</span> + <span class="keyword">new</span> String(r.getRow()));</div><div class="line">            <span class="keyword">for</span> (KeyValue keyValue : r.raw()) &#123;</div><div class="line">                System.out.println(<span class="string">"列："</span> + <span class="keyword">new</span> String(keyValue.getFamily())</div><div class="line">                        + <span class="string">"====值:"</span> + <span class="keyword">new</span> String(keyValue.getValue()));</div><div class="line">            &#125;</div><div class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * 单条件查询，查询多条记录</div><div class="line">     * Single condition query, query multiple records</div><div class="line">     * <span class="doctag">@param</span> tableName</div><div class="line">     */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">QueryByCondition</span><span class="params">(String tableName)</span> </span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            HTablePool pool = <span class="keyword">new</span> HTablePool(config, <span class="number">1000</span>);</div><div class="line">            HTableInterface table = pool.getTable(tableName);</div><div class="line"></div><div class="line">            <span class="comment">// 当列column1的值为aaa时进行查询</span></div><div class="line">            <span class="comment">//Query when the value of the column column1 is aaa</span></div><div class="line">            Filter filter = <span class="keyword">new</span> SingleColumnValueFilter(Bytes</div><div class="line">                    .toBytes(<span class="string">"column1"</span>), <span class="keyword">null</span>, CompareOp.EQUAL, Bytes</div><div class="line">                    .toBytes(<span class="string">"aaa"</span>));</div><div class="line"></div><div class="line"></div><div class="line">            Scan s = <span class="keyword">new</span> Scan();</div><div class="line">            s.setFilter(filter);</div><div class="line">            ResultScanner rs = table.getScanner(s);</div><div class="line">            <span class="keyword">for</span> (Result r : rs) &#123;</div><div class="line">                System.out.println(<span class="string">"Get rowkey:"</span> + <span class="keyword">new</span> String(r.getRow()));</div><div class="line">                <span class="keyword">for</span> (KeyValue keyValue : r.raw()) &#123;</div><div class="line">                    System.out.println(<span class="string">"column："</span> + <span class="keyword">new</span> String(keyValue.getFamily())</div><div class="line">                            + <span class="string">"====value:"</span> + <span class="keyword">new</span> String(keyValue.getValue()));</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * 组合条件查询</div><div class="line">     * Combination condition query</div><div class="line">     * <span class="doctag">@param</span> tableName</div><div class="line">     */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">QueryByMultiCondition</span><span class="params">(String tableName)</span> </span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            HTablePool pool = <span class="keyword">new</span> HTablePool(config, <span class="number">1000</span>);</div><div class="line">            HTableInterface table = pool.getTable(tableName);</div><div class="line"></div><div class="line">            List&lt;Filter&gt; filters = <span class="keyword">new</span> ArrayList&lt;Filter&gt;();</div><div class="line"></div><div class="line">            Filter filter1 = <span class="keyword">new</span> SingleColumnValueFilter(Bytes</div><div class="line">                    .toBytes(<span class="string">"column1"</span>), <span class="keyword">null</span>, CompareOp.EQUAL, Bytes</div><div class="line">                    .toBytes(<span class="string">"aaa"</span>));</div><div class="line">            filters.add(filter1);</div><div class="line"></div><div class="line">            Filter filter2 = <span class="keyword">new</span> SingleColumnValueFilter(Bytes</div><div class="line">                    .toBytes(<span class="string">"column2"</span>), <span class="keyword">null</span>, CompareOp.EQUAL, Bytes</div><div class="line">                    .toBytes(<span class="string">"bbb"</span>));</div><div class="line">            filters.add(filter2);</div><div class="line"></div><div class="line">            Filter filter3 = <span class="keyword">new</span> SingleColumnValueFilter(Bytes</div><div class="line">                    .toBytes(<span class="string">"column3"</span>), <span class="keyword">null</span>, CompareOp.EQUAL, Bytes</div><div class="line">                    .toBytes(<span class="string">"ccc"</span>));</div><div class="line">            filters.add(filter3);</div><div class="line"></div><div class="line">            FilterList filterList1 = <span class="keyword">new</span> FilterList(filters);</div><div class="line"></div><div class="line">            Scan scan = <span class="keyword">new</span> Scan();</div><div class="line">            scan.setFilter(filterList1);</div><div class="line">            ResultScanner rs = table.getScanner(scan);</div><div class="line">            <span class="keyword">for</span> (Result r : rs) &#123;</div><div class="line">                System.out.println(<span class="string">"Get rowkey:"</span> + <span class="keyword">new</span> String(r.getRow()));</div><div class="line">                <span class="keyword">for</span> (KeyValue keyValue : r.raw()) &#123;</div><div class="line">                    System.out.println(<span class="string">"column："</span> + <span class="keyword">new</span> String(keyValue.getFamily())</div><div class="line">                            + <span class="string">"====value:"</span> + <span class="keyword">new</span> String(keyValue.getValue()));</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">            rs.close();</div><div class="line"></div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h2 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h2><p>HTable通过put方法来插入数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(<span class="keyword">final</span> Put put)</span> <span class="keyword">throws</span> IOException</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(<span class="keyword">final</span> List puts)</span> <span class="keyword">throws</span> IOException</div></pre></td></tr></table></figure><p>可以传递单个批Put对象或者List put对象来分别实现单条插入和批量插入。</p><p>Put提供了3种构造方式：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">Put</span><span class="params">(<span class="keyword">byte</span> [] row)</span></span></div><div class="line"><span class="keyword">public</span> <span class="title">Put</span><span class="params">(<span class="keyword">byte</span> [] row, RowLock rowLock)</span></div><div class="line"><span class="keyword">public</span> <span class="title">Put</span><span class="params">(Put putToCopy)</span></div></pre></td></tr></table></figure><p>Put常用的方法有：</p><p>add:增加一个Cell</p><p>setTimeStamp:指定所有cell默认的timestamp,如果一个Cell没有指定timestamp,就会用到这个值。如果没有调用，HBase会将当前时间作为未指定timestamp的cell的timestamp.</p><p>setWriteToWAL: WAL是Write Ahead Log的缩写，指的是HBase在插入操作前是否写Log。默认是打开，关掉会提高性能，但是如果系统出现故障(负责插入的Region Server挂掉)，数据可能会丢失。</p><p>另外HTable也有两个方法也会影响插入的性能</p><p>setAutoFlash: AutoFlush指的是在每次调用HBase的Put操作，是否提交到HBase Server。默认是true,每次会提交。如果此时是单条插入，就会有更多的IO,从而降低性能.</p><p>setWriteBufferSize: Write Buffer Size在AutoFlush为false的时候起作用，默认是2MB,也就是当插入数据超过2MB,就会自动提交到Server</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> cn.lesion.operateTable;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.HTable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.HTableInterface;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.HTablePool;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created in Intellij IDEA .</div><div class="line"> * Author : 王乐.</div><div class="line"> * Date : 2016.05.03.19.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Insert_Data</span> </span>&#123;</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Configuration config = <span class="keyword">new</span> Configuration();</div><div class="line"></div><div class="line">    <span class="comment">//Loading configuration files on startup！</span></div><div class="line">    <span class="keyword">static</span> &#123;</div><div class="line"></div><div class="line">        <span class="comment">//add Create_Table config</span></div><div class="line">        config.addResource(<span class="string">"/conf/hbase-site.xml"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line"></div><div class="line">        <span class="comment">// Create a new table named is 'abc'</span></div><div class="line">        insertData(<span class="string">"mytest"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * According to tableName to insert data</div><div class="line">     * <span class="doctag">@param</span> tableName</div><div class="line">     */</div><div class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">insertData</span><span class="params">(String tableName)</span> </span>&#123;</div><div class="line">        System.out.println(<span class="string">"start insert data ......"</span>);</div><div class="line"></div><div class="line">        HTablePool pool = <span class="keyword">new</span> HTablePool(config, <span class="number">1000</span>);</div><div class="line"></div><div class="line">        <span class="comment">//Not Mandatory conversion to HTable  pool.getable return HTableInterface</span></div><div class="line">        HTableInterface table =  pool.getTable(tableName);</div><div class="line"></div><div class="line">        <span class="comment">// 一个PUT代表一行数据，再NEW一个PUT表示第二行数据,每行一个唯一的ROWKEY，此处rowkey为put构造方法中传入的值</span></div><div class="line">        <span class="comment">// one put == a row data, if new a 'put' it's a second row data</span></div><div class="line">        <span class="comment">// This rowkey is unique.</span></div><div class="line">        Put put = <span class="keyword">new</span> Put(<span class="string">"1"</span>.getBytes());</div><div class="line"></div><div class="line">        <span class="comment">// 本行数据的第一列</span></div><div class="line">        <span class="comment">// first data --&gt; first column</span></div><div class="line">        put.addColumn(<span class="string">"name"</span>.getBytes(), <span class="keyword">null</span>, <span class="string">"王乐"</span>.getBytes());</div><div class="line"></div><div class="line">        <span class="comment">// 本行数据的第三列</span></div><div class="line">        <span class="comment">// second column</span></div><div class="line">        put.addColumn(<span class="string">"age"</span>.getBytes(), <span class="keyword">null</span>, <span class="string">"19"</span>.getBytes());</div><div class="line"></div><div class="line">        <span class="comment">// 本行数据的第三列</span></div><div class="line">        <span class="comment">// third column</span></div><div class="line">        put.addColumn(<span class="string">"sex"</span>.getBytes(), <span class="keyword">null</span>, <span class="string">"男"</span>.getBytes());</div><div class="line"></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            table.put(put);</div><div class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line">        System.out.println(<span class="string">"end insert data ......"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="删除数据"><a href="#删除数据" class="headerlink" title="删除数据"></a>删除数据</h2><p>HTable 通过delete方法来删除数据。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">(<span class="keyword">final</span> Delete delete)</span></span></div></pre></td></tr></table></figure></p><p>Delete构造方法有：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">Delete</span><span class="params">(<span class="keyword">byte</span> [] row)</span></span></div><div class="line"><span class="keyword">public</span> <span class="title">Delete</span><span class="params">(<span class="keyword">byte</span> [] row, <span class="keyword">long</span> timestamp, RowLock rowLock)</span></div><div class="line"><span class="keyword">public</span> <span class="title">Delete</span><span class="params">(<span class="keyword">final</span> Delete d)</span></div></pre></td></tr></table></figure><p>Delete常用方法有</p><p>deleteFamily/deleteColumns:指定要删除的family或者column的数据。如果不调用任何这样的方法，将会删除整行。</p><p>注意：如果某个Cell的timestamp高于当前时间，这个Cell将不会被删除，仍然可以查出来。</p><p>示例：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> cn.lesion.operateTable;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Delete;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.HTable;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created in Intellij IDEA .</div><div class="line"> * Author : 王乐.</div><div class="line"> * Date : 2016.05.03.21.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Delete_Data</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Configuration config = <span class="keyword">new</span> Configuration();</div><div class="line"></div><div class="line">    <span class="comment">//Loading configuration files on startup！</span></div><div class="line">    <span class="keyword">static</span> &#123;</div><div class="line">        <span class="comment">//add Create_Table config</span></div><div class="line">        config.addResource(<span class="string">"/conf/hbase-site.xml"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        deleteRow(<span class="string">"hbase_test"</span>,<span class="string">"1"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * According to table's rowkey to delete data</div><div class="line">     * <span class="doctag">@param</span> tablename</div><div class="line">     * <span class="doctag">@param</span> rowkey</div><div class="line">     */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteRow</span><span class="params">(String tablename, String rowkey)</span>  </span>&#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            HTable table = <span class="keyword">new</span> HTable(config, tablename);</div><div class="line">            List&lt;Delete&gt; list = <span class="keyword">new</span> ArrayList&lt;Delete&gt;();</div><div class="line">            Delete d1 = <span class="keyword">new</span> Delete(rowkey.getBytes());</div><div class="line">            list.add(d1);</div><div class="line"></div><div class="line">            table.delete(list);</div><div class="line">            System.out.println(<span class="string">"Delete row success!"</span>);</div><div class="line"></div><div class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line"></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h2 id="切分表"><a href="#切分表" class="headerlink" title="切分表"></a>切分表</h2><p>HBaseAdmin提供split方法来将table 进行split.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">split</span><span class="params">(<span class="keyword">final</span> String tableNameOrRegionName)</span></span></div></pre></td></tr></table></figure><p>如果提供的tableName，那么会将table所有region进行split ;如果提供的region Name，那么只会split这个region.</p><p>由于split是一个异步操作，我们并不能确切的控制region的个数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">split</span><span class="params">(String tableName,<span class="keyword">int</span> number,<span class="keyword">int</span> timeout)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Configuration config = <span class="keyword">new</span> Configuration();</div><div class="line"></div><div class="line">    <span class="comment">//Loading configuration files on startup！</span></div><div class="line">    <span class="keyword">static</span> &#123;</div><div class="line">        <span class="comment">//add Create_Table config</span></div><div class="line">        config.addResource(<span class="string">"/conf/hbase-site.xml"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line">    HBaseAdmin hAdmin = <span class="keyword">new</span> HBaseAdmin(config);</div><div class="line"></div><div class="line">    HTable hTable = <span class="keyword">new</span> HTable(config,tableName);</div><div class="line"></div><div class="line">    <span class="keyword">int</span> oldsize = <span class="number">0</span>;</div><div class="line"></div><div class="line">    t =  System.currentTimeMillis();</div><div class="line"></div><div class="line">    <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</div><div class="line"></div><div class="line">       <span class="keyword">int</span> size = hTable.getRegionsInfo().size();</div><div class="line"></div><div class="line">       logger.info(“the region number=”+size);</div><div class="line"></div><div class="line">       <span class="keyword">if</span>(size&gt;=number ) <span class="keyword">break</span>;</div><div class="line"></div><div class="line">       <span class="keyword">if</span>(size!=oldsize)&#123;</div><div class="line"></div><div class="line">           hAdmin.split(hTable.getTableName());</div><div class="line"></div><div class="line">           oldsize = size;</div><div class="line"></div><div class="line">       &#125;       <span class="keyword">else</span> <span class="keyword">if</span>(System.currentTimeMillis()-t&gt;timeout)&#123;</div><div class="line"></div><div class="line">           <span class="keyword">break</span>;</div><div class="line"></div><div class="line">       &#125;</div><div class="line"></div><div class="line">       Thread.sleep(<span class="number">1000</span>*<span class="number">10</span>);</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;HBase的常用Java API
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="HBase" scheme="http://gmle.github.io/categories/HBase/"/>
    
    
      <category term="Hadoop" scheme="http://gmle.github.io/tags/Hadoop/"/>
    
      <category term="HBase" scheme="http://gmle.github.io/tags/HBase/"/>
    
      <category term="Java" scheme="http://gmle.github.io/tags/Java/"/>
    
      <category term="API" scheme="http://gmle.github.io/tags/API/"/>
    
  </entry>
  
  <entry>
    <title>hadoop常见面试题目（不定期更新）</title>
    <link href="http://gmle.github.io/2016/09/19/hadoop%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE/"/>
    <id>http://gmle.github.io/2016/09/19/hadoop常见面试题目/</id>
    <published>2016-09-19T00:53:20.000Z</published>
    <updated>2017-05-03T06:38:19.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>整理一些面试题，以便与日后查看（不定期更新）</code></pre><h2 id="2016-04-21-–-更新"><a href="#2016-04-21-–-更新" class="headerlink" title="2016.04.21 – 更新"></a>2016.04.21 – 更新</h2><p>下面哪个程序负责 HDFS 数据存储？<br>    a)NameNode<br>    b)Jobtracker<br>    <font color="#ff0000">c)Datanode</font><br>    d)secondaryNameNode<br>    e)tasktracker</p><a id="more"></a><p>HDfS 中的 block 默认保存几份？<br>    <font color="#ff0000">a)3 份</font><br>    b)2 份<br>    c)1 份<br>    d)不确定</p><p>下列哪个程序通常与 NameNode 在一个节点启动？<br>    a)SecondaryNameNode<br>    b)DataNode<br>    c)TaskTracker<br>    <font color="#ff0000">d)Jobtracker</font></p><pre><code>&gt;此题分析：    hadoop的集群是基于master/slave模式，namenode和jobtracker属于master，datanode和tasktracker属于slave，master只有一个，而slave有多个SecondaryNameNode内存需求和NameNode在一个数量级上，所以通常secondary NameNode（运行在单独的物理机器上）和NameNode运行在不同的机器上。    JobTracker和TaskTracker；    JobTracker 对应于 NameNode；    TaskTracker 对应于 DataNode；    DataNode 和NameNode 是针对数据存放来而言的；    JobTracker和TaskTracker是对于MapReduce执行而言的。    mapreduce中几个主要概念，mapreduce整体上可以分为这么几条执行线索：obclient，JobTracker与TaskTracker。    1、JobClient会在用户端通过JobClient类将应用已经配置参数打包成jar文件存储到hdfs，并把路径提交到Jobtracker,然后由JobTracker创建每一个Task（即MapTask和ReduceTask）并将它们分发到各个TaskTracker服务中去执行。    2、JobTracker是一个master服务，软件启动之后JobTracker接收Job，负责调度Job的每一个子任务task运行于TaskTracker上，并监控它们，如果发现有失败的task就重新运行它。一般情况应该把JobTracker部署在单独的机器上。    3、TaskTracker是运行在多个节点上的slaver服务。TaskTracker主动与JobTracker通信，接收作业，并负责直接执行每一个任务。TaskTracker都需要运行在HDFS的DataNode上。</code></pre><p>Hadoop 作者 答案C Doug cutting<br>    a)Martin Fowler<br>    b)Kent Beck<br>    <font color="#ff0000">c)Doug cutting</font></p><p>下列哪项通常是集群的最主要瓶颈:<br>    a)CPU<br>    b)网络<br>    <font color="#ff0000">c)磁盘IO</font><br>    d)内存</p><pre><code>&gt;该题解析：    首先集群的目的是为了节省成本，用廉价的pc机，取代小型机及大型机。小型机和大型机有什么特点？    1.cpu处理能力强    2.内存够大    所以集群的瓶颈不可能是a和d    3.网络是一种稀缺资源，但是并不是瓶颈。    4.由于大数据面临海量数据，读写数据都需要io，然后还要冗余数据，hadoop一般备3份数据，所以IO就会打折扣。</code></pre><p>关于 SecondaryNameNode 哪项是正确的？<br>    a)它是 NameNode 的热备<br>    b)它对内存没有要求<br>    <font color="#ff0000">c)它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间</font><br>    d)SecondaryNameNode 应与 NameNode 部署到一个节点。</p><p>下列哪项可以作为集群的管理？<br>    <font color="#ff0000">a)Puppet</font><br>    <font color="#ff0000">b)Pdsh</font><br>    c)Cloudera Manager<br>    <font color="#ff0000">d)Zookeeper</font></p><p>Cloudera 提供哪几种安装 CDH 的方法？<br>    <font color="#ff0000"><br>    a)Cloudera manager<br>    b)Tarball<br>    c)Yum<br>    d)Rpm<br>    </font></p><p>Ganglia 不仅可以进行监控，也可以进行告警。（ 正确）</p><pre><code>分析：此题的目的是考Ganglia的了解。严格意义上来讲是正确。ganglia作为一款最常用的Linux环境中的监控软件，它擅长的的是从节点中按照用户的需求以较低的代价采集数据。但是ganglia在预警以及发生事件后通知用户上并不擅长。最新的ganglia已经有了部分这方面的功能。但是更擅长做警告的还有Nagios。Nagios，就是一款精于预警、通知的软件。通过将Ganglia和Nagios组合起来，把Ganglia采集的数据作为Nagios的数据源，然后利用Nagios来发送预警通知，可以完美的实现一整套监控管理的系统。</code></pre><p>Block Size 是不可以修改的。（错误 ）</p><pre><code>分析：它是可以被修改的Hadoop的基础配置文件是hadoop-default.xml，默认建立一个Job的时候会建立Job的Config，Config首先读入hadoop-default.xml的配置，然后再读入hadoop-site.xml的配置（这个文件初始的时候配置为空），hadoop-site.xml中主要配置需要覆盖的hadoop-default.xml的系统级配置。</code></pre><p>Nagios 不可以监控 Hadoop 集群，因为它不提供 Hadoop 支持。（错误 ）</p><pre><code>分析：Nagios是集群监控工具，而且是云计算三大利器之一</code></pre><p>如果 NameNode 意外终止，SecondaryNameNode 会接替它使集群继续工作。（错误 ）</p><pre><code>分析：SecondaryNameNode是帮助恢复，而不是替代，如何恢复，可以查看</code></pre><p>Cloudera CDH 是需要付费使用的。（错误 ）</p><pre><code>分析：第一套付费产品是Cloudera Enterpris，Cloudera Enterprise在美国加州举行的 Hadoop 大会 (Hadoop Summit) 上公开，以若干私有管理、监控、运作工具加强 Hadoop 的功能。收费采取合约订购方式，价格随用的 Hadoop 叢集大小变动。</code></pre><p>Hadoop 是 Java 开发的，所以 MapReduce 只支持 Java 语言编写。（错误 ）</p><pre><code>分析：rhadoop是用R语言开发的，MapReduce是一个框架，可以理解是一种思想，可以使用其他语言开发。</code></pre><p>Hadoop 支持数据的随机读写。（错 ）</p><pre><code>分析：lucene是支持随机读写的，而hdfs只支持随机读。但是HBase可以来补救。HBase提供随机读写，来解决Hadoop不能处理的问题。HBase自底层设计开始即聚焦于各种可伸缩性问题：表可以很“高”，有数十亿个数据行；也可以很“宽”，有数百万个列；水平分区并在上千个普通商用机节点上自动复制。表的模式是物理存储的直接反映，使系统有可能提高高效的数据结构的序列化、存储和检索。</code></pre><p>NameNode 负责管理 metadata，client 端每次读写请求，它都会从磁盘中读取或则会写入 metadata 信息并反馈 client 端。（错误）</p><pre><code>此题分析：NameNode 不需要从磁盘读取 metadata，所有数据都在内存中，硬盘上的只是序列化的结果，只有每次 namenode 启动的时候才会读取。1）文件写入    Client向NameNode发起文件写入的请求。    NameNode根据文件大小和文件块配置情况，返回给Client它所管理部分DataNode的信息。    Client将文件划分为多个Block，根据DataNode的地址信息，按顺序写入到每一个DataNode块中。2）文件读取    Client向NameNode发起文件读取的请求。</code></pre><p>NameNode 本地磁盘保存了 Block 的位置信息。（ 个人认为正确，欢迎提出其它意见）</p><pre><code>分析：DataNode是文件存储的基本单元，它将Block存储在本地文件系统中，保存了Block的Meta-data，同时周期性地将所有存在的Block信息发送给NameNode。NameNode返回文件存储的DataNode的信息。Client读取文件信息。</code></pre><p>DataNode 通过长连接与 NameNode 保持通信。（ ）</p><pre><code>这个有分歧：具体正在找这方面的有利资料。下面提供资料可参考。首先明确一下概念：（1）.长连接    Client方与Server方先建立通讯连接，连接建立后不断开，然后再进行报文发送和接收。这种方式下由于通讯连接一直存在，此种方式常用于点对点通讯。（2）.短连接    Client方与Server每进行一次报文收发交易时才进行通讯连接，交易完毕后立即断开连接。此种方式常用于一点对多点通讯，比如多个Client连接一个Server.</code></pre><p>Hadoop 自身具有严格的权限管理和安全措施保障集群正常运行。（错误 ）</p><pre><code>hadoop只能阻止好人犯错，但是不能阻止坏人干坏事</code></pre><p>Slave 节点要存储数据，所以它的磁盘越大越好。（ 错误）</p><pre><code>分析：一旦Slave节点宕机，数据恢复是一个难题</code></pre><p>hadoop dfsadmin –report 命令用于检测 HDFS 损坏块。（错误 ）</p><p>Hadoop 默认调度器策略为 FIFO（正确 ）</p><p>集群内每个节点都应该配 RAID，这样避免单磁盘损坏，影响整个节点运行。（错误 ）</p><pre><code>分析：首先明白什么是RAID，可以参考百科磁盘阵列。这句话错误的地方在于太绝对，具体情况具体分析。题目不是重点，知识才是最重要的。因为hadoop本身就具有冗余能力，所以如果不是很严格不需要都配备RAID。具体参考第二题。</code></pre><p>因为 HDFS 有多个副本，所以 NameNode 是不存在单点问题的。（错误 ）</p><p>每个 map 槽就是一个线程。（错误 ）</p><pre><code>分析：首先我们知道什么是map 槽,map 槽-&gt;map slotmap slot 只是一个逻辑值 ( org.apache.hadoop.mapred.TaskTracker.TaskLauncher.numFreeSlots )，而不是对应着一个线程或者进程</code></pre><p>每个 map 槽就是一个线程。（错误 ）</p><pre><code>分析：首先我们知道什么是map 槽,map 槽-&gt;map slotmap slot 只是一个逻辑值 ( org.apache.hadoop.mapred.TaskTracker.TaskLauncher.numFreeSlots )，而不是对应着一个线程或者进程</code></pre><p>Mapreduce 的 input split 就是一个 block。（错误 ）</p><p> Hadoop 环境变量中的 HADOOP_HEAPSIZE 用于设置所有 Hadoop 守护线程的内存。它默认是 200 GB。（ 错误）</p><pre><code>hadoop为各个守护进程（namenode,secondarynamenode,jobtracker,datanode,tasktracker）统一分配的内存在hadoop-env.sh中设置，参数为HADOOP_HEAPSIZE，默认为1000M。</code></pre><p>NameNode 的 Web UI 端口是 50030，它通过 jetty 启动的 Web 服务。（错误 ）</p><p>DataNode 首次加入 cluster 的时候，如果 log 中报告不兼容文件版本，那需要 NameNode执行“Hadoop namenode -format”操作格式化磁盘。（错误 ）</p><pre><code>分析：首先明白介绍，什么ClusterIDClusterID添加了一个新的标识符ClusterID用于标识集群中所有的节点。当格式化一个Namenode，需要提供这个标识符或者自动生成。这个ID可以被用来格式化加入集群的其他Namenode。二次整理有的同学问题的重点不是上面分析内容：内容如下：这个报错是说明 DataNode 所装的Hadoop版本和其它节点不一致，应该检查DataNode的Hadoop版本</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;整理一些面试题，以便与日后查看（不定期更新）
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;2016-04-21-–-更新&quot;&gt;&lt;a href=&quot;#2016-04-21-–-更新&quot; class=&quot;headerlink&quot; title=&quot;2016.04.21 – 更新&quot;&gt;&lt;/a&gt;2016.04.21 – 更新&lt;/h2&gt;&lt;p&gt;下面哪个程序负责 HDFS 数据存储？&lt;br&gt;    a)NameNode&lt;br&gt;    b)Jobtracker&lt;br&gt;    &lt;font color=&quot;#ff0000&quot;&gt;c)Datanode&lt;/font&gt;&lt;br&gt;    d)secondaryNameNode&lt;br&gt;    e)tasktracker&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://gmle.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://gmle.github.io/tags/Hadoop/"/>
    
      <category term="面试" scheme="http://gmle.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>正则表达式</title>
    <link href="http://gmle.github.io/2016/09/19/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <id>http://gmle.github.io/2016/09/19/正则表达式/</id>
    <published>2016-09-19T00:53:20.000Z</published>
    <updated>2017-05-03T06:38:36.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>正则表达式的基础使用。正则表达式 --&gt; 专门勇于操作字符串；可以理解为 ‘正确的规则’。好处：用了一些符号来代表一些代码，书写起来更为简单。弊端：可读性差，而且要把符号学完但是底层也是要配合代码实现的。</code></pre><a id="more"></a><p>正则表达式再每个语言中都大同小异。<br>具体语法查看相应的API</p><h2 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h2><p>学习正则表达式的最好方法是从例子开始，理解例子之后再自己对例子进行修改，实验。</p><h2 id="符号学习"><a href="#符号学习" class="headerlink" title="符号学习"></a>符号学习</h2><p>1、了解一下常见的正则符号：<br>    1.1 [] ：【判断字符位上的内容】<br>    1.2 与定义字符：都带着反斜线<br>        .： 任意字符<br>        \\d : 数字 [0-9]<br>        \\D : 非数字 [^0-9]<br>        \\w : 单词字符[a-zA-Z_0-9]<br>    1.3 边界字符<br>        ^ : 行开头<br>        $ : 行结尾<br>        \b : 单词边界<br>    1.4 数量词：必须结合内容<br>        X? : x内容出现零次或一次<br>        X* : x内容出现零次或多次<br>        X+ : x内容出现一次或多次<br>        X(n) : x内容出现n此<br>        X(n,) : x内容出现至少n此<br>        X(n,m) : x内容出现n到m此</p><h2 id="常见功能操作"><a href="#常见功能操作" class="headerlink" title="常见功能操作"></a>常见功能操作</h2><p>字符串String类</p><h3 id="匹配-结果为-true-false"><a href="#匹配-结果为-true-false" class="headerlink" title="匹配  结果为 true | false"></a>匹配  结果为 true | false</h3><p>就是String类中的machers方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> cn.lesion.Regular;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.util.regex.Pattern;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created in Intellij IDEA .</div><div class="line"> * Author: 王乐.</div><div class="line"> * Date  : 16-5-17.</div><div class="line"> *</div><div class="line"> * 匹配手机号</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">phoneBoolen</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line"></div><div class="line">        <span class="comment">//手机号</span></div><div class="line">        String phone = <span class="string">"17093548877"</span>;</div><div class="line"></div><div class="line">        <span class="comment">//定义规则</span></div><div class="line">        String phone_Regex = <span class="string">"^((13[0-9])|(15[^4,\\D])|(18[0,5-9]))\\d&#123;8&#125;$"</span>;</div><div class="line"></div><div class="line">        System.out.println(phone+<span class="string">"::"</span>+phone.matches(phone_Regex));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="切割-结果为-String-字符串数组"><a href="#切割-结果为-String-字符串数组" class="headerlink" title="切割  结果为 String[] 字符串数组"></a>切割  结果为 String[] 字符串数组</h3><p>利用String中的方法 Splite<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> cn.lesion.Regular;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created in Intellij IDEA .</div><div class="line"> * Author: 王乐.</div><div class="line"> * Date  : 16-5-17.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">splitStr</span> </span>&#123;</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line"></div><div class="line"><span class="comment">//      String str = "zhagnsan    lisi  wangwu";</span></div><div class="line"><span class="comment">//      String regex = " +";</span></div><div class="line"></div><div class="line"><span class="comment">//      String str = "zhangsan,lisi,wangwu";</span></div><div class="line"><span class="comment">//      String regex = ","</span></div><div class="line"></div><div class="line"><span class="comment">//      String str = "zhangsan.lisi.wangwu";</span></div><div class="line"><span class="comment">//      String regex = "\\.";</span></div><div class="line"></div><div class="line">        <span class="comment">//正则规则的复用：想复用 先封装 正则封装用小括号完成</span></div><div class="line">        <span class="comment">//封装完成之后有编号，从1开始。规则中被小括号封装的称之为组，直接通过编号就可以调用对应的组</span></div><div class="line">        <span class="comment">//调用方式直接写已有的组的编号前面加上\\ 如 ()\\1，在使用已有的第一组内容。原则：必须要先有组</span></div><div class="line">        String str = <span class="string">"zhangsan@@@@lisi###wangwu"</span>;</div><div class="line">        String regex = <span class="string">"(.)\\1+"</span>;</div><div class="line"></div><div class="line"></div><div class="line">        String[] strs = str.split(regex);</div><div class="line"></div><div class="line">        <span class="keyword">for</span> (String s:</div><div class="line">             strs) &#123;</div><div class="line">            System.out.println(<span class="string">"--"</span>+s+<span class="string">"--"</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h3 id="替换-结果是一个新的字符串"><a href="#替换-结果是一个新的字符串" class="headerlink" title="替换 结果是一个新的字符串"></a>替换 结果是一个新的字符串</h3><p>利用String中的方法 replaceAll<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> cn.lesion.Regular;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created in Intellij IDEA .</div><div class="line"> * Author: 王乐.</div><div class="line"> * Date  : 16-5-17.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">replace</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line"><span class="comment">/*</span></div><div class="line">        String str = "ajkls@@@@@fbxcv@@@@@@opi";</div><div class="line"></div><div class="line">        //在参数列表中，其他参数钥匙用之前参数中规则的组，需要使用$组编号</div><div class="line">        str = str.replaceAll("(.)\\1+", "$1");</div><div class="line">*/</div><div class="line"><span class="comment">/*</span></div><div class="line"></div><div class="line">        String str = "13623372344"; //136****2344</div><div class="line">        str = str.replaceAll("(\\d&#123;3&#125;)\\d&#123;4&#125;(\\d&#123;4&#125;)", "$1****$2");</div><div class="line"></div><div class="line">*/</div><div class="line"></div><div class="line">        String str = <span class="string">"lzxiujcbvil1111111111111111111adgfadkgji23984567289345079"</span>;</div><div class="line">        <span class="comment">//替换N个以上的数字</span></div><div class="line">        str = str.replaceAll(<span class="string">"\\d&#123;4,&#125;"</span>, <span class="string">"***"</span>);</div><div class="line"></div><div class="line"></div><div class="line">        System.out.println(str);</div><div class="line"></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h3 id="获取-结果为-String-字符串数组"><a href="#获取-结果为-String-字符串数组" class="headerlink" title="获取 结果为 String[] 字符串数组"></a>获取 结果为 String[] 字符串数组</h3><p>其它三个功能内部最终是用的都是Pattern正则表达式对象。<br>现在需要其他功能时，字符串String类中没有对象的方法。<br>Pattern对象的使用原理</p><ol><li>将正则表达式字符串编译成正则对象 pattern</li><li>通过pattern对象获取macher对象（匹配器对象）</li><li>通过匹配器对象对字符串进行规则的匹配，结果都在匹配器中</li><li>通过匹配器对象的功能获取结果</li></ol><p>范例：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//编译为正则对象</span></div><div class="line">Pattern p = Pattern.compile(<span class="string">"a*b"</span>);</div><div class="line"><span class="comment">//返回一个匹配器对象</span></div><div class="line">Matcher m = p.matcher(<span class="string">"aaaaaab"</span>);</div><div class="line"><span class="comment">//判断</span></div><div class="line"><span class="keyword">boolean</span> b = m.matches();</div></pre></td></tr></table></figure></p><p>代码：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> cn.lesion.Regular;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.util.regex.Matcher;</div><div class="line"><span class="keyword">import</span> java.util.regex.Pattern;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created in Intellij IDEA .</div><div class="line"> * Author: 王乐.</div><div class="line"> * Date  : 16-5-17.</div><div class="line"> *</div><div class="line"> * 取出</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">takeOutWords</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line"></div><div class="line">        String str = <span class="string">"da jia zhu yi le, ming tian fang jia le!"</span>;</div><div class="line"></div><div class="line">        <span class="comment">//取出由三个字母组成的单词</span></div><div class="line">        String regex = <span class="string">"\\b[a-zA-Z]&#123;3&#125;\\b"</span>;</div><div class="line"></div><div class="line"></div><div class="line">        <span class="comment">//将正则表达式转换为对象</span></div><div class="line">        Pattern p = Pattern.compile(regex);</div><div class="line"></div><div class="line">        <span class="comment">//和要操作的字符串关联，获取对应的匹配器对象</span></div><div class="line">        Matcher m = p.matcher(str);</div><div class="line"></div><div class="line"></div><div class="line">        <span class="keyword">while</span> (m.find()) &#123;</div><div class="line">            <span class="comment">//第一种取值方式</span></div><div class="line">            System.out.println(m.group());</div><div class="line">            <span class="comment">//第二种取值方式 System.out.println(str.substring(m.start(),m.end()));</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;正则表达式的基础使用。
正则表达式 --&amp;gt; 专门勇于操作字符串；
可以理解为 ‘正确的规则’。
好处：用了一些符号来代表一些代码，书写起来更为简单。
弊端：可读性差，而且要把符号学完
但是底层也是要配合代码实现的。
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="正则表达式" scheme="http://gmle.github.io/categories/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
    
      <category term="Java" scheme="http://gmle.github.io/tags/Java/"/>
    
      <category term="正则表达式" scheme="http://gmle.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
      <category term="JavaSE" scheme="http://gmle.github.io/tags/JavaSE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop_玩转 HDFS之 ACL</title>
    <link href="http://gmle.github.io/2016/09/19/%E7%8E%A9%E8%BD%ACHDFS-ACL/"/>
    <id>http://gmle.github.io/2016/09/19/玩转HDFS-ACL/</id>
    <published>2016-09-19T00:53:20.000Z</published>
    <updated>2017-05-03T06:38:39.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>Hadoop从2.4.0版本开始支持hdfs的ACL，通俗的讲就是文件访问控制权限下面对其进行一些测试：</code></pre><table><thead><tr><th>unnamed user (file owner)</th><th>文件的拥有者</th></tr></thead><tbody><tr><td>unnamed group (file group)</td><td>文件的所属组</td></tr><tr><td>named user</td><td>除了文件的拥有者和拥有组之外，的其它用户</td></tr><tr><td>named group</td><td>除了文件的拥有者和拥有组之外，的其它用户</td></tr><tr><td>mask</td><td>权限掩码，用于过滤named user和named group的权限</td></tr></tbody></table><a id="more"></a><h2 id="一、启用ACL"><a href="#一、启用ACL" class="headerlink" title="一、启用ACL"></a>一、启用ACL</h2><p>启用ACL功能</p><p>修改hdfs-site.xml 增加如下属性 开启ACL<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.acls.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure></p><p>修改core-site.xml 设置用户组默认权限.<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.permissions.umask-mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>002<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure></p><p>一个访问控制列表（ACL）是一组ACL词目(entries)的集合，每个ACL词目会指定一个用户/组，并赋予读/写/执行上等权限。例如：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">user::rw-</div><div class="line">   user:bruce:rwx                  #effective:r--</div><div class="line">   group::r-x                      #effective:r--</div><div class="line">   group:sales:rwx                 #effective:r--</div><div class="line">   mask::r--</div><div class="line">  other::r--</div></pre></td></tr></table></figure></p><p>这里面，没有命名的用户/组即该文件的基本所属用户/组。每一个ACL都有一个掩码(mask)，如果用户不提供掩码，那么该掩码会自动根据所有ACL条目的并集来获得(属主除外）。在该文件上运行chmod会改变掩码的权限。由于掩码用于过滤，这有效地限制了权限的扩展ACL条目，而不是仅仅改变组条目，并可能丢失的其他扩展ACL条目。</p><p>定义默认 （default）ACL条目，新的子文件和目录会自动继承默认的ACL条目设置，而只有目录会有默认的ACL条目。例如：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">user::rwx</div><div class="line">  group::r-x</div><div class="line">  other::r-x</div><div class="line">  default:user::rwx</div><div class="line">  default:user:bruce:rwx #effective:r-x</div><div class="line">  default:group::r-x</div><div class="line">  default:group:sales:rwx#effective:r-x</div><div class="line">  default:mask::r-x</div><div class="line">  default:other::r-x</div></pre></td></tr></table></figure><h3 id="ACL相关的文件API："><a href="#ACL相关的文件API：" class="headerlink" title="ACL相关的文件API："></a>ACL相关的文件API：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">modifyAclEntries</span><span class="params">(Path path, List aclSpec)</span> <span class="keyword">throws</span> IOException</span>;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">removeAclEntries</span><span class="params">(Path path, List aclSpec)</span> <span class="keyword">throws</span> IOException</span>;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">removeDefaultAcl</span><span class="params">(Path path)</span> <span class="keyword">throws</span> IOException</span>;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">removeAcl</span><span class="params">(Path path)</span> <span class="keyword">throws</span> IOException</span>;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAcl</span><span class="params">(Path path, List aclSpec)</span> <span class="keyword">throws</span> IOException</span>;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> AclStatus <span class="title">getAclStatus</span><span class="params">(Path path)</span> <span class="keyword">throws</span> IOException</span>;</div></pre></td></tr></table></figure><h3 id="命令行命令："><a href="#命令行命令：" class="headerlink" title="命令行命令："></a>命令行命令：</h3><p>显示文件和目录的访问控制列表。如果一个目录有默认的ACL，getfacl也可以显示默认的ACL设置。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hdfs dfs -getfacl [-R] path</div></pre></td></tr></table></figure></p><p>设置文件和目录的ACL<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hdfs dfs -setfacl [-R] [-b|-k -m|-x acl_spec path]|[--set acl_spec path]</div></pre></td></tr></table></figure></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">-R: Use this option to recursively list ACLs for all files and directories.</div><div class="line">-b: Revoke all permissions except the base ACLs for user, groups and others.</div><div class="line">-k: Remove the default ACL.</div><div class="line">-m: Add new permissions to the ACL with this option. Does not affect existing permissions.</div><div class="line">-x: Remove only the ACL specified.</div></pre></td></tr></table></figure><p>当ls的权限位输出以+结束时，那么该文件或目录正在启用一个ACL。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hdfs dfs -ls args</div></pre></td></tr></table></figure></p><h2 id="实际使用："><a href="#实际使用：" class="headerlink" title="实际使用："></a>实际使用：</h2><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line"># 默认只有基本的权限控制</div><div class="line">hdfs dfs -getfacl /data</div><div class="line"># file: /data</div><div class="line"># owner: hive</div><div class="line"># group: hadoop</div><div class="line">user::rwx</div><div class="line">group::r-x</div><div class="line">other::r-x</div><div class="line">#递归显示/data下所有文件的ACL</div><div class="line">hdfs dfs -getfacl -R /data</div><div class="line"># file: /data</div><div class="line"># owner: hive</div><div class="line"># group: hadoop</div><div class="line">user::rwx</div><div class="line">group::r-x</div><div class="line">other::r-x</div><div class="line"></div><div class="line"># file: /data/test.zero</div><div class="line"># owner: hive</div><div class="line"># group: hadoop</div><div class="line">user::rw-</div><div class="line">group::r--</div><div class="line">other::r--</div><div class="line"></div><div class="line"># file: /data/test.zero.2</div><div class="line"># owner: hive</div><div class="line"># group: hadoop</div><div class="line">user::rw-</div><div class="line">group::r--</div><div class="line">other::r--</div><div class="line">#添加一个用户ACL条目</div><div class="line">hdfs dfs -setfacl -m user:hbase:rw- /data/test.zero</div><div class="line">#添加一个组ACL条目和一个用户ACL条目（如果设置一个未命名条目，可以用user::r-x，group::r-w或者other::r-x等来设置）</div><div class="line">hdfs dfs -setfacl -m group:crm:--x,user:app1:rwx /data/test.zero.2</div><div class="line">#移除一个ACL条目</div><div class="line">hdfs dfs -setfacl -x user:app1 /data/test.zero.2</div><div class="line">#“+”已开启了ACL功能</div><div class="line">hdfs dfs -ls -R /data</div><div class="line">-rw-rwxr--+  3 hive hadoop 1073741824 2014-12-21 15:32 /data/test.zero</div><div class="line">-rw-r-xr--+  3 hive hadoop 1073741824 2014-12-21 15:50 /data/test.zero.2</div><div class="line"># 查看当前ACL，此时mask已经被生成</div><div class="line">hdfs dfs -getfacl -R /data/test.zero.2</div><div class="line"># file: /data/test.zero.2</div><div class="line"># owner: hive</div><div class="line"># group: hadoop</div><div class="line">user::rw-</div><div class="line">group::r--</div><div class="line">group:crm:--x</div><div class="line">mask::r-x</div><div class="line">other::r--</div><div class="line">hdfs dfs -getfacl /data/test.zero.2</div><div class="line"># 为data目录添加default权限</div><div class="line">hdfs dfs -setfacl -m default:user:debugo:rwx /data</div><div class="line">hdfs dfs -mkdir /data/d1</div><div class="line">hdfs dfs -getfacl /data/d1</div><div class="line">user::rwx</div><div class="line">user:debugo:rwx#effective:r-x</div><div class="line">group::r-x</div><div class="line">mask::r-x</div><div class="line">other::r-x</div><div class="line">default:user::rwx</div><div class="line">default:user:debugo:rwx</div><div class="line">default:group::r-x</div><div class="line">default:mask::rwx</div><div class="line">default:other::r-x</div><div class="line">#可以看出，default虽然继承给了d1，但是被mask=r-x所过滤，所以这里还需要设置mask。此时debugo用户的权限可以被正常访问。</div><div class="line">hdfs dfs -setfacl -m mask::rwx /data/d1</div><div class="line">hdfs dfs -getfacl /data/d1</div><div class="line"># file: /data/d1</div><div class="line"># owner: hdfs</div><div class="line"># group: hadoop</div><div class="line">user::rwx</div><div class="line">user:debugo:rwx</div><div class="line">group::r-x</div><div class="line">mask::rwx</div><div class="line">other::r-x</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;Hadoop从2.4.0版本开始支持hdfs的ACL，
通俗的讲就是文件访问控制权限
下面对其进行一些测试：
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;unnamed user (file owner)&lt;/th&gt;
&lt;th&gt;文件的拥有者&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;unnamed group (file group)&lt;/td&gt;
&lt;td&gt;文件的所属组&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;named user&lt;/td&gt;
&lt;td&gt;除了文件的拥有者和拥有组之外，的其它用户&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;named group&lt;/td&gt;
&lt;td&gt;除了文件的拥有者和拥有组之外，的其它用户&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mask&lt;/td&gt;
&lt;td&gt;权限掩码，用于过滤named user和named group的权限&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://gmle.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://gmle.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="http://gmle.github.io/tags/HDFS/"/>
    
      <category term="ACL" scheme="http://gmle.github.io/tags/ACL/"/>
    
  </entry>
  
  <entry>
    <title>Scala中的 Option、Some、None、Null、Nil、Nothing</title>
    <link href="http://gmle.github.io/2016/09/19/Scala%E4%B8%AD%E7%9A%84%E5%87%A0%E4%B8%AA%E7%B1%BB%E5%9E%8B/"/>
    <id>http://gmle.github.io/2016/09/19/Scala中的几个类型/</id>
    <published>2016-09-19T00:53:20.000Z</published>
    <updated>2017-05-03T06:37:09.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>Scala的一些类型。</code></pre><a id="more"></a><h3 id="Scala的Option类型："><a href="#Scala的Option类型：" class="headerlink" title="Scala的Option类型："></a>Scala的Option类型：</h3><pre><code>Option可以存储任意类型的值，而Option的实例就是 Some 和 None 对象实例。    Some 和 None 都是 Option 的子类，而且都是 final 类型，所以没有派生子类。</code></pre><h4 id="Option的数据存取"><a href="#Option的数据存取" class="headerlink" title="Option的数据存取"></a>Option的数据存取</h4><pre><code>Option的数据存取就是对 Some 对象的操作。</code></pre><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">optionTest</span> <span class="keyword">extends</span> <span class="title">TestCase</span></span>&#123;</div><div class="line"></div><div class="line">  val capitals = Map(<span class="string">"france"</span> -&gt; <span class="string">"paris"</span>, <span class="string">"japan"</span> -&gt; <span class="string">"tokyo"</span>)</div><div class="line"></div><div class="line">  <span class="function">def <span class="title">test1</span><span class="params">()</span> </span>&#123;</div><div class="line">    val a = capitals get <span class="string">"france"</span></div><div class="line">    val b = capitals get <span class="string">"a"</span></div><div class="line">    println(a)         <span class="comment">//Some（paris）</span></div><div class="line">    println(b)         <span class="comment">//None</span></div><div class="line">  &#125;</div></pre></td></tr></table></figure><pre><code>结果对象类型为Some或者None当程序给你回传Some的时候，代表这个函数成功的给了你一个String，而你可以通过get()函数拿到那个String；如果程序返回的是None，也就是没有String返回的时候，如果还要调用，则会抛出异常：NoSuchElementExpection。</code></pre><h3 id="Scala的Null类型："><a href="#Scala的Null类型：" class="headerlink" title="Scala的Null类型："></a>Scala的Null类型：</h3><pre><code>Null是所有AnyRef的子类，在Scala的类型系统中，AnyRef是Any的子类，同是Any子类的还有AnyVal。对应Java值类型的所有类型都是AnyVal的子类。所以Null可以赋值给虽有的引用类型，而不能赋值给值类型，这个java的语义是相同的。null是Null的唯一对象。</code></pre><h3 id="Scala的Nothing类型："><a href="#Scala的Nothing类型：" class="headerlink" title="Scala的Nothing类型："></a>Scala的Nothing类型：</h3><pre><code>Nothing是所有类型的子类，也是Null的子类，Nothing没有对象，但是可以用来定义类型。例如，如果一个方法抛出异常，则异常的返回值类型就是Nothing（不会返回）</code></pre><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function">def <span class="title">get</span><span class="params">(index:Int)</span>:Int </span>= &#123;</div><div class="line"><span class="keyword">if</span>(x&lt;<span class="number">0</span>) <span class="keyword">throw</span> <span class="keyword">new</span> Expection(...)</div><div class="line"><span class="keyword">else</span> ...</div><div class="line">&#125;</div></pre></td></tr></table></figure><pre><code>if是表达式，必然有返回值，返回值必然会有类型。如果x&lt;0抛出异常，返回值的类型为Nothing，Nothing也是Int的子类，所以，if表达式的返回类型为Int，get方法的返回值类型也为Int</code></pre><h3 id="Scala的Nil类型："><a href="#Scala的Nil类型：" class="headerlink" title="Scala的Nil类型："></a>Scala的Nil类型：</h3><pre><code>Nil是一个空List,定义为 List[Nothing]，根据List的定义 LIst[+A]，所有的Nil是虽有List[T]的子类。</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;Scala的一些类型。
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="Scala" scheme="http://gmle.github.io/categories/Scala/"/>
    
    
      <category term="Scala" scheme="http://gmle.github.io/tags/Scala/"/>
    
      <category term="Option" scheme="http://gmle.github.io/tags/Option/"/>
    
      <category term="Some" scheme="http://gmle.github.io/tags/Some/"/>
    
      <category term="None" scheme="http://gmle.github.io/tags/None/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop_HDFS底层架构</title>
    <link href="http://gmle.github.io/2016/09/19/%E5%AF%B9HDFS%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%A4%E8%AF%86/"/>
    <id>http://gmle.github.io/2016/09/19/对HDFS的一些认识/</id>
    <published>2016-09-19T00:53:20.000Z</published>
    <updated>2017-05-03T06:38:42.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是我对HDFS的认知与了解，并结合网上查阅的资料进行的整合。</code></pre><h2 id="HDFS设计基础与目标"><a href="#HDFS设计基础与目标" class="headerlink" title="HDFS设计基础与目标"></a>HDFS设计基础与目标</h2><ul><li>硬件错误是常态，因此需要冗余。<ul><li>错误检测和快速、自动的恢复是HDFS最核心的架构目标</li></ul></li><li>小文件不适合存储，适合存储超大文件</li><li>流式数据访问</li></ul><a id="more"></a><ul><li>流式数据访问，即数据劈来给你读取而非随机读写，Hadoop需要的是数据分析而不是事务处理</li><li>大规模数据集</li><li>简单一致性模型，为了降低系统复杂度，对文件采用一次写多次读的理念：文件一经写入，关闭，再也不能修改。</li><li>程序采用 “数据就近” 原则分配节点执行。</li></ul><h3 id="HDFS的底层架构"><a href="#HDFS的底层架构" class="headerlink" title="HDFS的底层架构"></a>HDFS的底层架构</h3><h4 id="分布式文件系统"><a href="#分布式文件系统" class="headerlink" title="分布式文件系统"></a>分布式文件系统</h4><p>  优点：</p><pre><code>- 传统文件系统最大问题是容量和吞吐来那个的限制- 多用户多应用的并行读写是分布式文件系统产生的根源- 扩充存储空间的成本低廉    - 物理层存储的分布式    - 基于科户籍/服务器模式    - 通常情况下基于操作系统的本地文件系统</code></pre><h3 id="HDFS体系结构"><a href="#HDFS体系结构" class="headerlink" title="HDFS体系结构"></a>HDFS体系结构</h3><p><img src="http://7xt0cb.com1.z0.glb.clouddn.com/Hadoop%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84.png" alt="Hadoop体系结构图"></p><ul><li>NameNode<ul><li>管理文件系统的命名空间</li><li>记录每个文件数据快再各个DataNode上的文职和副本信息</li><li>协调客户端对文件的访问</li><li>协调客户端对文件的访问</li><li>记录命名空间内的改动或空间本身属性的改动</li><li>NameNode使用事务日志记录HDFS元数据的变化，使用映像文件存储文件系统的命名空间，包括文件映射，文件属性等</li></ul></li><li>DataNode<ul><li>负责所在物理节点的存储管理</li><li>一次写入，多次读取（不能修改）</li><li>文件由数据块组成，典型块的大小  0-1.0之间版本大小-&gt; 64M  1-2.x 大小为 128M</li><li>数据块尽量散布到各个节点内</li></ul></li><li>事务日志</li><li>映像文件</li><li>SecondaryNameNode</li></ul><h3 id="HDFS的高可用性"><a href="#HDFS的高可用性" class="headerlink" title="HDFS的高可用性"></a>HDFS的高可用性</h3><ul><li>HDFS集群中NameNode存在单点故障。<ul><li>躲雨只有一个NameNode的集群，如果NameNode及其出现意外downtime，那么整个集群将无法使用，知道NameNode重新启动</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是我对HDFS的认知与了解，并结合网上查阅的资料进行的整合。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;HDFS设计基础与目标&quot;&gt;&lt;a href=&quot;#HDFS设计基础与目标&quot; class=&quot;headerlink&quot; title=&quot;HDFS设计基础与目标&quot;&gt;&lt;/a&gt;HDFS设计基础与目标&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;硬件错误是常态，因此需要冗余。&lt;ul&gt;
&lt;li&gt;错误检测和快速、自动的恢复是HDFS最核心的架构目标&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;小文件不适合存储，适合存储超大文件&lt;/li&gt;
&lt;li&gt;流式数据访问&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://gmle.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://gmle.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="http://gmle.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop与HDFS的一些常用命令</title>
    <link href="http://gmle.github.io/2016/09/19/HSFS%E4%B8%AD%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/"/>
    <id>http://gmle.github.io/2016/09/19/HSFS中文件的一些操作命令/</id>
    <published>2016-09-19T00:53:20.000Z</published>
    <updated>2017-05-03T06:37:41.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>Hadoop与HDFS文件操作的一些常用命令，记个笔记。</code></pre><h3 id="Hadoop启动"><a href="#Hadoop启动" class="headerlink" title="Hadoop启动"></a>Hadoop启动</h3><a id="more"></a><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo ./sbin/start-dfs.sh</div><div class="line">sudo ./sbin/start-yarn.sh</div></pre></td></tr></table></figure><h4 id="启动之后的访问地址"><a href="#启动之后的访问地址" class="headerlink" title="启动之后的访问地址"></a>启动之后的访问地址</h4><h6 id="查看任务的运行情况"><a href="#查看任务的运行情况" class="headerlink" title="查看任务的运行情况"></a>查看任务的运行情况</h6><p><a href="http://localhost:8088/" target="_blank" rel="external">http://localhost:8088/</a></p><h6 id="查看-NameNode-和-Datanode-信息，还可以在线查看-HDFS-中的文件。"><a href="#查看-NameNode-和-Datanode-信息，还可以在线查看-HDFS-中的文件。" class="headerlink" title="查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件。"></a>查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件。</h6><p><a href="http://localhost:50070" target="_blank" rel="external">http://localhost:50070</a></p><h3 id="HDFS命令"><a href="#HDFS命令" class="headerlink" title="HDFS命令"></a>HDFS命令</h3><h4 id="查看HDFS基本统计信息"><a href="#查看HDFS基本统计信息" class="headerlink" title="查看HDFS基本统计信息"></a>查看HDFS基本统计信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ./bin/hdfs  dfsadmin -report</div></pre></td></tr></table></figure><h4 id="格式化namenode"><a href="#格式化namenode" class="headerlink" title="格式化namenode"></a>格式化namenode</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ./bin/hdfs namenode -format</div></pre></td></tr></table></figure><h4 id="HSFS文件操作"><a href="#HSFS文件操作" class="headerlink" title="HSFS文件操作"></a>HSFS文件操作</h4><h6 id="列出跟目录下所有的文件"><a href="#列出跟目录下所有的文件" class="headerlink" title="列出跟目录下所有的文件"></a>列出跟目录下所有的文件</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ./bin/hdfs dfs -ls /</div></pre></td></tr></table></figure><h6 id="path"><a href="#path" class="headerlink" title="path"></a>path</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ./bin/hdfs dfs -rm -r /path</div></pre></td></tr></table></figure><h6 id="增加一个path目录"><a href="#增加一个path目录" class="headerlink" title="增加一个path目录"></a>增加一个path目录</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ./bin/hdfs dfs -mkdir /path</div></pre></td></tr></table></figure><h6 id="列出跟目录下所有的文件-1"><a href="#列出跟目录下所有的文件-1" class="headerlink" title="列出跟目录下所有的文件"></a>列出跟目录下所有的文件</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ./bin/hdfs dfs -ls /</div></pre></td></tr></table></figure><h6 id="递归显示path下的所有文件"><a href="#递归显示path下的所有文件" class="headerlink" title="递归显示path下的所有文件"></a>递归显示path下的所有文件</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ./bin/hdfs dfs -lsr /path</div></pre></td></tr></table></figure><h6 id="将本地文件或目录localSrc上传到HDFS中的dest路径"><a href="#将本地文件或目录localSrc上传到HDFS中的dest路径" class="headerlink" title="将本地文件或目录localSrc上传到HDFS中的dest路径"></a>将本地文件或目录localSrc上传到HDFS中的dest路径</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ./bin/hdfs dfs –put /localSrc /dest</div></pre></td></tr></table></figure><h6 id="与-put命令相同"><a href="#与-put命令相同" class="headerlink" title="与-put命令相同"></a>与-put命令相同</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ./bin/hdfs dfs –copyFromLocal /localSrc /dest</div></pre></td></tr></table></figure><h6 id="显示文件内容到标准输出上。"><a href="#显示文件内容到标准输出上。" class="headerlink" title="显示文件内容到标准输出上。"></a>显示文件内容到标准输出上。</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ./bin/hdfs dfs –cat /filename</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;Hadoop与HDFS文件操作的一些常用命令，记个笔记。
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;Hadoop启动&quot;&gt;&lt;a href=&quot;#Hadoop启动&quot; class=&quot;headerlink&quot; title=&quot;Hadoop启动&quot;&gt;&lt;/a&gt;Hadoop启动&lt;/h3&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://gmle.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://gmle.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="http://gmle.github.io/tags/HDFS/"/>
    
      <category term="常用命令" scheme="http://gmle.github.io/tags/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop_对HDFS中文件的操作</title>
    <link href="http://gmle.github.io/2016/09/19/HDFS%E7%9A%84%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/"/>
    <id>http://gmle.github.io/2016/09/19/HDFS的文件操作/</id>
    <published>2016-09-19T00:53:20.000Z</published>
    <updated>2017-05-03T06:37:48.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>Hadoop没有当前目录的概念，当然也就没有 “cd” 命令HDFS文件操作的方法。</code></pre><h2 id="命令行方式"><a href="#命令行方式" class="headerlink" title="命令行方式"></a>命令行方式</h2><p>上一篇文章已经写到命令行方式操作HDFS，不再多说。<br><a href="http://gmle.github.io/2016/04/21/HSFS%E4%B8%AD%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/">进入传送门～</a></p><a id="more"></a><p>根据上一篇文章，写入两个文件：<br>Web查看文件的方式：<br>text1 : hello world.<br>text2 : hello hadoop.<br><img src="http://7xt0cb.com2.z0.glb.clouddn.com/文件访问方式.png" alt="Web查看文件的方式"></p><h2 id="Java-API操作HDFS"><a href="#Java-API操作HDFS" class="headerlink" title="Java API操作HDFS"></a>Java API操作HDFS</h2><p>新建一个Maven项目，不再细说<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</div><div class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></div><div class="line">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></div><div class="line">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</div><div class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>cn.lesion<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>bigData<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></div><div class="line"></div><div class="line"><span class="comment">&lt;!-- log4j日志包 --&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0-beta9<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line"></div><div class="line"><span class="comment">&lt;!-- Hadoop开发包 --&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line"></div><div class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></div></pre></td></tr></table></figure></p><p>新建一个类，写入如下代码：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> cn.lesion.data;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created in Intellij IDEA .</div><div class="line"> * Author : 王乐.</div><div class="line"> * Date : 2016.04.19.20.</div><div class="line"> *</div><div class="line"> * 说明：读取hdfs中的文件内容</div><div class="line"> */</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.InputStream;</div><div class="line"><span class="keyword">import</span> java.net.URI;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line"></div><div class="line">        <span class="comment">//hdfs的地址</span></div><div class="line">        String uri = <span class="string">"hdfs://lele:9000/"</span>;</div><div class="line">        Configuration config = <span class="keyword">new</span> Configuration();</div><div class="line">        FileSystem fs = FileSystem.get(URI.create(uri), config);</div><div class="line"></div><div class="line">        <span class="comment">// 显示在hdfs的/tmp/input下指定文件的内容</span></div><div class="line">        InputStream is = fs.open(<span class="keyword">new</span> Path(<span class="string">"/user/test1.txt"</span>));</div><div class="line">        IOUtils.copyBytes(is, System.out, <span class="number">1024</span>, <span class="keyword">true</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>运行之后：<br>Hello<br><img src="http://7xt0cb.com2.z0.glb.clouddn.com/Hello-Hadoop.png" alt="美美的Hello"></p>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;Hadoop没有当前目录的概念，当然也就没有 “cd” 命令
HDFS文件操作的方法。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;命令行方式&quot;&gt;&lt;a href=&quot;#命令行方式&quot; class=&quot;headerlink&quot; title=&quot;命令行方式&quot;&gt;&lt;/a&gt;命令行方式&lt;/h2&gt;&lt;p&gt;上一篇文章已经写到命令行方式操作HDFS，不再多说。&lt;br&gt;&lt;a href=&quot;http://gmle.github.io/2016/04/21/HSFS%E4%B8%AD%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/&quot;&gt;进入传送门～&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://gmle.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://gmle.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="http://gmle.github.io/tags/HDFS/"/>
    
      <category term="HDFS操作" scheme="http://gmle.github.io/tags/HDFS%E6%93%8D%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>HBase的配置(伪分布式)</title>
    <link href="http://gmle.github.io/2016/09/19/HBase%E7%9A%84%E9%85%8D%E7%BD%AE(%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F)/"/>
    <id>http://gmle.github.io/2016/09/19/HBase的配置(伪分布式)/</id>
    <published>2016-09-19T00:53:20.000Z</published>
    <updated>2017-05-03T06:38:09.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>HBase的CRUD</code></pre><p>###源码链接<br><a href="https://github.com/gmle/hbase" target="_blank" rel="external">https://github.com/gmle/hbase</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;code&gt;HBase的CRUD
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;###源码链接&lt;br&gt;&lt;a href=&quot;https://github.com/gmle/hbase&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/g
      
    
    </summary>
    
      <category term="HBase" scheme="http://gmle.github.io/categories/HBase/"/>
    
    
      <category term="Hadoop" scheme="http://gmle.github.io/tags/Hadoop/"/>
    
      <category term="HBase" scheme="http://gmle.github.io/tags/HBase/"/>
    
  </entry>
  
</feed>
